import{_ as t}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as i,c as e,e as a}from"./app-3V_FvGMF.js";const n="/pinkpig/assets/nlp_046-G3lTp08H.png",o="/pinkpig/assets/nlp_047-yy8p4I3i.png",r="/pinkpig/assets/nlp_048-kmdFq9Z4.png",p="/pinkpig/assets/nlp_055-RSvLVzow.png",l="/pinkpig/assets/nlp_057-7HsXkTYN.png",s="data:image/png;base64,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",d="/pinkpig/assets/nlp_050--RrZolxA.png",c="/pinkpig/assets/nlp_051-T3yJ7GxC.png",A="/pinkpig/assets/nlp_052-NJgYF3MI.png",g="/pinkpig/assets/nlp_053-RwP7UdoM.png",u={},h=a('<h1 id="attention-注意力机制" tabindex="-1"><a class="header-anchor" href="#attention-注意力机制" aria-hidden="true">#</a> Attention(注意力机制)</h1><h2 id="_1-朴素seq2seq的信息瓶颈" tabindex="-1"><a class="header-anchor" href="#_1-朴素seq2seq的信息瓶颈" aria-hidden="true">#</a> 1. 朴素seq2seq的信息瓶颈</h2><figure><img src="'+n+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>用Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。 这里存在一个问题：Encoder的最后一个隐状态（Context Vector）承载了源句子的所有信息，成为整个模型的“信息”瓶颈。</p><h2 id="_2-注意力机制就是希望打破这个信息瓶颈。宏观来看-attention直接把decoder中的每一个隐层-和encoder中的每一个隐层-都连接起来了" tabindex="-1"><a class="header-anchor" href="#_2-注意力机制就是希望打破这个信息瓶颈。宏观来看-attention直接把decoder中的每一个隐层-和encoder中的每一个隐层-都连接起来了" aria-hidden="true">#</a> 2. 注意力机制就是希望打破这个信息瓶颈。宏观来看，Attention直接把Decoder中的每一个隐层，和Encoder中的每一个隐层，都连接起来了！</h2><p>我们在解码的时候，是一个词一个词生成的。对于每一个词，它不仅可以接受Context Vector作为辅助信息, 也可以通过“直连”线路去获取Encoder RNN中的其他的隐状态的信息，从而为Decoder的解码进行辅助。</p><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_3-向量的点积-dot-product-矩阵的乘法" tabindex="-1"><a class="header-anchor" href="#_3-向量的点积-dot-product-矩阵的乘法" aria-hidden="true">#</a> 3. 向量的点积(Dot Product)==矩阵的乘法</h2><h3 id="_3-1-向量的点积" tabindex="-1"><a class="header-anchor" href="#_3-1-向量的点积" aria-hidden="true">#</a> 3.1. 向量的点积</h3><p>点积的结果是一个标量，反映了两个向量的相似程度</p><p>是线性代数中的一种运算，它是两个同维度向量之间的一种运算方式，通常用符号“·”表示。 也就是说，点积将两个向量中对应位置的元素相乘后再求和。</p><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-矩阵的乘法" tabindex="-1"><a class="header-anchor" href="#_3-2-矩阵的乘法" aria-hidden="true">#</a> 3.2. 矩阵的乘法</h3><p>可以看成是多个向量同时计算点积</p><p>只要满足矩阵乘法的条件就可以计算</p><h2 id="_4-qkv-每个词有q-k-v-query-key-value" tabindex="-1"><a class="header-anchor" href="#_4-qkv-每个词有q-k-v-query-key-value" aria-hidden="true">#</a> 4. QKV : 每个词有q,k,v(query,key,value)</h2><ul><li>query可以让你问一个关于这个词的问题，比如，非洲正在发生什么</li><li>key可以查看所有其他词，帮助得出与query最有关联的答案</li><li>value表示当前key对应的这个词的信息本身</li></ul><p>Attention的QKV起源</p><ul><li>有一种解释说，Attention中的Query，Key，Value的概念源于信息检索系统。举个简单的例子，当你在淘宝搜索某件商品时，你在搜索栏中输入的信息为Query，然后系统根据Query为你匹配Key，根据Query和Key的相似度得到匹配内容。</li></ul><h2 id="_5-attention机制的步骤" tabindex="-1"><a class="header-anchor" href="#_5-attention机制的步骤" aria-hidden="true">#</a> 5. Attention机制的步骤</h2><p>公式：</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>举例</p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>假设序列里只有2个词，这是向量级别的运算，实际运算是所有词组成矩阵一同运算的</p><h3 id="_5-1-注意力得分-attention-score-计算相关性" tabindex="-1"><a class="header-anchor" href="#_5-1-注意力得分-attention-score-计算相关性" aria-hidden="true">#</a> 5.1. 注意力得分(Attention Score)（计算相关性）</h3><ul><li><p>分子部分</p><ul><li>Decoder当前步与Encoder所有步(N步)分别进行点积计算，得到N个标量，即注意力得分(Attention Score)</li><li>这N个标量构成一个N维向量，它反映了解码器中当前时间步的隐状态与编码器中各个时间步的隐状态的相关性打分</li><li>自注意力为每个词和所有词分别求点积计算相关性</li></ul></li><li><p>分母部分</p><ul><li>将计算的每个点积同倍缩小</li><li>分母部分是点积的缩放，避免值太大，这样的注意力也叫缩放点积注意力(the scaled dot-product attention)</li><li>这里的除以8（Q、K、V向量的维度64的根号值）更多的其实是一种基于实践的工程经验</li></ul></li></ul><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-2-注意力分布-attention-distribution-获取概率密度分布" tabindex="-1"><a class="header-anchor" href="#_5-2-注意力分布-attention-distribution-获取概率密度分布" aria-hidden="true">#</a> 5.2. 注意力分布(Attention Distribution)（获取概率密度分布）</h3><ul><li>将注意力得分使用Softmax函数得到概率分布，即注意力分布</li><li>softmax()部分</li></ul><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>这个Attention distribution告诉了Decoder在时刻t，更应该把“注意力”放在Encoder的哪些隐状态上。</p><h3 id="_5-3-注意力输出-attention-output-求期望值" tabindex="-1"><a class="header-anchor" href="#_5-3-注意力输出-attention-output-求期望值" aria-hidden="true">#</a> 5.3. 注意力输出(Attention Output)（求期望值）</h3><ul><li>将注意力分布作为权重，对所有词进行加权求和</li><li>softmax()·V部分</li></ul><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>既然注意力分布是一个概率密度函数，那注意力输出其实很类似数学期望</p><h2 id="_6-self-attention-自注意力" tabindex="-1"><a class="header-anchor" href="#_6-self-attention-自注意力" aria-hidden="true">#</a> 6. Self-Attention(自注意力)</h2><p>和seq2seq的attention一模一样，唯一区别是，那个不是self，这个是self</p><h2 id="_7-multi-head-attention-多头注意力" tabindex="-1"><a class="header-anchor" href="#_7-multi-head-attention-多头注意力" aria-hidden="true">#</a> 7. Multi-Head Attention(多头注意力)</h2><p>多头注意力的理解：多头指多个角度来分析</p><p>eg. 法语译为英语：Jane visite I&#39;Afrique en septembre--&gt;Jane visits Africa on september.</p><ul><li><p>head1角度：what&#39;s happen?</p></li><li><p>head2角度：when?</p></li><li><p>head3角度：who?</p></li></ul><p>多个头的权重矩阵是不同的</p><p>计算出多头后，连起来，在乘以Wo，就是多头注意力的值</p><p>可学习的模型参数: 头数*QKV的3个权重矩阵+输出时的权重矩阵</p><h2 id="_8-attention机制带来的好处" tabindex="-1"><a class="header-anchor" href="#_8-attention机制带来的好处" aria-hidden="true">#</a> 8. Attention机制带来的好处</h2><figure><img src="'+A+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',48),f=[h];function m(x,z){return i(),e("div",null,f)}const D=t(u,[["render",m],["__file","05_attention.html.vue"]]);export{D as default};
