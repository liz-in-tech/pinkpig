import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as r,c as e,e as a}from"./app-D6wNGShW.js";const n={},t=a('<h1 id="预训练" tabindex="-1"><a class="header-anchor" href="#预训练" aria-hidden="true">#</a> 预训练</h1><ul><li>PTM,Pre-trained Model(预训练模型)</li></ul><h2 id="_1-预训练、微调和强化学习" tabindex="-1"><a class="header-anchor" href="#_1-预训练、微调和强化学习" aria-hidden="true">#</a> 1. 预训练、微调和强化学习</h2><p><strong>预训练学知识，指令微调学格式，强化学习对齐人类偏好</strong></p><p>预训练阶段旨在通过大规模无标注文本建立模型的基础能力</p><p>微调阶段使用有标注数据对于模型进行特定任务的适配</p><p>目标</p><ul><li>预训练：通用知识和理解能力</li><li>微调：特定任务；听懂指令</li><li>对齐：符合人类偏好 数据</li><li>预训练：无标签文本数据进行训练，例如维基百科、网页文本等</li><li>微调：带有标签的任务相关数据进行训练。这些数据通常是人工标注的，包含了输入文本和对应的标签或目标 训练方式</li><li>预训练：无监督</li><li>微调：有监督</li></ul><h2 id="_2-预训练思想-共性学习" tabindex="-1"><a class="header-anchor" href="#_2-预训练思想-共性学习" aria-hidden="true">#</a> 2. 预训练思想：共性学习</h2><ol><li><p>将训练任务拆解成共性学习和特性学习两个步骤，预训练时进行共性学习，下游任务进行特性学习</p></li><li><p>下游任务的模型参数不再是随机初始化，而是用预训练学到的共性信息对下游任务的模型进行参数初始化</p></li></ol><h2 id="_3-预训练模型用于下游任务的两种策略" tabindex="-1"><a class="header-anchor" href="#_3-预训练模型用于下游任务的两种策略" aria-hidden="true">#</a> 3. 预训练模型用于下游任务的两种策略</h2><h3 id="_3-1-fixed-feature-extractor-固定特征提取器" tabindex="-1"><a class="header-anchor" href="#_3-1-fixed-feature-extractor-固定特征提取器" aria-hidden="true">#</a> 3.1. Fixed Feature Extractor(固定特征提取器)</h3><p>适用：这种策略适用于下游任务的数据量较少的情况，可以避免过拟合。</p><p>what：将预训练好的模型的权重冻结，只使用其特征提取器提取下游任务数据的特征，然后将这些特征输入到下游任务的模型中进行训练。总之，冻结原来的参数，只训练一个新的分类层。</p><h3 id="_3-2-fine-tuning-微调" tabindex="-1"><a class="header-anchor" href="#_3-2-fine-tuning-微调" aria-hidden="true">#</a> 3.2. Fine-Tuning(微调)</h3><p>适用：微调一般是在有监督学习的框架下进行的。</p><p>是一种迁移学习方法，指在预训练模型的基础上，对部分或全部的参数进行微小的修改，以适应新的任务。</p><h2 id="_4-nlp预训练发展史" tabindex="-1"><a class="header-anchor" href="#_4-nlp预训练发展史" aria-hidden="true">#</a> 4. NLP预训练发展史</h2><p>图像预训练 → word embedding → word2vec → elmo → transformer → gpt → bert → GPT 234</p><h2 id="_5-继续预训练-增量预训练" tabindex="-1"><a class="header-anchor" href="#_5-继续预训练-增量预训练" aria-hidden="true">#</a> 5. 继续预训练 / 增量预训练</h2><h3 id="_5-1-为什么要增量预训练" tabindex="-1"><a class="header-anchor" href="#_5-1-为什么要增量预训练" aria-hidden="true">#</a> 5.1. 为什么要增量预训练？</h3><p><strong>预训练学知识</strong>，<strong>指令微调学格式</strong>，<strong>强化学习对齐人类偏好</strong>，所以要想大模型有领域知识，得增量预训练（靠指令微调记知识不靠谱，不是几十w条数据能做到的）。</p><h3 id="_5-2-进行增量预训练需要做哪些准备工作" tabindex="-1"><a class="header-anchor" href="#_5-2-进行增量预训练需要做哪些准备工作" aria-hidden="true">#</a> 5.2. 进行增量预训练需要做哪些准备工作？</h3><ol><li><strong>选取底座模型</strong>：可以根据自己的项目需求和硬件基础来选择合适的底座模型及模型参数量的大小。</li><li><strong>收集数据</strong>：一般来说需要收集大量的文本数据，包含各个领域，主要从互联网上获取，一般预训练数据的大小都是 TB 级别的。</li><li><strong>数据清洗</strong>：所有的信息都能够在互联网信息中被找到，只是<strong>信息密度</strong>相比「人工精选数据集」要更低。例如「明星信息」、「如何写代码」这些信息都能在新闻网站、或是问答网站中找到，只不过「维基百科」或是「Github」则是将这些信息给「高密度」且「结构化」地进行了存储。这使得我们在使用维基百科作为训练语料的时候，模型能够更快的学习到这些高密度信息（人物的经历、年龄、性别、职业等等），而这些内容在互联网信息（如新闻）中的信息密度则较低，即很少会有一条新闻完整的介绍一个艺人的过往经历。只要我们<strong>对互联网信息进行严格的处理</strong>（去除冗余信息，提高有用信息的密度），就能够加快模型的学习速度。</li></ol><h3 id="_5-3-增量预训练所用训练框架" tabindex="-1"><a class="header-anchor" href="#_5-3-增量预训练所用训练框架" aria-hidden="true">#</a> 5.3. 增量预训练所用训练框架？</h3><ul><li><strong>超大规模训练</strong>：选用 3D 并行，Megatron-Deepspeed拥有多个成功案例 </li><li><strong>少量节点训练</strong>：选用张量并行，但张量并行只有在 nvlink 环境下才会起正向作用，但提升也不会太明显。 </li><li><strong>少量卡训练</strong>：如果资源特别少，显存怎么也不够，可以使用 LoRA 进行增量预训练。</li></ul><h3 id="_5-4-增量预训练数据选取思路有哪些" tabindex="-1"><a class="header-anchor" href="#_5-4-增量预训练数据选取思路有哪些" aria-hidden="true">#</a> 5.4. 增量预训练数据选取思路有哪些？</h3><p>垂直领域预训练有三种思路：</p><ul><li>先用大规模通用语料预训练，再用小规模领域语料二次训练</li><li>直接进行大规模领域语料预训练</li><li>通用语料比例混合领域语料同时训练</li></ul><h3 id="_5-5-增量预训练训练流程是怎么样" tabindex="-1"><a class="header-anchor" href="#_5-5-增量预训练训练流程是怎么样" aria-hidden="true">#</a> 5.5. 增量预训练训练流程是怎么样？</h3><ol><li><strong>数据预处理</strong>：参考 LLaMA 的预训练长度，也把数据处理成2048长度（如果不够，做补全）。</li><li><strong>分词器</strong>：如果使用 LLaMA 可能需要添加中文词表，目前有不少人做了相关工作，当然也可以自己添加自己需要的词表。</li><li><strong>原始模型</strong>：各家框架的模型层名不太一样，训练时可能需要做一些调整，在预训练时尽量选择基座模型，不选 Chat 模型。</li><li><strong>训练模型</strong>：跑通只是第一步，根据训练情况反复调整比较重要。</li><li><strong>模型转换</strong>：不同框架的checkpoint格式不同，还会根据并行度分成很多个文件。</li><li><strong>模型测试</strong>：简单测试下续写能力，验证下模型是否正常。</li></ol><h3 id="_5-6-领域模型continue-pretrain-数据选取" tabindex="-1"><a class="header-anchor" href="#_5-6-领域模型continue-pretrain-数据选取" aria-hidden="true">#</a> 5.6. 领域模型Continue PreTrain 数据选取？</h3><p>在领域模型的Continue PreTrain过程中，数据选取是一个关键的步骤。选择合适的数据可以提高模型在特定领域上的性能和泛化能力。以下是一些常见的数据选取方法：</p><ul><li>领域相关数据：首先，可以收集与目标领域相关的数据。这些数据可以是从互联网上爬取的、来自特定领域的文档或者公司内部的数据等。这样的数据可以提供领域相关的语言和知识，有助于模型在特定领域上的表现。</li><li>领域专家标注：如果有领域专家可用，可以请他们对领域相关的数据进行标注。标注可以是分类、命名实体识别、关系抽取等任务，这样可以提供有监督的数据用于模型的训练。</li><li>伪标签：如果没有领域专家或者标注数据的成本较高，可以使用一些自动化的方法生成伪标签。例如，可以使用预训练的模型对领域相关的数据进行预测，将预测结果作为伪标签，然后使用这些伪标签进行模型的训练。</li><li>数据平衡：在进行数据选取时，需要注意数据的平衡性。如果某个类别的数据样本较少，可以考虑使用数据增强技术或者对该类别进行过采样，以平衡各个类别的数据量。</li><li>数据质量控制：在进行数据选取时，需要对数据的质量进行控制。可以使用一些质量评估指标，如数据的准确性、一致性等，来筛选和过滤数据。</li><li>数据预处理：在进行数据选取之前，可能需要对数据进行一些预处理，如分词、去除停用词、标准化等，以准备好输入模型进行训练。</li></ul>',34),l=[t];function h(o,d){return r(),e("div",null,l)}const _=i(n,[["render",h],["__file","03_预训练.html.vue"]]);export{_ as default};
