import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as o,o as l,c as r,a as n,d as e,b as s,e as t}from"./app-ieMuNFND.js";const p="/pinkpig/assets/image-4-DTBG9LoY.png",c="/pinkpig/assets/image-7-OmR60zRT.png",u="/pinkpig/assets/image-5-9dgTOHbk.png",d="/pinkpig/assets/image-6-141GNNfS.png",g="/pinkpig/assets/image-8-aiOnZel0.png",m="/pinkpig/assets/image-9-C6RBwEmB.png",h="/pinkpig/assets/image-10-teqTG5BI.png",v={},k=t('<h1 id="推理引擎-推理框架-推理加速引擎" tabindex="-1"><a class="header-anchor" href="#推理引擎-推理框架-推理加速引擎" aria-hidden="true">#</a> 推理引擎 / 推理框架 / 推理加速引擎</h1><h2 id="概览" tabindex="-1"><a class="header-anchor" href="#概览" aria-hidden="true">#</a> 概览</h2><figure><img src="'+p+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>vllm <ul><li>continus batching，而不是static batching</li><li>PagedAttention对kv cache的有效管理</li></ul></li><li>Text Generation Inference (TGI) <ul><li>HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理</li><li>Continuous batching</li><li>flash-attention 和 Paged Attention</li><li>项目架构： <ul><li>launcher 启动器（下载模型、启动server、启动router）</li><li>router 和 serve 负责主要的逻辑处理与模型调用 <ul><li>router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理</li><li>server (Python gRPC服务) ： 在每个显卡上都启动了一个 server</li></ul></li></ul></li></ul></li><li>faster transformer (NVIDIA FasterTransformer (FT)) <ul><li>是一个用于实现基于Transformer的神经网络推理的加速引擎</li><li>英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了</li><li>与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理</li><li>在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等</li><li>除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点</li></ul></li><li>TensorRT-LLM 英伟达家的</li><li>Triton</li></ul><h2 id="vllm" tabindex="-1"><a class="header-anchor" href="#vllm" aria-hidden="true">#</a> vLLM</h2><p>vLLM的吞吐量比HuggingFace Transformers（HF）高14x-24倍，比HuggingFace Text Generation Inference（TGI）高2.2x-2.5倍。</p><h3 id="_1-1-使用" tabindex="-1"><a class="header-anchor" href="#_1-1-使用" aria-hidden="true">#</a> 1.1 使用</h3><p>1.安装</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pip install vllm
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>2.安装完成后，运行examples/offline_inference.py即可，命令行运行</p><p>3.离线批量推理offline_inference代码</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> vllm <span class="token keyword">import</span> LLM<span class="token punctuation">,</span> SamplingParams

<span class="token comment"># Sample prompts</span>
prompts <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">&quot;Funniest joke ever:&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;The capital of France is&quot;</span><span class="token punctuation">,</span>
    <span class="token string">&quot;The future of AI is&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

<span class="token comment"># Create a sampling params objec</span>
sampling_params <span class="token operator">=</span> SamplingParams<span class="token punctuation">(</span>temperature<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">,</span> top_p<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>

<span class="token comment"># Create an LLM</span>
llm <span class="token operator">=</span> LLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;huggyllama/llama-13b&quot;</span><span class="token punctuation">)</span>

<span class="token comment"># Generate texts from the prompts. The output is a list of RequestOutput objects</span>
<span class="token comment"># that contain the prompt, generated text, and other information.</span>
outputs <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> sampling_params<span class="token punctuation">)</span>

<span class="token comment"># Print the outputs</span>
<span class="token keyword">for</span> output <span class="token keyword">in</span> outputs<span class="token punctuation">:</span>
    prompt <span class="token operator">=</span> output<span class="token punctuation">.</span>prompt
    generated_text <span class="token operator">=</span> output<span class="token punctuation">.</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Prompt: </span><span class="token interpolation"><span class="token punctuation">{</span>prompt<span class="token conversion-option punctuation">!r</span><span class="token punctuation">}</span></span><span class="token string">, Generated text: </span><span class="token interpolation"><span class="token punctuation">{</span>generated_text<span class="token conversion-option punctuation">!r</span><span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="vllm架构" tabindex="-1"><a class="header-anchor" href="#vllm架构" aria-hidden="true">#</a> vLLM架构</h3><p>vLLM 采用一种集中式调度器（scheduler）来协调分布式 GPU 工作器（worker）的执行。KV 缓存管理器由 PagedAttention 驱动，能以分页方式有效管理 KV 缓存。具体来说，KV 缓存管理器通过集中式调度器发送的指令来管理 GPU 工作器上的物理 KV 缓存内存。</p><figure><img src="`+c+'" alt="vLLM架构" tabindex="0" loading="lazy"><figcaption>vLLM架构</figcaption></figure><h3 id="_1-2-核心优化点" tabindex="-1"><a class="header-anchor" href="#_1-2-核心优化点" aria-hidden="true">#</a> 1.2 核心优化点</h3><p><strong>Continuous batching</strong></p><p>一旦一个batch中的某个seq完成生成，发射了一个end-of-seq token，就可以在其位置插入新的seq继续生成token，从而达到比static batching更高的GPU利用率 <img src="'+u+'" alt="" loading="lazy"></p><p><strong>PagedAttention：解决内存瓶颈</strong></p><p>受操作系统中虚拟内存和分页的经典思想启发的注意力算法，这就是模型加速的秘诀。</p><ul><li>PagedAttention是对kv cache所占空间的分页管理</li><li>PagedAttention 允许在非连续的内存空间中存储连续的键和值</li><li>具体来说，PagedAttention 将每个序列的 KV 缓存划分为块，每个块包含固定数量 token 的键和值（可以将块视为页面，将 token 视为字节，将序列视为进程）</li><li>通过<strong>块表</strong>将连续逻辑块映射到非连续物理块中</li><li>在注意力计算期间，PagedAttention 内核可以有效地识别和获取这些块</li><li>在 PagedAttention 中，<strong>内存浪费只会发生在序列的最后一个块中</strong>。</li><li>PagedAttention 还有另一个关键优势 —— <strong>高效的内存共享</strong>。例如在并行采样中，多个输出序列是由同一个提示（prompt）生成的。在这种情况下，提示的计算和内存可以在输出序列中共享。（同一个输入prompt采样多个输出时，可以共享输入prompt）</li></ul><p>KV 缓存有其自己的独特性质：</p><ul><li>内存占用大</li><li>动态且不可预测 <ul><li>KV 缓存的大小取决于序列长度，这是高度可变和不可预测的</li><li>它会在模型生成新 token 的过程中随时间动态地增长，而且它的持续时间和长度是无法事先知晓的</li></ul></li></ul><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>上图（左）展示了一个 130 亿参数的 LLM 在一台 40GB RAM 的英伟达 A100 GPU 上的内存分布</p><ul><li>65% 的内存都分配给了模型权重</li><li>30% 的内存是用于存储 KV 缓存</li><li>其余占比很小的内存则是用于其它数据，包括激活 —— 评估 LLM 时创建的临时张量</li></ul><p><strong>KV 缓存的管理方式就成了决定最大批量大小的关键</strong>。如图（右）所示，如果管理方式很低效，KV 缓存内存就会极大限制批量大小，并由此限制 LLM 的吞吐量。</p><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图示例：其键和值向量分布在三个块上，并且这三个块在物理内存上并不相邻连续。</p><figure><img src="'+m+'" alt="vLLM 管理单个序列的内存的示例" tabindex="0" loading="lazy"><figcaption>vLLM 管理单个序列的内存的示例</figcaption></figure><figure><img src="'+h+'" alt="vLLM 管理两个序列的内存的示例" tabindex="0" loading="lazy"><figcaption>vLLM 管理两个序列的内存的示例</figcaption></figure><h3 id="_1-3-优点" tabindex="-1"><a class="header-anchor" href="#_1-3-优点" aria-hidden="true">#</a> 1.3 <strong>优点</strong></h3><ul><li><strong>文本生成的速度</strong>**：** 实验多次，发现vLLM的推理速度是最快的；</li><li><strong>高吞吐量服务</strong>**：** 支持各种解码算法，比如parallel sampling, beam search等；</li><li><strong>与OpenAI API兼容</strong>**：** 如果使用OpenAI API，只需要替换端点的URL即可；</li></ul><h3 id="_1-4-缺点" tabindex="-1"><a class="header-anchor" href="#_1-4-缺点" aria-hidden="true">#</a> 1.4 <strong>缺点</strong></h3><ul><li><strong>添加自定义模型</strong>：虽然可以合并自己的模型，但如果模型没有使用与vLLM中现有模型类似的架构，则过程会变得更加复杂。例如，增加Falcon的支持，这似乎很有挑战性；</li><li><strong>缺乏对适配器（LoRA、QLoRA等）的支持</strong>：当针对特定任务进行微调时，开源LLM具有重要价值。然而，在当前的实现中，没有单独使用模型和适配器权重的选项，这限制了有效利用此类模型的灵活性。</li><li><strong>缺少权重量化</strong>：有时，LLM可能不需要使用GPU内存，这对于减少GPU内存消耗至关重要。</li></ul><p>这是LLM推理最快的库。得益于其内部优化，它显著优于竞争对手。尽管如此，它在支持有限范围的模型方面确实存在弱点。</p>',36),b=n("strong",null,"使用vLLM的开发路线可以参考：",-1),f={href:"https://link.zhihu.com/?target=https%3A//github.com/vllm-project/vllm/issues/244",title:"https://github.com/vllm-project/vllm/issues/244",target:"_blank",rel:"noopener noreferrer"},_=n("strong",null,"https://github.com/vllm-project/vllm/issues/244",-1),L=t(`<h2 id="text-generation-inference-tgi" tabindex="-1"><a class="header-anchor" href="#text-generation-inference-tgi" aria-hidden="true">#</a> Text Generation Inference (TGI)</h2><p>Text generation inference是用于文本生成推断的Rust、Python和gRPC服务器，在HuggingFace中已有LLM 推理API使用。</p><h4 id="_2-1使用" tabindex="-1"><a class="header-anchor" href="#_2-1使用" aria-hidden="true">#</a> 2.1使用</h4><p><strong>使用docker运行web server</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token function">mkdir</span> data
<span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all --shm-size 1g <span class="token parameter variable">-p</span> <span class="token number">8080</span>:80 <span class="token punctuation">\\</span>
<span class="token parameter variable">-v</span> data:/data ghcr.io/huggingface/text-generation-inference:0.9 <span class="token punctuation">\\</span>
  --model-id huggyllama/llama-13b <span class="token punctuation">\\</span>
  --num-shard <span class="token number">1</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>查询实例</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># pip install text-generation</span>
from text_generation <span class="token function">import</span> Client

client <span class="token operator">=</span> Client<span class="token punctuation">(</span><span class="token string">&quot;http://127.0.0.1:8080&quot;</span><span class="token punctuation">)</span>
prompt <span class="token operator">=</span> <span class="token string">&quot;Funniest joke ever:&quot;</span>
print<span class="token punctuation">(</span>client.generate<span class="token punctuation">(</span>prompt, <span class="token assign-left variable">max_new_tokens</span><span class="token operator">=</span><span class="token number">17</span> <span class="token assign-left variable">temperature</span><span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span>.generated_text<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="_2-2功能" tabindex="-1"><a class="header-anchor" href="#_2-2功能" aria-hidden="true">#</a> 2.2<strong>功能</strong></h4><ul><li><strong>内置服务评估</strong>**：** 可以监控服务器负载并深入了解其性能；</li><li><strong>使用flash attention（和v2）和Paged attention优化transformer推理代码</strong>**：** 并非所有模型都内置了对这些优化的支持，该技术可以对未使用该技术的模型可以进行优化；</li></ul><h4 id="_2-3-优点" tabindex="-1"><a class="header-anchor" href="#_2-3-优点" aria-hidden="true">#</a> 2.3 <strong>优点</strong></h4><ul><li><strong>所有的依赖项都安装在Docker中</strong>**：** 会得到一个现成的环境；</li><li><strong>支持HuggingFace模型</strong>**：** 轻松运行自己的模型或使用任何HuggingFace模型中心；</li><li><strong>对模型推理的控制</strong>：该框架提供了一系列管理模型推理的选项，包括精度调整、量化、张量并行性、重复惩罚等；</li></ul><h4 id="_2-4缺点" tabindex="-1"><a class="header-anchor" href="#_2-4缺点" aria-hidden="true">#</a> 2.4<strong>缺点</strong></h4>`,12),x=n("strong",null,"缺乏对适配器的支持",-1),A={href:"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DHI3cYN0c9ZU",title:"https://www.youtube.com/watch?v=HI3cYN0c9ZU",target:"_blank",rel:"noopener noreferrer"},T=n("li",null,[n("strong",null,"从源代码（Rust+CUDA内核）编译"),s("**：** 对于不熟悉Rust的人，将客户化代码纳入库中变得很有挑战性；")],-1),P=n("li",null,[n("strong",null,"文档不完整"),s("：所有信息都可以在项目的自述文件中找到。尽管它涵盖了基础知识，但必须在问题或源代码中搜索更多细节；")],-1),M=n("strong",null,"使用Text generation inference的开发路线可以参考：",-1),w={href:"https://link.zhihu.com/?target=https%3A//github.com/huggingface/text-generation-inference/issues/232",title:"https://github.com/huggingface/text-generation-inference/issues/232",target:"_blank",rel:"noopener noreferrer"},I=n("strong",null,"https://github.com/huggingface/text-generation-inference/issues/232",-1),y=n("h2",{id:"reference",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#reference","aria-hidden":"true"},"#"),s(" Reference")],-1),F=n("ul",null,[n("li",null,"https://blog.vllm.ai/2023/06/20/vllm.html"),n("li",null,"https://www.eula.club/blogs/%E5%9F%BA%E4%BA%8EvLLM%E5%8A%A0%E9%80%9F%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%B9%B6%E8%AF%84%E4%BC%B0%E6%80%A7%E8%83%BD.html")],-1);function E(G,q){const a=o("ExternalLinkIcon");return l(),r("div",null,[k,n("p",null,[b,n("a",f,[_,e(a)])]),L,n("ul",null,[n("li",null,[x,s("**：** 需要注意的是，尽管可以使用适配器部署LLM（可以参考"),n("a",A,[s("https://www.youtube.com/watch?v=HI3cYN0c9ZU"),e(a)]),s("），但目前还没有官方支持或文档；")]),T,P]),n("p",null,[M,n("a",w,[I,e(a)])]),y,F])}const B=i(v,[["render",E],["__file","03_推理框架.html.vue"]]);export{B as default};
