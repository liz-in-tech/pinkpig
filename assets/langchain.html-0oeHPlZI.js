import{_ as n}from"./plugin-vue_export-helper-x3n3nnut.js";import{r,o as i,c as o,d as s,w as p,e as a,b as h}from"./app-K2payoM4.js";const d="/pinkpig/assets/langchain_001-uhtu7_01.png",c="/pinkpig/assets/langchain_002-dk3_-kTD.png",l="/pinkpig/assets/langchain_003-7o8W3KK5.png",u="/pinkpig/assets/langchain_004-B-3BKt__.png",e="/pinkpig/assets/langchain_005-jNk7hFX2.png",g="/pinkpig/assets/langchain_006-WCASrFus.png",m="/pinkpig/assets/langchain_007-T-IgoIbV.png",f="/pinkpig/assets/langchain_008-ie6o8Txc.png",_="/pinkpig/assets/langchain_009-KiWIj2sj.png",b="/pinkpig/assets/langchain_010-5501uHoS.png",x="/pinkpig/assets/langchain_011-W3STqv-a.png",q="/pinkpig/assets/langchain_012-3BWl_RWM.png",y="/pinkpig/assets/langchain_013-UdPNr8ov.png",v="/pinkpig/assets/langchain_014-bdw45o61.png",k="/pinkpig/assets/langchain_015-L3NWhGxP.png",L="/pinkpig/assets/langchain_016-hqAJYss9.png",A="/pinkpig/assets/langchain_017-pWTat7bZ.png",C="/pinkpig/assets/langchain_018-OyCfraeV.png",M="/pinkpig/assets/langchain_019-LcgYPISK.png",P="/pinkpig/assets/langchain_020-qx1mWj3j.png",T="/pinkpig/assets/langchain_021-_vLMO6nP.png",w="/pinkpig/assets/langchain_022-FZESzi5f.png",S="/pinkpig/assets/langchain_023-_NzEcn6E.png",I="/pinkpig/assets/langchain_024-oMIJT4mC.png",O="/pinkpig/assets/langchain_025-hnVoI-6x.png",z="/pinkpig/assets/langchain_026-2XJ5VPKB.png",B="/pinkpig/assets/langchain_027-fHVUNVVX.png",R="/pinkpig/assets/langchain_028-_yCs65da.png",E="/pinkpig/assets/langchain_029-GcgrtaMC.png",D="/pinkpig/assets/langchain_030-uEESUee5.png",N="/pinkpig/assets/langchain_031-SLNNsxyx.png",V="/pinkpig/assets/langchain_032-hnbQbstG.png",F="/pinkpig/assets/langchain_033--rq_zC8Y.png",K="/pinkpig/assets/langchain_034-1eUkXHHo.png",W="/pinkpig/assets/langchain_035-zY1rzFsX.png",G="/pinkpig/assets/langchain_036-vcGcCYzg.jpg",U="/pinkpig/assets/langchain_037-JV8Ru5Zr.png",H="/pinkpig/assets/langchain_038-QjLY2bv3.png",Q="/pinkpig/assets/langchain_039-n1m6wL6E.png",Y="/pinkpig/assets/langchain_040-jo8AHJHd.png",J={},j=a('<h1 id="langchain" tabindex="-1"><a class="header-anchor" href="#langchain" aria-hidden="true">#</a> LangChain</h1><h2 id="简介" tabindex="-1"><a class="header-anchor" href="#简介" aria-hidden="true">#</a> 简介</h2><h3 id="what" tabindex="-1"><a class="header-anchor" href="#what" aria-hidden="true">#</a> what</h3><h4 id="langchain是一个用于快速构建llm应用的开源框架" tabindex="-1"><a class="header-anchor" href="#langchain是一个用于快速构建llm应用的开源框架" aria-hidden="true">#</a> LangChain是一个用于快速构建LLM应用的开源框架</h4><p>将一些使用LLM时的通用行为进行了抽象，封装为API，统一了使用方式，简化了开发流程</p><h4 id="它使应用程序能够" tabindex="-1"><a class="header-anchor" href="#它使应用程序能够" aria-hidden="true">#</a> 它使应用程序能够：</h4><p>具有上下文意识：将语言模型连接到上下文的来源（提示指令、少量示例、用于支撑其回应的内容等）。</p><p>进行推理：依靠语言模型来进行推理（关于如何基于提供的上下文回答问题、采取什么行动等）。</p><p>人机交互这块，模型能力附加</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="how" tabindex="-1"><a class="header-anchor" href="#how" aria-hidden="true">#</a> how</h3><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>此处的Embedding的选择，很影响效果。 如果用OpenAI的语言模型，最好使用OpenAI的Embedding,效果会更好</p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="langchain框架结构" tabindex="-1"><a class="header-anchor" href="#langchain框架结构" aria-hidden="true">#</a> langchain框架结构</h3><p>聊天模式是会话模式的一种</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>聊天模式、会话模式、检索模式都是一条链路过来的，中间没有任何的循环</p><p>agent模式，不符合预期会进行循环，直到符合预期，才输出给用户</p><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="架构图-组件化设计" tabindex="-1"><a class="header-anchor" href="#架构图-组件化设计" aria-hidden="true">#</a> 架构图(组件化设计)</h2><h3 id="" tabindex="-1"><a class="header-anchor" href="#" aria-hidden="true">#</a></h3><p>这些组件共同简化了整个LLM应用程序的生命周期 <img src="'+g+'" alt="" loading="lazy"></p><h4 id="langsmith-产品化" tabindex="-1"><a class="header-anchor" href="#langsmith-产品化" aria-hidden="true">#</a> LangSmith(产品化)</h4><h5 id="开发者平台-可观测平台" tabindex="-1"><a class="header-anchor" href="#开发者平台-可观测平台" aria-hidden="true">#</a> 开发者平台(可观测平台)</h5><p>Monitoring, Feedback, Evaluation, Annotation, Playground, Testing, Debugging (监控、反馈、评估、注释、沙盒环境、测试和调试) Langchain的链</p><h4 id="langserve-部署" tabindex="-1"><a class="header-anchor" href="#langserve-部署" aria-hidden="true">#</a> LangServe(部署)</h4><h5 id="chains-as-rest-apis" tabindex="-1"><a class="header-anchor" href="#chains-as-rest-apis" aria-hidden="true">#</a> Chains as Rest APIs</h5><p>将LangChain的链作为REST API进行部署</p><h4 id="langchain库-开发" tabindex="-1"><a class="header-anchor" href="#langchain库-开发" aria-hidden="true">#</a> LangChain库(开发)</h4><h5 id="templates-模板" tabindex="-1"><a class="header-anchor" href="#templates-模板" aria-hidden="true">#</a> Templates(模板)</h5><h6 id="reference-applications" tabindex="-1"><a class="header-anchor" href="#reference-applications" aria-hidden="true">#</a> Reference Applications</h6><p>一系列易于部署的参考架构，适用于广泛的任务。 提供了参考应用程序的模板，帮助开发者快速部署LangChain应用。</p><h5 id="langchain包-核心组件-构成应用程序认知架构的链条、代理和检索策略" tabindex="-1"><a class="header-anchor" href="#langchain包-核心组件-构成应用程序认知架构的链条、代理和检索策略" aria-hidden="true">#</a> langchain包：核心组件，构成应用程序认知架构的链条、代理和检索策略</h5><h6 id="chains-链" tabindex="-1"><a class="header-anchor" href="#chains-链" aria-hidden="true">#</a> Chains(链)</h6><p>Off-the-shelf chains(内置的链)</p><p>Components</p><p>Agents(代理)</p><p>Advanced Retrieval Strategies(高级检索策略)</p><h5 id="langchain-community包-集成组件-第三方集成" tabindex="-1"><a class="header-anchor" href="#langchain-community包-集成组件-第三方集成" aria-hidden="true">#</a> langchain-community包：集成组件/第三方集成</h5><h6 id="model-i-o-模型输入-输出处理" tabindex="-1"><a class="header-anchor" href="#model-i-o-模型输入-输出处理" aria-hidden="true">#</a> Model I/O(模型输入/输出处理)</h6><p>Model(模型)</p><p>Prompt(提示)</p><p>Example Selector(示例选择器)</p><p>Output Parser(输出解析器)</p><h6 id="retrieval-检索" tabindex="-1"><a class="header-anchor" href="#retrieval-检索" aria-hidden="true">#</a> Retrieval(检索)</h6><p>Retriever(检索器)</p><p>Document Loader(文档加载器)</p><p>Vector Store(向量存储)</p><p>Text Splitter(文本分割器)</p><p>Embedding Model(嵌入模型)</p><h6 id="agent-tooling-代理工具" tabindex="-1"><a class="header-anchor" href="#agent-tooling-代理工具" aria-hidden="true">#</a> Agent Tooling(代理工具)</h6><p>Tools(工具)</p><p>Toolkits(工具集)</p><h5 id="langchain-core包-框架基础-lcel-langchain表达式语言-langchain-expression-language" tabindex="-1"><a class="header-anchor" href="#langchain-core包-框架基础-lcel-langchain表达式语言-langchain-expression-language" aria-hidden="true">#</a> langchain-core包（框架基础）：LCEL(LangChain表达式语言, LangChain Expression Language)</h5><p>Parallelization, Fallbacks, Tracing, Batching, Streaming, Async, Composition (包括并行化、回退机制、追踪、批处理、流式处理、异步处理和组合等协议)</p><h2 id="案例" tabindex="-1"><a class="header-anchor" href="#案例" aria-hidden="true">#</a> 案例</h2><h3 id="langchain-chatchat" tabindex="-1"><a class="header-anchor" href="#langchain-chatchat" aria-hidden="true">#</a> langchain-Chatchat</h3><h4 id="agent-executor-是一个chain" tabindex="-1"><a class="header-anchor" href="#agent-executor-是一个chain" aria-hidden="true">#</a> agent_executor(是一个chain)</h4><p>智能体/人</p><h5 id="agent-不是一个chain" tabindex="-1"><a class="header-anchor" href="#agent-不是一个chain" aria-hidden="true">#</a> agent(不是一个chain)</h5><p>规划能力/大脑(planning)</p><h6 id="llm-chain" tabindex="-1"><a class="header-anchor" href="#llm-chain" aria-hidden="true">#</a> llm_chain</h6><p>####### prompt ######## prompt_template</p><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>######## tools ######### 是一个chain的 ########## search_knowledgebase_complex(搜索本地知识库) ########### Step1：llm_knowledge_chain</p><p>根据query和数据库名称，让llm输出哪个数据库查什么信息 ############ ChatOpenAI或其他模型 callbacks</p><p>############ prompt</p><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>input_key: str = &quot;question&quot;</p><p>output_key: str = &quot;answer&quot;</p><p>########### Step2：搜索知识库 kb_doc_api.py里的def search_docs函数</p><p>########### Step3：rerank（可选，是否启用reranker模型） ############ 文档压缩 from langchain.retrievers.document_compressors.base import BaseDocumentCompressor</p><p>########### Step4：llm_chain</p><p>和search_internet一致 ############ ChatOpenAI或其他模型 callbacks</p><p>############ prompt prompt模板(如图)+context是搜索知识库的结果+question是用户的问题</p><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>input_key: str = &quot;question&quot;</p><p>output_key: str = &quot;answer&quot;</p><p>########## calculate ########### llm_math_chain ############ ChatOpenAI或其他模型 callbacks</p><p>############ prompt</p><figure><img src="'+b+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+x+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>input_key: str = &quot;question&quot;</p><p>output_key: str = &quot;answer&quot;</p><p>########## weather_chack ########### llm_weather_chain ############ ChatOpenAI或其他模型 callbacks</p><p>############ prompt</p><figure><img src="'+q+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>input_key: str = &quot;question&quot;</p><p>output_key: str = &quot;answer&quot;</p><p>########## search_internet ########### Step1：调API 调搜索引擎API</p><p>########### Step2：llm_chain ############ ChatOpenAI或其他模型 callbacks</p><p>############ prompt prompt模板(如图)+context是搜索引擎搜索的结果+question是用户的问题</p><figure><img src="'+v+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>input_key: str = &quot;question&quot;</p><p>output_key: str = &quot;answer&quot;</p><p>######### 调API的 arxiv</p><p>shell</p><p>wolfram</p><p>search_youtube</p><p>####### model ######## ChatOpenAI或其他模型 callbacks</p><p>output_parser</p><h6 id="stop" tabindex="-1"><a class="header-anchor" href="#stop" aria-hidden="true">#</a> stop</h6><p>停止词</p><h6 id="allowed-tools" tabindex="-1"><a class="header-anchor" href="#allowed-tools" aria-hidden="true">#</a> allowed_tools</h6><p>tool_names</p><p>tools</p><p>memory</p><h5 id="input" tabindex="-1"><a class="header-anchor" href="#input" aria-hidden="true">#</a> input</h5><p>用户询问的问题</p><p>callbacks</p><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="langchain-python" tabindex="-1"><a class="header-anchor" href="#langchain-python" aria-hidden="true">#</a> LangChain Python</h2><h3 id="langchain基类" tabindex="-1"><a class="header-anchor" href="#langchain基类" aria-hidden="true">#</a> langchain基类</h3><h4 id="-1" tabindex="-1"><a class="header-anchor" href="#-1" aria-hidden="true">#</a></h4><figure><img src="'+k+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="runnableserializable的子类" tabindex="-1"><a class="header-anchor" href="#runnableserializable的子类" aria-hidden="true">#</a> RunnableSerializable的子类</h5><figure><img src="'+L+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h6 id="chain" tabindex="-1"><a class="header-anchor" href="#chain" aria-hidden="true">#</a> Chain</h6><p>AgentExecutor</p><p>BasePromptTemplate</p><p>BaseLanguageModel</p><p>BaseOutputParser</p><p>BaseTool</p><p>BaseRetriever</p><h4 id="langchain-expression-language-lcel-langchain-表达式语言" tabindex="-1"><a class="header-anchor" href="#langchain-expression-language-lcel-langchain-表达式语言" aria-hidden="true">#</a> LangChain Expression Language(LCEL, LangChain 表达式语言)</h4><h5 id="runnable" tabindex="-1"><a class="header-anchor" href="#runnable" aria-hidden="true">#</a> Runnable</h5><h6 id="输入输出各有不同" tabindex="-1"><a class="header-anchor" href="#输入输出各有不同" aria-hidden="true">#</a> 输入输出各有不同</h6><p>####### Prompt Dictionary=&gt;PromptValue</p><p>####### LLM Single string, list of chat messages or a PromptValue=&gt;String</p><p>####### ChatModel Single string, list of chat messages or a PromptValue=&gt;ChatMessage</p><p>####### OutputParser The output of an LLM or ChatModel=&gt;Depends on the parser</p><p>####### Retriever Single string=&gt;List of documents</p><p>####### Tool Single string, or dictionary, depending on the tool=&gt;Depending on the tool</p><h3 id="model-i-o" tabindex="-1"><a class="header-anchor" href="#model-i-o" aria-hidden="true">#</a> Model I/O</h3><h4 id="interface-with-language-models" tabindex="-1"><a class="header-anchor" href="#interface-with-language-models" aria-hidden="true">#</a> Interface with language models</h4><p>与模型进行交互</p><p>与LLM交互的模块，包括规范输入、调用LLM、规范输出</p><figure><img src="'+A+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="prompts" tabindex="-1"><a class="header-anchor" href="#prompts" aria-hidden="true">#</a> Prompts</h4><p>模板化、动态选择和管理模型输入</p><h5 id="prompt-templates-prompt-模板" tabindex="-1"><a class="header-anchor" href="#prompt-templates-prompt-模板" aria-hidden="true">#</a> Prompt templates(Prompt 模板)</h5><p>带标记的文本字符串，结合用户的输入，生成Prompt</p><p>用户的指令和输入，通常是纯文本或者ChatMessage</p><h5 id="example-selectors-示例选择器" tabindex="-1"><a class="header-anchor" href="#example-selectors-示例选择器" aria-hidden="true">#</a> Example selectors(示例选择器)</h5><p>具体问题的示例</p><h5 id="prompt" tabindex="-1"><a class="header-anchor" href="#prompt" aria-hidden="true">#</a> prompt</h5><h6 id="baseprompttemplate" tabindex="-1"><a class="header-anchor" href="#baseprompttemplate" aria-hidden="true">#</a> BasePromptTemplate</h6><p>####### BaseChatPromptTemplate ######## ChatPromptTemplate 主要应用是在交互式聊天或对话系统中格式化和构建对话提示，以便根据特定的场景和参数定制对话</p><p>format_messages</p><p>####### StringPromptTemplate ######## PromptTemplate ######### 方法 from_template</p><p>format</p><p>FewShotPromptTemplate</p><h6 id="basemessageprompttemplate" tabindex="-1"><a class="header-anchor" href="#basemessageprompttemplate" aria-hidden="true">#</a> BaseMessagePromptTemplate</h6><p>####### BaseStringMessagePromptTemplate ######## SystemMessagePromptTemplate 系统消息提示词模板</p><p>HumanMessagePromptTemplate</p><h6 id="basemessage" tabindex="-1"><a class="header-anchor" href="#basemessage" aria-hidden="true">#</a> BaseMessage</h6><p>####### SystemMessage 系统消息，通常作为第一条消息</p><p>####### AIMessage 来自AI的Message</p><p>####### HumanMessage 来自人的Message</p><h5 id="prompts-1" tabindex="-1"><a class="header-anchor" href="#prompts-1" aria-hidden="true">#</a> prompts</h5><h6 id="源码位置" tabindex="-1"><a class="header-anchor" href="#源码位置" aria-hidden="true">#</a> 源码位置</h6><p>所有类源码都在langchain_core的prompts和example_selectors下</p><p>所有类都整合在langchain的prompts下 （基础的类整合在base.py中）</p><h6 id="基类" tabindex="-1"><a class="header-anchor" href="#基类" aria-hidden="true">#</a> 基类</h6><figure><img src="'+C+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h6 id="prompt-chatprompttemplate-from-messages" tabindex="-1"><a class="header-anchor" href="#prompt-chatprompttemplate-from-messages" aria-hidden="true">#</a> prompt = ChatPromptTemplate.from_messages([</h6><pre><code>(&quot;system&quot;, &quot;You are world class technical documentation writer.&quot;),
(&quot;user&quot;, &quot;{input}&quot;)
</code></pre><p>]) 转换messages的格式 生成ChatPromptTemplate的一个实例并返回</p><h6 id="prompt-chatprompttemplate-from-messages-1" tabindex="-1"><a class="header-anchor" href="#prompt-chatprompttemplate-from-messages-1" aria-hidden="true">#</a> prompt = ChatPromptTemplate.from_messages([</h6><pre><code>(&quot;system&quot;, &quot;Answer the user&#39;s questions based on the below context:\\n\\n{context}&quot;),
MessagesPlaceholder(variable_name=&quot;chat_history&quot;),
(&quot;user&quot;, &quot;{input}&quot;)
</code></pre><p>]) ####### MessagesPlaceholder(variable_name=&quot;chat_history&quot;)</p><figure><img src="`+M+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>实例化的MessagesPlaceholder</p><p>是一个Prompt模板，将变量chat_history看作是一个消息列表</p><h6 id="prompt-chatprompttemplate-from-template-answer-the-following-question-based-only-on-the-provided-context" tabindex="-1"><a class="header-anchor" href="#prompt-chatprompttemplate-from-template-answer-the-following-question-based-only-on-the-provided-context" aria-hidden="true">#</a> prompt = ChatPromptTemplate.from_template(&quot;&quot;&quot;Answer the following question based only on the provided context:</h6>',177),X=a('<p>Question: {input}&quot;&quot;&quot;) 也是调用的from_messages方法</p><figure><img src="'+P+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h6 id="chatprompttemplate" tabindex="-1"><a class="header-anchor" href="#chatprompttemplate" aria-hidden="true">#</a> ChatPromptTemplate</h6><p>####### from_messages ######## def from_messages( cls, messages: Sequence[MessageLikeRepresentation], ) -&gt; ChatPromptTemplate: &quot;&quot;&quot; Create a chat prompt template from a variety of message formats. &quot;&quot;&quot; _messages = [_convert_to_message(message) for message in messages]</p><pre><code>    # Automatically infer input variables from messages
    input_vars: Set[str] = set()
    for _message in _messages:
        if isinstance(
            _message, (BaseChatPromptTemplate, BaseMessagePromptTemplate)
        ):
            input_vars.update(_message.input_variables)

    return cls(input_variables=sorted(input_vars), messages=_messages)
</code></pre><p>######### _convert_to_message ########## 将多种格式的message转为 BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate 三种格式中的一种</p><p>def _convert_to_message( message: MessageLikeRepresentation, ) -&gt; Union[BaseMessage, BaseMessagePromptTemplate, BaseChatPromptTemplate]: &quot;&quot;&quot;Instantiate a message from a variety of message formats.</p><figure><img src="`+T+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+w+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+S+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+I+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>######### return cls(input_variables=sorted(input_vars), messages=_messages) ########## 这里的cls是一个类方法里面的特殊变量，它代指的是当前的类。 使用cls可以调用类的属性和方法，也可以实例化一个新的类对象。</p><p>创建一个当前类的实例并返回</p><p>####### 类的定义 class ChatPromptTemplate(BaseChatPromptTemplate): input_variables: List[str] &quot;&quot;&quot;List of input variables in template messages. Used for validation.&quot;&quot;&quot; messages: List[MessageLike] &quot;&quot;&quot;List of messages consisting of either message prompt templates or messages.&quot;&quot;&quot; validate_template: bool = False &quot;&quot;&quot;Whether or not to try validating the template.&quot;&quot;&quot;</p><h4 id="language-models-llm" tabindex="-1"><a class="header-anchor" href="#language-models-llm" aria-hidden="true">#</a> Language models, LLM</h4><h5 id="通过通用接口调用语言模型" tabindex="-1"><a class="header-anchor" href="#通过通用接口调用语言模型" aria-hidden="true">#</a> 通过通用接口调用语言模型</h5><p>可以将多个LLM结合使用</p><h5 id="llms" tabindex="-1"><a class="header-anchor" href="#llms" aria-hidden="true">#</a> LLMs</h5><h6 id="llm" tabindex="-1"><a class="header-anchor" href="#llm" aria-hidden="true">#</a> LLM</h6><p>####### OpenAI ######## GPT-4 https://openai.com/research/gpt-4</p><p>####### Google ######## PaLM https://palm.com</p><p>####### Meta ######## Llama 2 https://ai.meta.com/llama</p><p>...</p><p>Models that take a text string as input and return a text string(纯文本=&gt;纯文本)</p><h5 id="chat-models" tabindex="-1"><a class="header-anchor" href="#chat-models" aria-hidden="true">#</a> Chat models</h5><p>Models that are backed by a language model but take a list of Chat Messages as input and return a Chat Message</p><h6 id="chatmessage-chatmessage" tabindex="-1"><a class="header-anchor" href="#chatmessage-chatmessage" aria-hidden="true">#</a> ChatMessage=&gt;ChatMessage</h6><p>type: System, AI, Human</p><p>ChatMessage: type+text</p><p>Embeddings</p><h5 id="llm-1" tabindex="-1"><a class="header-anchor" href="#llm-1" aria-hidden="true">#</a> LLM</h5><h6 id="baselanguagemodel" tabindex="-1"><a class="header-anchor" href="#baselanguagemodel" aria-hidden="true">#</a> BaseLanguageModel</h6><p>####### 接口 ######## predict str=&gt;str</p><p>######## predict_messages List[BaseMessage]=&gt;BaseMessage</p><p>generate_prompt</p><p>####### BaseLLM ######## LLM ######### BaseOpenAI OpenAI</p><p>AzureOpenAI</p><p>AI21</p><p>QianfanLLMEndpoint</p><p>LlamaCpp</p><h5 id="chat-models-1" tabindex="-1"><a class="header-anchor" href="#chat-models-1" aria-hidden="true">#</a> chat_models</h5><h6 id="源码位置-1" tabindex="-1"><a class="header-anchor" href="#源码位置-1" aria-hidden="true">#</a> 源码位置</h6><p>基础的类源码在langchain_core的language_models的chat_models下</p><p>实现的类源码在langchain_community的chat_models下</p><p>所有的类都整合在langchain的chat_models下（基础的类整合在base.py中）</p><h6 id="经典-chatopenai" tabindex="-1"><a class="header-anchor" href="#经典-chatopenai" aria-hidden="true">#</a> 经典：ChatOpenAI</h6><figure><img src="'+O+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>####### llm = ChatOpenAI() llm.invoke(&quot;how can langsmith help with testing?&quot;) ######## 总结 ######### 实例化 实例化ChatOpenAI对象，使用@root_validator()功能，调用validate_environment函数初始化</p><p>######### invoke 1.对用户输入进行数据格式的封装 2.重试调用openai的completion补全API，获取结果（缓存获取中间结果和历史结果） 3.对结果进行解析和封装，返回结果</p><p>####### 实例化 @root_validator() def validate_environment(cls, values: Dict) -&gt; Dict: &quot;&quot;&quot;Validate that api key and python package exists in environment.&quot;&quot;&quot; if values[&quot;n&quot;] &lt; 1: raise ValueError(&quot;n must be at least 1.&quot;) if values[&quot;n&quot;] &gt; 1 and values[&quot;streaming&quot;]: raise ValueError(&quot;n must be 1 when streaming.&quot;)</p><pre><code>    values[&quot;openai_api_key&quot;] = get_from_dict_or_env(
        values, &quot;openai_api_key&quot;, &quot;OPENAI_API_KEY&quot;
    )
    # Check OPENAI_ORGANIZATION for backwards compatibility.
    values[&quot;openai_organization&quot;] = (
        values[&quot;openai_organization&quot;]
        or os.getenv(&quot;OPENAI_ORG_ID&quot;)
        or os.getenv(&quot;OPENAI_ORGANIZATION&quot;)
    )
    values[&quot;openai_api_base&quot;] = values[&quot;openai_api_base&quot;] or os.getenv(
        &quot;OPENAI_API_BASE&quot;
    )
    values[&quot;openai_proxy&quot;] = get_from_dict_or_env(
        values,
        &quot;openai_proxy&quot;,
        &quot;OPENAI_PROXY&quot;,
        default=&quot;&quot;,
    )
    try:
        import openai

    except ImportError:
        raise ImportError(
            &quot;Could not import openai python package. &quot;
            &quot;Please install it with \`pip install openai\`.&quot;
        )

    if is_openai_v1():
        client_params = {
            &quot;api_key&quot;: values[&quot;openai_api_key&quot;],
            &quot;organization&quot;: values[&quot;openai_organization&quot;],
            &quot;base_url&quot;: values[&quot;openai_api_base&quot;],
            &quot;timeout&quot;: values[&quot;request_timeout&quot;],
            &quot;max_retries&quot;: values[&quot;max_retries&quot;],
            &quot;default_headers&quot;: values[&quot;default_headers&quot;],
            &quot;default_query&quot;: values[&quot;default_query&quot;],
            &quot;http_client&quot;: values[&quot;http_client&quot;],
        }

        if not values.get(&quot;client&quot;):
            values[&quot;client&quot;] = openai.OpenAI(**client_params).chat.completions
        if not values.get(&quot;async_client&quot;):
            values[&quot;async_client&quot;] = openai.AsyncOpenAI(
                **client_params
            ).chat.completions
    elif not values.get(&quot;client&quot;):
        values[&quot;client&quot;] = openai.ChatCompletion
    else:
        pass
    return values
</code></pre><p>####### invoke ######## BaseChatModel ######### 1）invoke ########## <img src="`+z+'" alt="" loading="lazy"></p><p>########### 细节 1.调用generate_prompt方法-&gt;LLMResult</p><p>2.取LLMResult.generations[0][0]结果，类型为Generation，这里转为ChatGeneration</p><figure><img src="'+B+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>3.取ChatGeneration.message结果，类型为BaseMessage</p><p>########### 2）_convert_input</p><figure><img src="'+R+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>########### 3）generate_prompt ############ <img src="'+E+`" alt="" loading="lazy"></p><p>############# 4）generate ############## def generate( self, messages: List[List[BaseMessage]], stop: Optional[List[str]] = None, callbacks: Callbacks = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, **kwargs: Any, ) -&gt; LLMResult: &quot;&quot;&quot;Pass a sequence of prompts to the model and return model generations.</p><pre><code>    &quot;&quot;&quot;
    results = []
    for i, m in enumerate(messages):
        try:
            results.append(
                self._generate_with_cache(
                    m,
                    stop=stop,
                    run_manager=run_managers[i] if run_managers else None,
                    **kwargs,
                )
            )
        except BaseException as e:
            raise e
    llm_output = self._combine_llm_outputs([res.llm_output for res in results])
    generations = [res.generations for res in results]
    output = LLMResult(generations=generations, llm_output=llm_output) 
    return output
</code></pre><p>############### 5）_generate_with_cache def _generate_with_cache( self, messages: List[BaseMessage], stop: Optional[List[str]] = None, run_manager: Optional[CallbackManagerForLLMRun] = None, **kwargs: Any, ) -&gt; ChatResult: return self._generate(messages, stop=stop, **kwargs)</p><p>######## ChatOpenAI ######### 6）_generate ########## <img src="`+D+'" alt="" loading="lazy"></p><p>########### 7）_create_message_dicts ############ <img src="'+N+'" alt="" loading="lazy"></p><p>############# 8）convert_message_to_dict</p><figure><img src="'+V+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>########### 9）completion_with_retry ############ <img src="'+F+'" alt="" loading="lazy"></p><p>真正的调用openai的补全API</p><p>from openai import OpenAI</p><p>OpenAI().chat.completions.create(xxx)</p><p>疑问：self.client什么时候被赋值的</p><p>########### 10）_create_chat_result</p><figure><img src="'+K+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>######### 11）_combine_llm_outputs</p><figure><img src="'+W+`" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="output-parsers" tabindex="-1"><a class="header-anchor" href="#output-parsers" aria-hidden="true">#</a> Output parsers</h4><p>从模型中提取信息</p><p>List Parser</p><p>...</p><h5 id="basellmoutputparser-t" tabindex="-1"><a class="header-anchor" href="#basellmoutputparser-t" aria-hidden="true">#</a> BaseLLMOutputParser[T]</h5><h6 id="baseoutputparser" tabindex="-1"><a class="header-anchor" href="#baseoutputparser" aria-hidden="true">#</a> BaseOutputParser</h6><p>####### ListOutputParser str=&gt;List[str]</p><p>将输出格式化为逗号分隔的字符串列表</p><p>JSONAgentOutputParser</p><h5 id="output-parsers-1" tabindex="-1"><a class="header-anchor" href="#output-parsers-1" aria-hidden="true">#</a> output_parsers</h5><h6 id="源码位置-2" tabindex="-1"><a class="header-anchor" href="#源码位置-2" aria-hidden="true">#</a> 源码位置</h6><p>####### 源码 langchain_core的output_parsers下</p><p>langchain_core的outputs下</p><p>langchain_core的pydantic_v1下</p><p>所有的类都整合在langchain的output_parsers或schema或pydantic_v1下</p><h6 id="output-parser-stroutputparser" tabindex="-1"><a class="header-anchor" href="#output-parser-stroutputparser" aria-hidden="true">#</a> output_parser = StrOutputParser()</h6><p>实例化StrOutputParser</p><p>StrOutputParser</p><h3 id="chains-链-工作流模块" tabindex="-1"><a class="header-anchor" href="#chains-链-工作流模块" aria-hidden="true">#</a> Chains(链，工作流模块)</h3><h4 id="作用" tabindex="-1"><a class="header-anchor" href="#作用" aria-hidden="true">#</a> 作用</h4><p>Construct sequences of calls(构造调用序列)</p><h4 id="特点" tabindex="-1"><a class="header-anchor" href="#特点" aria-hidden="true">#</a> 特点</h4><ul><li><p>有状态（Stateful）：为任何 Chain 添加 Memory，即可为其赋予状态。</p></li><li><p>可观察（Observable）：向 Chain 传递 Callbacks，就执行主组件调用序列之外的附加功能，如日志记录。</p></li><li><p>可组合（Composable）：Chain API 足够灵活，便于将 Chain 与其他组件结合，包括其他 Chain。</p></li></ul><h4 id="chain-1" tabindex="-1"><a class="header-anchor" href="#chain-1" aria-hidden="true">#</a> Chain</h4><h5 id="属性" tabindex="-1"><a class="header-anchor" href="#属性" aria-hidden="true">#</a> 属性</h5><h6 id="memory" tabindex="-1"><a class="header-anchor" href="#memory" aria-hidden="true">#</a> memory</h6><p>在chain开始的时候组装inputs</p><p>在chain结束的时候保存inputs和outputs</p><h6 id="callbacks" tabindex="-1"><a class="header-anchor" href="#callbacks" aria-hidden="true">#</a> callbacks</h6><p>回调，插桩，可在chain执行的过程中插桩</p><h5 id="源码位置-3" tabindex="-1"><a class="header-anchor" href="#源码位置-3" aria-hidden="true">#</a> 源码位置</h5><p>所有的源码都在langchain的chains下</p><h5 id="接口" tabindex="-1"><a class="header-anchor" href="#接口" aria-hidden="true">#</a> 接口</h5><h6 id="invoke" tabindex="-1"><a class="header-anchor" href="#invoke" aria-hidden="true">#</a> invoke</h6><p>####### 源码 ######## def invoke( self, input: Dict[str, Any], config: Optional[RunnableConfig] = None, **kwargs: Any, ) -&gt; Dict[str, Any]: config = ensure_config(config) callbacks = config.get(&quot;callbacks&quot;) tags = config.get(&quot;tags&quot;) metadata = config.get(&quot;metadata&quot;) run_name = config.get(&quot;run_name&quot;) include_run_info = kwargs.get(&quot;include_run_info&quot;, False) return_only_outputs = kwargs.get(&quot;return_only_outputs&quot;, False)</p><pre><code>    inputs = self.prep_inputs(input) # 验证和准备chain的输入，包括添加来自memory的输入
    callback_manager = CallbackManager.configure(
        callbacks,
        self.callbacks,
        self.verbose,
        tags,
        self.tags,
        metadata,
        self.metadata,
    )
    new_arg_supported = inspect.signature(self._call).parameters.get(&quot;run_manager&quot;) # 看是否支持run_manager，即看_call的参数是否有run_manager
    run_manager = callback_manager.on_chain_start(
        dumpd(self),
        inputs,
        name=run_name,
    )
    try:
        outputs = (
            self._call(inputs, run_manager=run_manager) # chain的子类实现
            if new_arg_supported
            else self._call(inputs)
        )
    except BaseException as e:
        run_manager.on_chain_error(e)
        raise e
    run_manager.on_chain_end(outputs)
    final_outputs: Dict[str, Any] = self.prep_outputs( # 验证并准备chain的输出，并将此次运行的信息保存到memory中
        inputs, outputs, return_only_outputs
    )
    if include_run_info:
        final_outputs[RUN_KEY] = RunInfo(run_id=run_manager.run_id)
    return final_outputs
</code></pre><p>######### 逻辑 ########## prep_inputs ########### 准备输入参数 验证和准备chain的输入，包括添加来自memory的输入</p><p>########### 和memory.load_memory_variables组合组装新的inputs memory相关</p><p>########## ctx.on_chain_start callback相关</p><p>########## self._call 抽象方法，子类实现</p><p>########## ctx.on_chain_error callback相关</p><p>########## ctx.on_chain_end callback相关</p><p>########## prep_outputs ########### 准备输出结果 验证并准备chain的输出，并将此次运行的信息保存到memory中</p><p>########### memory.save_context ############ 将inputs, outputs保存到memory memory相关</p><p>####### 其他形式本质也是调用invoke ######## run 底层是__call__</p><p>######## <strong>call</strong> 底层是invoke</p><h5 id="实现" tabindex="-1"><a class="header-anchor" href="#实现" aria-hidden="true">#</a> 实现</h5><p>_call</p><h6 id="实现类" tabindex="-1"><a class="header-anchor" href="#实现类" aria-hidden="true">#</a> 实现类</h6><p>####### 所有的chain映射 type_to_loader_dict</p><p>APIChain</p><p>LLMChain</p><p>####### BaseQAWithSourcesChain 问答链</p><p>######## RetrievalQAWithSourcesChain 基于BaseRetriever实现问答</p><p>######## VectorDBQAWithSourcesChain 基于VectorStore实现问答</p><p>QAWithSourcesChain</p><h6 id="常见chain" tabindex="-1"><a class="header-anchor" href="#常见chain" aria-hidden="true">#</a> 常见Chain</h6><p>####### LLMChain 连接LLM</p><p>####### RouterChain 根据输入/上下文决定使用哪一条Chain、Prompt</p><p>####### SequentialChain ######## 串行执行Chain 单输入输出</p><p>多输入输出</p><p>####### StuffDocumentsChain 直接把内容填充到Document中</p><p>只适合文档比较小的，大文档就不适用了</p><p>RefineDocumentsChain</p><p>####### MapReduceDocumentsChain 将大文档分成小文档分别执行，然后合并结果</p><p>####### MapRerankDocumentsChain 适用于大文档的问答，会得到多个结果和分数，根据分数返回</p><h4 id="chains的使用" tabindex="-1"><a class="header-anchor" href="#chains的使用" aria-hidden="true">#</a> chains的使用</h4><p>chain = prompt | llm</p><p>chain = prompt | llm | output_parser</p><p>document_chain = create_stuff_documents_chain(llm, prompt)</p><p>retrieval_chain = create_retrieval_chain(retriever, document_chain)</p><p>retriever_chain = create_history_aware_retriever(llm, retriever, prompt)</p><p>chain.invoke({&quot;input&quot;: &quot;how can langsmith help with testing?&quot;})</p><p>response = retrieval_chain.invoke({&quot;input&quot;: &quot;how can langsmith help with testing?&quot;})</p><p>retriever_chain.invoke({ &quot;chat_history&quot;: chat_history, &quot;input&quot;: &quot;Tell me how&quot; })</p><h3 id="agentexecutor-agents模块-代理模块" tabindex="-1"><a class="header-anchor" href="#agentexecutor-agents模块-代理模块" aria-hidden="true">#</a> AgentExecutor(Agents模块，代理模块)</h3><p>AgentExecutor=Agent+Memory+Tools</p><h4 id="agentexecutor-call" tabindex="-1"><a class="header-anchor" href="#agentexecutor-call" aria-hidden="true">#</a> AgentExecutor._call</h4><h5 id="源码流程图" tabindex="-1"><a class="header-anchor" href="#源码流程图" aria-hidden="true">#</a> 源码流程图</h5><figure><img src="`+G+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="迭代器" tabindex="-1"><a class="header-anchor" href="#迭代器" aria-hidden="true">#</a> 迭代器</h5><h6 id="停止条件" tabindex="-1"><a class="header-anchor" href="#停止条件" aria-hidden="true">#</a> 停止条件</h6><p>####### iterations&gt;=self.max_iterations 迭代次数限制</p><p>####### time_elapsed&gt;=self.max_execution_time 迭代时间限制</p><p>####### isinstance(next_step_output, AgentFinish) 停止信号AgentFinish</p><h6 id="迭代函数-take-next-step" tabindex="-1"><a class="header-anchor" href="#迭代函数-take-next-step" aria-hidden="true">#</a> 迭代函数_take_next_step</h6><p>####### 逻辑 ######## _prepare_intermediate_steps 预处理参数</p><p>######## agent.plan ######### 抽象方法，在具体的Agent里实现 执行agent的plan方法, 得到AgentAction列表</p><p>######## tool 根据AgentAction得到tool或者tool不存在</p><p>######### 合法tool tool.run</p><p>######### tool不存在 InvaildTool</p><p>######## result 将tool的执行结果append到result返回</p><p>####### _take_next_step源码 ######## <img src="'+U+`" alt="" loading="lazy"></p><p>######### self._iter_next_step ########## 1.调LLM生成一个计划plan plan要么是AgentFinish，要么是AgentAction</p><p>2.如果是AgentFinish则返回，结束</p><p>3.如果是AgentAction则调对应的工具</p><p>4.调对应工具生成observation</p><p>5.生成并返回AgentStep(action=agent_action, observation=observation)，结束</p><p>######### self._consume_next_step ########## 返回Union[AgentFinish, List[Tuple[AgentAction, str]]] 如果是AgentFinish，直接返回</p><p>########### 如果是AgentStep，则转换为List[Tuple[AgentAction,str]]后返回 return [ (a.action, a.observation) for a in values if isinstance(a, AgentStep) ]</p><h5 id="call源码" tabindex="-1"><a class="header-anchor" href="#call源码" aria-hidden="true">#</a> _call源码</h5><pre><code>def _call(
    self,
    inputs: Dict[str, str],
    run_manager: Optional[CallbackManagerForChainRun] = None,
) -&gt; Dict[str, Any]:
    &quot;&quot;&quot;Run text through and get agent response.&quot;&quot;&quot;
    # Construct a mapping of tool name to tool for easy lookup
    name_to_tool_map = {tool.name: tool for tool in self.tools}
    # We construct a mapping from each tool to a color, used for logging.
    color_mapping = get_color_mapping(
        [tool.name for tool in self.tools], excluded_colors=[&quot;green&quot;, &quot;red&quot;]
    )
    intermediate_steps: List[Tuple[AgentAction, str]] = []
    # Let&#39;s start tracking the number of iterations and time elapsed
    iterations = 0
    time_elapsed = 0.0
    start_time = time.time()
    # We now enter the agent loop (until it returns something).
    while self._should_continue(iterations, time_elapsed):
        next_step_output = self._take_next_step(
            name_to_tool_map,
            color_mapping,
            inputs,
            intermediate_steps,
            run_manager=run_manager,
        )
        if isinstance(next_step_output, AgentFinish):
            return self._return(
                next_step_output, intermediate_steps, run_manager=run_manager
            )

        intermediate_steps.extend(next_step_output)
        if len(next_step_output) == 1:
            next_step_action = next_step_output[0]
            # See if tool should return directly
            tool_return = self._get_tool_return(next_step_action)
            if tool_return is not None:
                return self._return(
                    tool_return, intermediate_steps, run_manager=run_manager
                )
        iterations += 1
        time_elapsed = time.time() - start_time
    output = self.agent.return_stopped_response(
        self.early_stopping_method, intermediate_steps, **inputs
    )
    return self._return(output, intermediate_steps, run_manager=run_manager)
</code></pre><h4 id="agent" tabindex="-1"><a class="header-anchor" href="#agent" aria-hidden="true">#</a> Agent</h4><h5 id="initialize-agent" tabindex="-1"><a class="header-anchor" href="#initialize-agent" aria-hidden="true">#</a> initialize_agent</h5><p>工厂函数，构造AgentExecutor</p><h5 id="内置类型" tabindex="-1"><a class="header-anchor" href="#内置类型" aria-hidden="true">#</a> 内置类型</h5><h6 id="agent-to-class" tabindex="-1"><a class="header-anchor" href="#agent-to-class" aria-hidden="true">#</a> AGENT TO CLASS</h6><p>一个map, 映射agent type字符串到class</p><h6 id="zero-shot-react-description" tabindex="-1"><a class="header-anchor" href="#zero-shot-react-description" aria-hidden="true">#</a> zero-shot-react-description</h6><p>根据工具的描述来确定要使用的工具，可以提供任意数量的工具。要求为每个工具提供说明</p><p>最通用的操作代理</p><p>A zero shot agent that does a reasoning step before acting</p><p>ZeroShotAgent</p><h6 id="react-docstore" tabindex="-1"><a class="header-anchor" href="#react-docstore" aria-hidden="true">#</a> react-docstore</h6><p>####### 只允许存在两个工具，并且工具名必须为Lookup和Search Search:从文档库中检索相关的页面</p><p>Lookup:从检索出的相关页面中，查找相关的具体内容</p><p>使用传统的检索方法来存储和检索信息</p><p>ReActDocstoreAgent</p><h6 id="self-ask-with-search" tabindex="-1"><a class="header-anchor" href="#self-ask-with-search" aria-hidden="true">#</a> self-ask-with-search</h6><p>根据需要执行搜索并问一些进一步的问题，以获得最终的答案</p><p>SelfAskDocstoreAgent</p><h6 id="conversational-react-description" tabindex="-1"><a class="header-anchor" href="#conversational-react-description" aria-hidden="true">#</a> conversational-react-description</h6><p>拥有记忆能力</p><p>ConversationalAgent</p><h6 id="chat-zero-shot-react-description" tabindex="-1"><a class="header-anchor" href="#chat-zero-shot-react-description" aria-hidden="true">#</a> chat-zero-shot-react-description</h6><p>ChatAgent</p><h6 id="chat-conversational-react-description" tabindex="-1"><a class="header-anchor" href="#chat-conversational-react-description" aria-hidden="true">#</a> chat-conversational-react-description</h6><p>ConversationalChatAgent</p><h6 id="structured-chat-zero-shot-react-description" tabindex="-1"><a class="header-anchor" href="#structured-chat-zero-shot-react-description" aria-hidden="true">#</a> structured-chat-zero-shot-react-description</h6><p>StructuredChatAgent</p><h6 id="openai-functions" tabindex="-1"><a class="header-anchor" href="#openai-functions" aria-hidden="true">#</a> openai-functions</h6><p>OpenAIFunctionsAgent</p><h6 id="openai-multi-functions" tabindex="-1"><a class="header-anchor" href="#openai-multi-functions" aria-hidden="true">#</a> openai-multi-functions</h6><p>OpenAIMultiFunctionsAgent</p><h5 id="output-parser" tabindex="-1"><a class="header-anchor" href="#output-parser" aria-hidden="true">#</a> output parser</h5><h6 id="agentoutputparser" tabindex="-1"><a class="header-anchor" href="#agentoutputparser" aria-hidden="true">#</a> AgentOutputParser</h6><p>OpenAIFunctionAgentOutputParser</p><figure><img src="`+H+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="核心方法-plan方法" tabindex="-1"><a class="header-anchor" href="#核心方法-plan方法" aria-hidden="true">#</a> 核心方法：plan方法</h5><h6 id="llmsingleactionagent的实现" tabindex="-1"><a class="header-anchor" href="#llmsingleactionagent的实现" aria-hidden="true">#</a> LLMSingleActionAgent的实现</h6><p>####### <img src="'+Q+'" alt="" loading="lazy"></p><p>######## 走Chain的invoke方法 -&gt; LLMChain的_call方法</p><figure><img src="'+Y+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="agents-代理模块" tabindex="-1"><a class="header-anchor" href="#agents-代理模块" aria-hidden="true">#</a> Agents(代理模块)</h5><h6 id="有些时候-需要根据用户的输入动态选择要调用的llm或者其他工具-tools-抽象为agent" tabindex="-1"><a class="header-anchor" href="#有些时候-需要根据用户的输入动态选择要调用的llm或者其他工具-tools-抽象为agent" aria-hidden="true">#</a> 有些时候，需要根据用户的输入动态选择要调用的LLM或者其他工具(Tools)，抽象为Agent</h6><p>代理模块集成了各类工具，由LLM决定要不要使用工具以及使用什么工具</p><h6 id="planning-规划" tabindex="-1"><a class="header-anchor" href="#planning-规划" aria-hidden="true">#</a> Planning(规划)</h6><p>####### Task Decomposition ######## Chain-of-Thought https://arxiv.org/abs/2201.11903</p><p>######## Tree of Thoughts https://arxiv.org/abs/2305.10601</p><p>####### Self-Reflection ######## ReAct https://react-lm.github.io</p><p>######## Reflexion https://arxiv.org/abs/2303.11366</p><p>######## Chain of Hindsight https://arxiv.org/abs/2302.02676</p><h6 id="memory-记忆" tabindex="-1"><a class="header-anchor" href="#memory-记忆" aria-hidden="true">#</a> Memory(记忆)</h6><p>Memory类型(Types of Memory)</p><p>MIPS(Maximum Inner Product Search)</p><p>Tools(工具)</p><h5 id="basesingleactionagent" tabindex="-1"><a class="header-anchor" href="#basesingleactionagent" aria-hidden="true">#</a> BaseSingleActionAgent</h5><h6 id="接口-1" tabindex="-1"><a class="header-anchor" href="#接口-1" aria-hidden="true">#</a> 接口</h6><p>plan</p><p>Agent</p><h4 id="memory-记忆模块" tabindex="-1"><a class="header-anchor" href="#memory-记忆模块" aria-hidden="true">#</a> Memory(记忆模块)</h4><p>记忆模块，记录对话，支持存储用户与LLM交互的上下文，以便优化后续的交互，是一个Key-Value存储。包括局部存储和全局存储(全局存储是可供不同chain共享的)</p><h5 id="补充历史上下文" tabindex="-1"><a class="header-anchor" href="#补充历史上下文" aria-hidden="true">#</a> 补充历史上下文</h5><p>Persist application state between runs of a chain</p><p>We call this ability to store information about past interactions &quot;memory&quot;</p><h5 id="常见memory" tabindex="-1"><a class="header-anchor" href="#常见memory" aria-hidden="true">#</a> 常见Memory</h5><h6 id="conversationbuffermemory" tabindex="-1"><a class="header-anchor" href="#conversationbuffermemory" aria-hidden="true">#</a> ConversationBufferMemory</h6><p>基于历史对话</p><h6 id="conversationbufferwindowmemory" tabindex="-1"><a class="header-anchor" href="#conversationbufferwindowmemory" aria-hidden="true">#</a> ConversationBufferWindowMemory</h6><p>基于最后k个对话</p><h6 id="conversationentitymemory" tabindex="-1"><a class="header-anchor" href="#conversationentitymemory" aria-hidden="true">#</a> ConversationEntityMemory</h6><p>从对话构建实体有关信息，基于实体信息</p><h6 id="conversationkgmemory" tabindex="-1"><a class="header-anchor" href="#conversationkgmemory" aria-hidden="true">#</a> ConversationKGMemory</h6><p>基于知识图谱</p><h6 id="conversationsummarymemory" tabindex="-1"><a class="header-anchor" href="#conversationsummarymemory" aria-hidden="true">#</a> ConversationSummaryMemory</h6><p>####### 对历史会话进行总结 优点是数据量可控，适合长对话</p><p>缺点是总结会消耗Token</p><h6 id="conversationsummarybuffermemory" tabindex="-1"><a class="header-anchor" href="#conversationsummarybuffermemory" aria-hidden="true">#</a> ConversationSummaryBufferMemory</h6><p>保留最近K个会话，并且会对会话进行总结</p><h6 id="conversationtokenbuffermemory" tabindex="-1"><a class="header-anchor" href="#conversationtokenbuffermemory" aria-hidden="true">#</a> ConversationTokenBufferMemory</h6><p>保留最后N个Token</p><h6 id="vectorstoreretrievermemory" tabindex="-1"><a class="header-anchor" href="#vectorstoreretrievermemory" aria-hidden="true">#</a> VectorStoreRetrieverMemory</h6><p>和向量数据库结合，基于相关度最高的K个Document</p><h5 id="basememory" tabindex="-1"><a class="header-anchor" href="#basememory" aria-hidden="true">#</a> BaseMemory</h5><p>memory是chain的状态，可以存储和获取chain的上下文</p><h6 id="接口-2" tabindex="-1"><a class="header-anchor" href="#接口-2" aria-hidden="true">#</a> 接口</h6><p>####### memory_variables 支持的变量key列表</p><p>####### load_memory_variables 加载已有的数据</p><p>####### save_context 将input和output保存到chain</p><p>####### clear 清除状态</p><p>实现</p><h4 id="tool" tabindex="-1"><a class="header-anchor" href="#tool" aria-hidden="true">#</a> Tool</h4><h5 id="basetool" tabindex="-1"><a class="header-anchor" href="#basetool" aria-hidden="true">#</a> BaseTool</h5><h6 id="接口-3" tabindex="-1"><a class="header-anchor" href="#接口-3" aria-hidden="true">#</a> 接口</h6><p>####### description Agent主要根据description来判断接下来将要采用哪个工具来执行后续的操作</p><p>这个描述准确比较重要</p><p>####### _run Tool的执行逻辑</p><p>是抽象方法，需要用户自行实现</p><p>####### run 调用的_run</p><h6 id="实现-1" tabindex="-1"><a class="header-anchor" href="#实现-1" aria-hidden="true">#</a> 实现</h6><p>JiraAction</p><p>GmailSearch</p><p>...</p><h5 id="basetoolkit" tabindex="-1"><a class="header-anchor" href="#basetoolkit" aria-hidden="true">#</a> BaseToolKit</h5><p>可以理解为Tool的列表</p><h6 id="接口-4" tabindex="-1"><a class="header-anchor" href="#接口-4" aria-hidden="true">#</a> 接口</h6><p>####### get_tools 返回BaseTool列表</p><h6 id="实现-2" tabindex="-1"><a class="header-anchor" href="#实现-2" aria-hidden="true">#</a> 实现</h6><p>####### VectorStoreToolkit ######## 包含的Tool VectorStoreQATool</p><p>VectorStoreQAWithSourcesTool</p><p>######## 其他 基于VectorStore实现</p><h4 id="哥哥谈生产级的智能体应该具备哪些能力" tabindex="-1"><a class="header-anchor" href="#哥哥谈生产级的智能体应该具备哪些能力" aria-hidden="true">#</a> 哥哥谈生产级的智能体应该具备哪些能力</h4><h5 id="llm-2" tabindex="-1"><a class="header-anchor" href="#llm-2" aria-hidden="true">#</a> LLM</h5><h6 id="文本" tabindex="-1"><a class="header-anchor" href="#文本" aria-hidden="true">#</a> 文本</h6><p>Answer</p><p>Function Call</p><p>多模态</p><h5 id="prompt-1" tabindex="-1"><a class="header-anchor" href="#prompt-1" aria-hidden="true">#</a> Prompt</h5><h6 id="自定义" tabindex="-1"><a class="header-anchor" href="#自定义" aria-hidden="true">#</a> 自定义</h6><p>####### Template(脚本) variables</p><p>fixed</p><p>placeholder</p><p>expression</p><p>运行时</p><h5 id="lora-model-params" tabindex="-1"><a class="header-anchor" href="#lora-model-params" aria-hidden="true">#</a> LORA/Model Params</h5><h6 id="lora" tabindex="-1"><a class="header-anchor" href="#lora" aria-hidden="true">#</a> LORA</h6><p>是专门领域的LLM</p><h6 id="model-params" tabindex="-1"><a class="header-anchor" href="#model-params" aria-hidden="true">#</a> Model Params</h6><p>LLM可调节的参数</p><h5 id="tools" tabindex="-1"><a class="header-anchor" href="#tools" aria-hidden="true">#</a> Tools</h5><h6 id="apis" tabindex="-1"><a class="header-anchor" href="#apis" aria-hidden="true">#</a> APIs</h6><p>调API的</p><h6 id="retriever-tool" tabindex="-1"><a class="header-anchor" href="#retriever-tool" aria-hidden="true">#</a> retriever_tool</h6><p>把检索知识库的能力作为一个工具</p><p>Environments(极轻量容器)</p><h5 id="knowledge-base-知识库-所有存起来的数据" tabindex="-1"><a class="header-anchor" href="#knowledge-base-知识库-所有存起来的数据" aria-hidden="true">#</a> Knowledge Base(知识库，所有存起来的数据)</h5><p>知识图谱</p><h6 id="数据库" tabindex="-1"><a class="header-anchor" href="#数据库" aria-hidden="true">#</a> 数据库</h6><p>MongoDB</p><p>####### K-V数据库 Redis</p><p>向量数据库</p><p>Retriever</p><p>文档切片</p><p>Embedding模型</p><h5 id="retriever" tabindex="-1"><a class="header-anchor" href="#retriever" aria-hidden="true">#</a> Retriever</h5><h6 id="问题" tabindex="-1"><a class="header-anchor" href="#问题" aria-hidden="true">#</a> 问题</h6><p>####### q1 ######## 将user_query改写 model1</p><p>####### q2 ######## 需求文档/tool description model2</p><h6 id="资源-向量-高维空间" tabindex="-1"><a class="header-anchor" href="#资源-向量-高维空间" aria-hidden="true">#</a> 资源(向量，高维空间)</h6><p>####### KB Unit(Knowledge Base Unit) embedding model</p><p>####### Tool API embedding model</p><h6 id="匹配算法" tabindex="-1"><a class="header-anchor" href="#匹配算法" aria-hidden="true">#</a> 匹配算法</h6><p>####### network q1+q2</p><p>####### network KB Unit + Tool API</p><p>算加权分：相邻匹配和es最长子序列</p><p>Recall=&gt;粗排=&gt;精排</p><h6 id="结果" tabindex="-1"><a class="header-anchor" href="#结果" aria-hidden="true">#</a> 结果</h6><p>拿到与user_query,prompt需要的KB Unit/Tool API集合（一般5个）</p><h5 id="code" tabindex="-1"><a class="header-anchor" href="#code" aria-hidden="true">#</a> Code</h5><p>LLM生成前后流程控制</p><p>Callback</p><h5 id="agent-orchestration-agent编排" tabindex="-1"><a class="header-anchor" href="#agent-orchestration-agent编排" aria-hidden="true">#</a> Agent Orchestration(Agent编排)</h5><p>将复杂任务拆解（LLM可能要运行成千上万次，如：帮我写一个能跑起来的网站）</p><h6 id="类似game-engine" tabindex="-1"><a class="header-anchor" href="#类似game-engine" aria-hidden="true">#</a> 类似Game Engine</h6><p>####### 两种实现方式 ######## Graph LangGraph</p><p>AgentFlow</p><p>######## Language SGLang</p><p>ProAgent</p><h5 id="hierarchy-memory-多级memory" tabindex="-1"><a class="header-anchor" href="#hierarchy-memory-多级memory" aria-hidden="true">#</a> Hierarchy Memory(多级Memory)</h5><h6 id="context-prompt" tabindex="-1"><a class="header-anchor" href="#context-prompt" aria-hidden="true">#</a> Context Prompt</h6><p>作为input的一部分传入</p><p>Messages</p><h6 id="history-memory" tabindex="-1"><a class="header-anchor" href="#history-memory" aria-hidden="true">#</a> History/Memory</h6><p>存储到向量数据库中进行检索</p><p>1.积累 2.裁剪/摘要</p><h6 id="分类" tabindex="-1"><a class="header-anchor" href="#分类" aria-hidden="true">#</a> 分类</h6><p>####### 有timeline的 时间胶囊</p><p>####### 无timeline的 存KB Unit再组织</p><h5 id="贯穿全流程的能力" tabindex="-1"><a class="header-anchor" href="#贯穿全流程的能力" aria-hidden="true">#</a> 贯穿全流程的能力</h5><h6 id="计费" tabindex="-1"><a class="header-anchor" href="#计费" aria-hidden="true">#</a> 计费</h6><p>各类型token数*单价</p><p>Tool Api调用次数*单价</p><p>memory占空间大小费用</p><h6 id="a-b实验" tabindex="-1"><a class="header-anchor" href="#a-b实验" aria-hidden="true">#</a> A/B实验</h6><p>####### tags 全链路trace追踪埋点+报表</p><h6 id="feedback" tabindex="-1"><a class="header-anchor" href="#feedback" aria-hidden="true">#</a> Feedback</h6><p>将KB Unit 和 Tool API的调用结果总结调用经验到description中</p><h6 id="observability-可观测性" tabindex="-1"><a class="header-anchor" href="#observability-可观测性" aria-hidden="true">#</a> Observability(可观测性)</h6><p>####### PlayGround（可观测可调试平台） 调试</p><p>####### 参考浏览器event lines 每个具体event按时间线，或串行或并行排列</p><p>######## 每个具体的event指 一次agent call</p><p>一次llm调用</p><p>一次callback回调</p><p>####### 传统互联网 log</p><p>metric</p><p>trace</p><h3 id="retrieval-检索模块-集成数据源模块" tabindex="-1"><a class="header-anchor" href="#retrieval-检索模块-集成数据源模块" aria-hidden="true">#</a> Retrieval(检索模块/集成数据源模块)</h3><h4 id="baseloader" tabindex="-1"><a class="header-anchor" href="#baseloader" aria-hidden="true">#</a> BaseLoader</h4><p>Document加载器</p><h5 id="方法" tabindex="-1"><a class="header-anchor" href="#方法" aria-hidden="true">#</a> 方法</h5><h6 id="load" tabindex="-1"><a class="header-anchor" href="#load" aria-hidden="true">#</a> load</h6><p>List[Document]</p><h6 id="load-and-split" tabindex="-1"><a class="header-anchor" href="#load-and-split" aria-hidden="true">#</a> load_and_split</h6><p>List[Document]</p><h6 id="lazy-load" tabindex="-1"><a class="header-anchor" href="#lazy-load" aria-hidden="true">#</a> lazy_load</h6><p>Iterator[Document]</p><h5 id="子类" tabindex="-1"><a class="header-anchor" href="#子类" aria-hidden="true">#</a> 子类</h5><h6 id="textloader" tabindex="-1"><a class="header-anchor" href="#textloader" aria-hidden="true">#</a> TextLoader</h6><p>加载文本数据</p><h6 id="recursiveurlloader" tabindex="-1"><a class="header-anchor" href="#recursiveurlloader" aria-hidden="true">#</a> RecursiveUrlLoader</h6><p>从链接加载</p><h6 id="webbaseloader" tabindex="-1"><a class="header-anchor" href="#webbaseloader" aria-hidden="true">#</a> WebBaseLoader</h6><p>用urllib加载网页</p><p>BeautifulSoup解析网页</p><p>CSVLoader</p><p>DirectoryLoader</p><p>JSONLoader</p><h6 id="unstructuredbaseloader" tabindex="-1"><a class="header-anchor" href="#unstructuredbaseloader" aria-hidden="true">#</a> UnstructuredBaseLoader</h6><p>####### UnstructuredFileLoader UnstructuredHTMLLoader</p><p>UnstructuredMarkdownLoader</p><h4 id="document" tabindex="-1"><a class="header-anchor" href="#document" aria-hidden="true">#</a> Document</h4><p>文本+meta信息</p><h4 id="basedocumenttransformer" tabindex="-1"><a class="header-anchor" href="#basedocumenttransformer" aria-hidden="true">#</a> BaseDocumentTransformer</h4><p>Document转换器</p><h5 id="方法-1" tabindex="-1"><a class="header-anchor" href="#方法-1" aria-hidden="true">#</a> 方法</h5><p>transform_documents</p><h5 id="textsplitter" tabindex="-1"><a class="header-anchor" href="#textsplitter" aria-hidden="true">#</a> TextSplitter</h5><p>文本分割</p><h6 id="方法-2" tabindex="-1"><a class="header-anchor" href="#方法-2" aria-hidden="true">#</a> 方法</h6><p>####### create_documents List[Document]</p><p>####### split_text List[str]</p><p>####### split_documents List[Document]</p><h6 id="子类-1" tabindex="-1"><a class="header-anchor" href="#子类-1" aria-hidden="true">#</a> 子类</h6><p>RecursiveCharacterTextSplitter</p><p>####### CharacterTextSplitter 按字符拆分</p><p>####### NLTKTextSplitter 使用NLTK算法按自然语言拆分</p><p>SentenceTransformersTokenTextSplitter</p><h4 id="embeddings" tabindex="-1"><a class="header-anchor" href="#embeddings" aria-hidden="true">#</a> Embeddings</h4><p>向量表示</p><h5 id="方法-3" tabindex="-1"><a class="header-anchor" href="#方法-3" aria-hidden="true">#</a> 方法</h5><h6 id="embed-documents" tabindex="-1"><a class="header-anchor" href="#embed-documents" aria-hidden="true">#</a> embed_documents</h6><p>List[str]=&gt;List[List[float]]</p><p>批量执行</p><h6 id="embed-query" tabindex="-1"><a class="header-anchor" href="#embed-query" aria-hidden="true">#</a> embed_query</h6><p>str=&gt;List[float]</p><p>将文本向量化</p><h5 id="qianfanembeddingsendpoint" tabindex="-1"><a class="header-anchor" href="#qianfanembeddingsendpoint" aria-hidden="true">#</a> QianfanEmbeddingsEndpoint</h5><p>百度千帆</p><h4 id="vectorstore" tabindex="-1"><a class="header-anchor" href="#vectorstore" aria-hidden="true">#</a> VectorStore</h4><p>向量数据库</p><h5 id="方法-4" tabindex="-1"><a class="header-anchor" href="#方法-4" aria-hidden="true">#</a> 方法</h5><h6 id="add-texts" tabindex="-1"><a class="header-anchor" href="#add-texts" aria-hidden="true">#</a> add_texts</h6><p>iterable[str]=&gt;List[str]</p><p>添加数据</p><h6 id="delete" tabindex="-1"><a class="header-anchor" href="#delete" aria-hidden="true">#</a> delete</h6><p>通过vector ID删除数据</p><h6 id="add-documents" tabindex="-1"><a class="header-anchor" href="#add-documents" aria-hidden="true">#</a> add_documents</h6><p>List[Document]=&gt;List[str]</p><h6 id="search" tabindex="-1"><a class="header-anchor" href="#search" aria-hidden="true">#</a> search</h6><p>搜索数据</p><p>str=&gt;List[Document]</p><h5 id="实现-3" tabindex="-1"><a class="header-anchor" href="#实现-3" aria-hidden="true">#</a> 实现</h5><h6 id="alibabacloudopensearch" tabindex="-1"><a class="header-anchor" href="#alibabacloudopensearch" aria-hidden="true">#</a> AlibabaCloudOpenSearch</h6><p>阿里云</p><p>Chroma</p><p>...</p><h4 id="baseretriever" tabindex="-1"><a class="header-anchor" href="#baseretriever" aria-hidden="true">#</a> BaseRetriever</h4><p>检索器</p><h4 id="retrieval-集成数据源模块-检索模块" tabindex="-1"><a class="header-anchor" href="#retrieval-集成数据源模块-检索模块" aria-hidden="true">#</a> Retrieval(集成数据源模块/检索模块)</h4><p>Note:发音[吹]</p><h5 id="interface-with-application-specific-data" tabindex="-1"><a class="header-anchor" href="#interface-with-application-specific-data" aria-hidden="true">#</a> Interface with application-specific data</h5><p>集成其他数据源的抽象</p><p>包括加载数据，数据向量化、向量数据的存储和获取</p><h5 id="document-loaders-数据加载" tabindex="-1"><a class="header-anchor" href="#document-loaders-数据加载" aria-hidden="true">#</a> Document loaders(数据加载)</h5><p>数据源加载器，包含文本和元数据 (A document is a piece of text and associated metadata)</p><h6 id="常见loaders" tabindex="-1"><a class="header-anchor" href="#常见loaders" aria-hidden="true">#</a> 常见loaders</h6><p>CSV</p><p>File Directory</p><p>HTML</p><p>JSON</p><p>Markdown</p><p>PDF</p><h5 id="document-transformers-数据处理" tabindex="-1"><a class="header-anchor" href="#document-transformers-数据处理" aria-hidden="true">#</a> Document transformers(数据处理)</h5><h6 id="将加载的document进行进一步处理-以便更好的来使用" tabindex="-1"><a class="header-anchor" href="#将加载的document进行进一步处理-以便更好的来使用" aria-hidden="true">#</a> 将加载的Document进行进一步处理，以便更好的来使用</h6><p>(Once you&#39;ve loaded documents, you&#39;ll often want to transform them to better suit your application) Text splitters</p><p>Post retrieval</p><h5 id="text-embedding-models" tabindex="-1"><a class="header-anchor" href="#text-embedding-models" aria-hidden="true">#</a> Text embedding models</h5><p>向量化</p><h5 id="vector-stores-向量存储" tabindex="-1"><a class="header-anchor" href="#vector-stores-向量存储" aria-hidden="true">#</a> Vector stores(向量存储)</h5><p>存储和搜索非结构化数据的最常见方式之一是将其向量化 (One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors)</p><h6 id="常见向量数据库" tabindex="-1"><a class="header-anchor" href="#常见向量数据库" aria-hidden="true">#</a> 常见向量数据库</h6><p>####### Chroma https://www.trychroma.com</p><p>https://github.com/chroma-core/chroma</p><p>####### Pinecone https://www.pinecone.io</p><p>####### Weaviate https://weaviate.io</p><p>####### Milvus https://milvus.io</p><p>####### Faiss https://faiss.ai</p><h5 id="retrievers-检索器" tabindex="-1"><a class="header-anchor" href="#retrievers-检索器" aria-hidden="true">#</a> Retrievers(检索器)</h5><p>A retriever is an interface that returns documents given an unstructured query</p><p>将Document和Language models结合</p><p>上一节的Vector stores可以作为Retrievers的backbone(骨干, 支撑)，但是也有其他的实现</p><h3 id="callbacks-回调模块" tabindex="-1"><a class="header-anchor" href="#callbacks-回调模块" aria-hidden="true">#</a> Callbacks(回调模块)</h3><p>切面(Aspect)/钩子(Hook)/插桩</p><p>Log and stream intermediate steps of any chain</p><p>允许在使用Langchain构造应用的任意阶段插入用户自定义的Callback。</p><p>计费，流速控制，日志</p><h4 id="callbackmanager" tabindex="-1"><a class="header-anchor" href="#callbackmanager" aria-hidden="true">#</a> CallbackManager</h4><p>on_llm_start</p><p>on_chat_model_start</p><p>on_chain_start</p><p>on_tool_start</p><p>on_retriever_start</p>',487);function Z($,ee){const t=r("context");return i(),o("div",null,[j,s(t,null,{default:p(()=>[h(" {context} ")]),_:1}),X])}const ne=n(J,[["render",Z],["__file","langchain.html.vue"]]);export{ne as default};
