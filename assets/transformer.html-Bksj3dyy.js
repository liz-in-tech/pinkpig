const e=JSON.parse('{"key":"v-a54f7fc6","path":"/llm/llm/transformer.html","title":"Transformer源码解读","lang":"en-US","frontmatter":{"icon":"lightbulb","date":"2024-05-24T00:00:00.000Z","category":["LLM"],"tag":["LLM"],"description":"Transformer源码解读 About 模型总体架构 超参数 张量维度转换 可训练参数量 源码","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/llm/transformer.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Transformer源码解读"}],["meta",{"property":"og:description","content":"Transformer源码解读 About 模型总体架构 超参数 张量维度转换 可训练参数量 源码"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-10-29T16:42:17.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2024-05-24T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-10-29T16:42:17.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Transformer源码解读\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-05-24T00:00:00.000Z\\",\\"dateModified\\":\\"2024-10-29T16:42:17.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. About","slug":"_1-about","link":"#_1-about","children":[]},{"level":2,"title":"2. 模型总体架构","slug":"_2-模型总体架构","link":"#_2-模型总体架构","children":[]},{"level":2,"title":"3. 超参数","slug":"_3-超参数","link":"#_3-超参数","children":[]},{"level":2,"title":"4. 张量维度转换","slug":"_4-张量维度转换","link":"#_4-张量维度转换","children":[]},{"level":2,"title":"5. 可训练参数量","slug":"_5-可训练参数量","link":"#_5-可训练参数量","children":[{"level":3,"title":"5.1. MultiHeadedAttention","slug":"_5-1-multiheadedattention","link":"#_5-1-multiheadedattention","children":[]},{"level":3,"title":"5.2. PositionwiseFeedForward","slug":"_5-2-positionwisefeedforward","link":"#_5-2-positionwisefeedforward","children":[]},{"level":3,"title":"5.3. LayerNorm","slug":"_5-3-layernorm","link":"#_5-3-layernorm","children":[]},{"level":3,"title":"5.4. Embeddings","slug":"_5-4-embeddings","link":"#_5-4-embeddings","children":[]},{"level":3,"title":"5.5. 总的可训练参数量","slug":"_5-5-总的可训练参数量","link":"#_5-5-总的可训练参数量","children":[]}]},{"level":2,"title":"6. 源码","slug":"_6-源码","link":"#_6-源码","children":[{"level":3,"title":"6.1. 完整模型","slug":"_6-1-完整模型","link":"#_6-1-完整模型","children":[]},{"level":3,"title":"6.2. EncoderDecoder（编码器-解码器结构）","slug":"_6-2-encoderdecoder-编码器-解码器结构","link":"#_6-2-encoderdecoder-编码器-解码器结构","children":[]},{"level":3,"title":"6.3. Encoder（编码器）","slug":"_6-3-encoder-编码器","link":"#_6-3-encoder-编码器","children":[]},{"level":3,"title":"6.4. Decoder（解码器）","slug":"_6-4-decoder-解码器","link":"#_6-4-decoder-解码器","children":[]},{"level":3,"title":"6.5. MultiHeadedAttention（多头注意力）","slug":"_6-5-multiheadedattention-多头注意力","link":"#_6-5-multiheadedattention-多头注意力","children":[]},{"level":3,"title":"6.6. PositionwiseFeedForward（基于位置的前馈网络）","slug":"_6-6-positionwisefeedforward-基于位置的前馈网络","link":"#_6-6-positionwisefeedforward-基于位置的前馈网络","children":[]},{"level":3,"title":"6.7. Embeddings（嵌入层）","slug":"_6-7-embeddings-嵌入层","link":"#_6-7-embeddings-嵌入层","children":[]},{"level":3,"title":"6.8. PositionalEncoding（位置编码）","slug":"_6-8-positionalencoding-位置编码","link":"#_6-8-positionalencoding-位置编码","children":[]},{"level":3,"title":"6.9. Generator（生成器）","slug":"_6-9-generator-生成器","link":"#_6-9-generator-生成器","children":[]},{"level":3,"title":"6.10. clones（克隆）","slug":"_6-10-clones-克隆","link":"#_6-10-clones-克隆","children":[]},{"level":3,"title":"6.11. LayerNorm（层归一化）","slug":"_6-11-layernorm-层归一化","link":"#_6-11-layernorm-层归一化","children":[]},{"level":3,"title":"6.12. SublayerConnection（子层连接）","slug":"_6-12-sublayerconnection-子层连接","link":"#_6-12-sublayerconnection-子层连接","children":[]},{"level":3,"title":"6.13. 模型使用的例子","slug":"_6-13-模型使用的例子","link":"#_6-13-模型使用的例子","children":[]}]}],"git":{"createdTime":1730220137000,"updatedTime":1730220137000,"contributors":[{"name":"unknown","email":"15721607377@163.com","commits":1}]},"readingTime":{"minutes":24.4,"words":7321},"filePathRelative":"llm/llm/transformer.md","localizedDate":"May 24, 2024","excerpt":"<h1> Transformer源码解读</h1>\\n<ul>\\n<li>\\n<ol>\\n<li>About</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"2\\">\\n<li>模型总体架构</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"3\\">\\n<li>超参数</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"4\\">\\n<li>张量维度转换</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"5\\">\\n<li>可训练参数量</li>\\n</ol>\\n</li>\\n<li>\\n<ol start=\\"6\\">\\n<li>源码</li>\\n</ol>\\n</li>\\n</ul>\\n","autoDesc":true}');export{e as data};
