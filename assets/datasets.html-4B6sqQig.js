import{_ as t}from"./plugin-vue_export-helper-x3n3nnut.js";import{r as n,o as l,c as r,a as e,b as i,e as o,f as s}from"./app-9W6NtUlH.js";const d="/pinkpig/assets/datasets_001-hIAuSvNb.png",c={},h=e("h1",{id:"预训练数据集-findweb",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#预训练数据集-findweb","aria-hidden":"true"},"#"),i(" 预训练数据集 FindWeb")],-1),p=e("p",null,"https://huggingface.co/datasets/HuggingFaceFW/fineweb",-1),u=e("p",null,"https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu",-1),g=e("p",null,"https://huggingface.co/datasets/HuggingFaceFW/fineweb-2",-1),f=e("h2",{id:"找原始数据",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#找原始数据","aria-hidden":"true"},"#"),i(" 找原始数据")],-1),_=e("p",null,"两个选择",-1),m=e("li",null,"自己爬取（像OpenAI和Anthropic就是这么做的）",-1),b={href:"https://commoncrawl.org/",target:"_blank",rel:"noopener noreferrer"},x=s('<h2 id="大规模处理" tabindex="-1"><a class="header-anchor" href="#大规模处理" aria-hidden="true">#</a> 大规模处理</h2><p>开源的数据处理库：https://github.com/huggingface/datatrove</p><h2 id="评估数据的好坏" tabindex="-1"><a class="header-anchor" href="#评估数据的好坏" aria-hidden="true">#</a> 评估数据的好坏</h2><p>在数据集的代表性子集上训练小模型，并在评估任务集上评估它们</p><ul><li>使用小模型是为了控制训练成本和时间</li><li>重要的是选择一个多样化且具有代表性的数据集-评估任务集，并尽量避免对任何一个单独的基准过度拟合（overfit），因为这可能会损害预训练后获得的大语言模型（LLM）的泛化能力</li></ul><h2 id="findweb-pipeline" tabindex="-1"><a class="header-anchor" href="#findweb-pipeline" aria-hidden="true">#</a> FindWeb Pipeline</h2><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Step1: text extraction CommonCrawl 数据有两种主要格式：WARC 和 WET。</p><ul><li>WARC（Web ARChive 格式）文件包含爬取的原始数据，包括完整的页面 HTML 和请求元数据。</li><li>WET（WARC 封装文本）文件则提供了这些网站的纯文本版本。</li></ul><p>低预算团队可以直接采用WET数据，如果有更多预算，可以考虑自己从WARC中抽取出文本内容，数据质量会更高</p><p>Step2: base filtering</p><ul><li>URL Filtering</li><li>Language Filtering</li><li>Repetition Filtering</li><li>Quaility Filtering</li></ul><h3 id="deduplicating-the-data-数据去重" tabindex="-1"><a class="header-anchor" href="#deduplicating-the-data-数据去重" aria-hidden="true">#</a> Deduplicating the data 数据去重</h3><p>数据去重的意义</p><ul><li>去重已被证明与模型性能的提升相关，并可以减少模型对预训练数据的死记硬背，有助于更好的泛化能力</li><li>去重等同于提高训练效率，移除了重复数据后，模型可以用更少的训练迭代达到相同的性能水平，对于给定数量的训练token数，模型将能接触到更多样化的数据</li></ul><p>数据去重的方法：</p><p>识别甚至定义重复数据有不同的方法。常见的方法依赖哈希技术（hashing techniques）来加速过程，或者构建高效的数据结构来索引数据（例如后缀数组，suffix arrays）。方法还可以是“模糊的”（fuzzy），通过使用某种相似性度量（similarity metric）将文档标记为重复，或者“精确的”（exact），通过检查两个文档（或行、段落，或任何其他使用的粒度级别）之间的完全匹配来进行判断[9]。</p>',17);function W(F,w){const a=n("ExternalLinkIcon");return l(),r("div",null,[h,p,u,g,f,_,e("ul",null,[m,e("li",null,[i("使用已爬网页的公开仓库，如CommonCrawl "),e("ul",null,[e("li",null,[e("a",b,[i("https://commoncrawl.org/"),o(a)])])])])]),x])}const k=t(c,[["render",W],["__file","datasets.html.vue"]]);export{k as default};
