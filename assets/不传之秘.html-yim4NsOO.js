const l=JSON.parse('{"key":"v-17735133","path":"/llm/06_llm_experience/%E4%B8%8D%E4%BC%A0%E4%B9%8B%E7%A7%98.html","title":"不传之秘","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"不传之秘 哥哥总结的不传之秘 训练性能 SFT 很容易导致在其他领域上的能力急剧下滑与遗忘 ppo相对容易能力下滑和遗忘 grpo根本不忘 loss 数据量小的时候（小于几十万条），loss 下降平缓只是证明训练过程没 bug，不能证明模型学得好 loss 中间有突刺或者下降不平缓，很可能模型就傻了 小模型与大模型 小模型（小于7B）不能理解稍微复杂点的语言思考 很多训练策略与结论在小模型上和大点儿的模型上是两个世界 小模型最好学短思维链，大模型学长思维链 训练数据要点（一定不能让模型有通过记忆找到很类似的题解答，进行偷懒的机会） 去重 多样性 先易后难学习（curriculum learning） 简单的方法比复杂的方法往往要好 模型是知行合一的，对自己生成的评分最高","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/06_llm_experience/%E4%B8%8D%E4%BC%A0%E4%B9%8B%E7%A7%98.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"不传之秘"}],["meta",{"property":"og:description","content":"不传之秘 哥哥总结的不传之秘 训练性能 SFT 很容易导致在其他领域上的能力急剧下滑与遗忘 ppo相对容易能力下滑和遗忘 grpo根本不忘 loss 数据量小的时候（小于几十万条），loss 下降平缓只是证明训练过程没 bug，不能证明模型学得好 loss 中间有突刺或者下降不平缓，很可能模型就傻了 小模型与大模型 小模型（小于7B）不能理解稍微复杂点的语言思考 很多训练策略与结论在小模型上和大点儿的模型上是两个世界 小模型最好学短思维链，大模型学长思维链 训练数据要点（一定不能让模型有通过记忆找到很类似的题解答，进行偷懒的机会） 去重 多样性 先易后难学习（curriculum learning） 简单的方法比复杂的方法往往要好 模型是知行合一的，对自己生成的评分最高"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-04-15T13:47:04.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-04-15T13:47:04.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"不传之秘\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-04-15T13:47:04.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"哥哥总结的不传之秘","slug":"哥哥总结的不传之秘","link":"#哥哥总结的不传之秘","children":[]},{"level":2,"title":"多轮对话微调","slug":"多轮对话微调","link":"#多轮对话微调","children":[]},{"level":2,"title":"这个人偶尔会说点训模型过程中的不传之密","slug":"这个人偶尔会说点训模型过程中的不传之密","link":"#这个人偶尔会说点训模型过程中的不传之密","children":[]},{"level":2,"title":"领域模型训练","slug":"领域模型训练","link":"#领域模型训练","children":[]}],"git":{"createdTime":1744724824000,"updatedTime":1744724824000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-4.local","commits":1}]},"readingTime":{"minutes":3.44,"words":1031},"filePathRelative":"llm/06_llm_experience/不传之秘.md","localizedDate":"April 15, 2025","excerpt":"<h1> 不传之秘</h1>\\n<h2> 哥哥总结的不传之秘</h2>\\n<ul>\\n<li>训练性能\\n<ul>\\n<li>SFT 很容易导致在其他领域上的能力急剧下滑与遗忘</li>\\n<li>ppo相对容易能力下滑和遗忘</li>\\n<li>grpo根本不忘</li>\\n</ul>\\n</li>\\n<li>loss\\n<ul>\\n<li>数据量小的时候（小于几十万条），loss 下降平缓只是证明训练过程没 bug，不能证明模型学得好</li>\\n<li>loss 中间有突刺或者下降不平缓，很可能模型就傻了</li>\\n</ul>\\n</li>\\n<li>小模型与大模型\\n<ul>\\n<li>小模型（小于7B）不能理解稍微复杂点的语言思考</li>\\n<li>很多训练策略与结论在小模型上和大点儿的模型上是两个世界</li>\\n<li>小模型最好学短思维链，大模型学长思维链</li>\\n</ul>\\n</li>\\n<li>训练数据要点（一定不能让模型有通过记忆找到很类似的题解答，进行偷懒的机会）\\n<ul>\\n<li>去重</li>\\n<li>多样性</li>\\n<li>先易后难学习（curriculum learning）</li>\\n</ul>\\n</li>\\n<li>简单的方法比复杂的方法往往要好</li>\\n<li>模型是知行合一的，对自己生成的评分最高</li>\\n</ul>","autoDesc":true}');export{l as data};
