const e=JSON.parse('{"key":"v-4e242f56","path":"/llm/02_llm_architecture/gpt.html","title":"GPT 自然语言生成","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"GPT 自然语言生成 1. Transformer 到 GPT 模型名称GPT是生成式预训练（Generative Pretrained Transformer）的缩写 Decoder-Only Transformer GPT是一个12层的Transformer解码器 12层解码器，768维词嵌入，12个注意力头 去掉了第二个Encoder-decoder Attention子层 因为没有Encoder，第二个子层自然无法获得K和V 此时和Encoder的唯一差别是第一个多头注意力层有没有带掩码了 如果没带掩码那就是Encoder，如果带了掩码那就是Decoder","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/02_llm_architecture/gpt.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"GPT 自然语言生成"}],["meta",{"property":"og:description","content":"GPT 自然语言生成 1. Transformer 到 GPT 模型名称GPT是生成式预训练（Generative Pretrained Transformer）的缩写 Decoder-Only Transformer GPT是一个12层的Transformer解码器 12层解码器，768维词嵌入，12个注意力头 去掉了第二个Encoder-decoder Attention子层 因为没有Encoder，第二个子层自然无法获得K和V 此时和Encoder的唯一差别是第一个多头注意力层有没有带掩码了 如果没带掩码那就是Encoder，如果带了掩码那就是Decoder"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"GPT 自然语言生成\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Transformer 到 GPT","slug":"_1-transformer-到-gpt","link":"#_1-transformer-到-gpt","children":[]},{"level":2,"title":"2. GPT系列模型的技术演变","slug":"_2-gpt系列模型的技术演变","link":"#_2-gpt系列模型的技术演变","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":4.2,"words":1259},"filePathRelative":"llm/02_llm_architecture/gpt.md","localizedDate":"March 28, 2025","excerpt":"<h1> GPT 自然语言生成</h1>\\n<h2> 1. Transformer 到 GPT</h2>\\n<p>模型名称GPT是生成式预训练（Generative Pretrained Transformer）的缩写</p>\\n<p>Decoder-Only Transformer</p>\\n<p>GPT是一个12层的Transformer解码器</p>\\n<p>12层解码器，768维词嵌入，12个注意力头</p>\\n<figure><figcaption></figcaption></figure>\\n<ul>\\n<li>去掉了第二个Encoder-decoder Attention子层\\n<ul>\\n<li>因为没有Encoder，第二个子层自然无法获得K和V</li>\\n</ul>\\n</li>\\n<li>此时和Encoder的唯一差别是第一个多头注意力层有没有带掩码了\\n<ul>\\n<li>如果没带掩码那就是Encoder，如果带了掩码那就是Decoder</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{e as data};
