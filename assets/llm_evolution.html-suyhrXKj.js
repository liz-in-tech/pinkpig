const e=JSON.parse('{"key":"v-2d445952","path":"/llm/01_llm_basic/llm_theory/llm_evolution.html","title":"LLM发展历程","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"LLM发展历程 1. n-gram 具有固定上下文长度 𝑛 的统计语言模型 根据一个固定长度的前缀来预测目标单词（预测下一个词的出现概率） 2. Word Embedding（“词嵌入”，分布式词向量，稠密向量的非零表征，隐含语义的特征表示） what 词转为对应的向量，用于在后续任务中提取语义特征 词向量表即词和向量一一对应的字典 how 独热编码（One-hot向量，One-hot编码，One-Hot Representation，基于词典空间的稀疏词向量表示） 举例：假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0 优点：简单 缺点 向量维度非常大，且每个向量是稀疏的 不同词的one-hot编码向量是正交的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0 Embedding (词嵌入，词向量) 用维度有限的稠密的向量来表示所有的词汇，传统的Transformer中，词嵌入有512维；BERT中，词嵌入有768维和1024维两个版本。 one-hot与embedding的关系 词向量就是one hot的全连接层的权重矩阵参数 例如，对于一个有30000个词汇量的词典V，每一个词都是30000维的一个稀疏向量。对于每一个词，我们给他乘一个30000*512的权重矩阵，最终就得到一个512维的向量了 词向量算法代表：word2vec word2vec是一个具有代表性的词嵌入学习模型，它构建了一个简化的浅层神经网络来学习词表示 两种训练方式/两种模型 连续词袋模型 CBOW （Continuous Bag of Words） CBOW模型根据上下文单词的平均向量来预测中心单词 思想：如果两个词的上下文很相似，那么这两个词很相似 Skip-gram模型 根据一个中心单词来预测它周围的上下文单词 思想：如果两个词很相似，那么这两个词的上下文也会很相似","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/01_llm_basic/llm_theory/llm_evolution.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"LLM发展历程"}],["meta",{"property":"og:description","content":"LLM发展历程 1. n-gram 具有固定上下文长度 𝑛 的统计语言模型 根据一个固定长度的前缀来预测目标单词（预测下一个词的出现概率） 2. Word Embedding（“词嵌入”，分布式词向量，稠密向量的非零表征，隐含语义的特征表示） what 词转为对应的向量，用于在后续任务中提取语义特征 词向量表即词和向量一一对应的字典 how 独热编码（One-hot向量，One-hot编码，One-Hot Representation，基于词典空间的稀疏词向量表示） 举例：假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0 优点：简单 缺点 向量维度非常大，且每个向量是稀疏的 不同词的one-hot编码向量是正交的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0 Embedding (词嵌入，词向量) 用维度有限的稠密的向量来表示所有的词汇，传统的Transformer中，词嵌入有512维；BERT中，词嵌入有768维和1024维两个版本。 one-hot与embedding的关系 词向量就是one hot的全连接层的权重矩阵参数 例如，对于一个有30000个词汇量的词典V，每一个词都是30000维的一个稀疏向量。对于每一个词，我们给他乘一个30000*512的权重矩阵，最终就得到一个512维的向量了 词向量算法代表：word2vec word2vec是一个具有代表性的词嵌入学习模型，它构建了一个简化的浅层神经网络来学习词表示 两种训练方式/两种模型 连续词袋模型 CBOW （Continuous Bag of Words） CBOW模型根据上下文单词的平均向量来预测中心单词 思想：如果两个词的上下文很相似，那么这两个词很相似 Skip-gram模型 根据一个中心单词来预测它周围的上下文单词 思想：如果两个词很相似，那么这两个词的上下文也会很相似"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM发展历程\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. n-gram","slug":"_1-n-gram","link":"#_1-n-gram","children":[]},{"level":2,"title":"2. Word Embedding（“词嵌入”，分布式词向量，稠密向量的非零表征，隐含语义的特征表示）","slug":"_2-word-embedding-词嵌入-分布式词向量-稠密向量的非零表征-隐含语义的特征表示","link":"#_2-word-embedding-词嵌入-分布式词向量-稠密向量的非零表征-隐含语义的特征表示","children":[]},{"level":2,"title":"3. 预训练语言模型（Pre-trained Language Model, PLM）","slug":"_3-预训练语言模型-pre-trained-language-model-plm","link":"#_3-预训练语言模型-pre-trained-language-model-plm","children":[]},{"level":2,"title":"4. 大语言模型（Large Language Model, LLM）","slug":"_4-大语言模型-large-language-model-llm","link":"#_4-大语言模型-large-language-model-llm","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":2.45,"words":735},"filePathRelative":"llm/01_llm_basic/llm_theory/llm_evolution.md","localizedDate":"March 28, 2025","excerpt":"<h1> LLM发展历程</h1>\\n<figure><figcaption></figcaption></figure>\\n<h2> 1. n-gram</h2>\\n<ul>\\n<li>具有固定上下文长度 𝑛 的统计语言模型</li>\\n<li>根据一个固定长度的前缀来预测目标单词（预测下一个词的出现概率）</li>\\n</ul>\\n<h2> 2. Word Embedding（“词嵌入”，分布式词向量，稠密向量的非零表征，隐含语义的特征表示）</h2>\\n<ul>\\n<li>what\\n<ul>\\n<li>词转为对应的向量，用于在后续任务中提取语义特征</li>\\n<li>词向量表即词和向量一一对应的字典</li>\\n</ul>\\n</li>\\n<li>how\\n<ul>\\n<li>独热编码（One-hot向量，One-hot编码，One-Hot Representation，基于词典空间的稀疏词向量表示）\\n<ul>\\n<li>举例：假如词典中共有10000个词，则这个one-hot向量长度就是10000，该词在词典中所处位置对应的值为1，其他值为0</li>\\n<li>优点：简单</li>\\n<li>缺点\\n<ul>\\n<li>向量维度非常大，且每个向量是稀疏的</li>\\n<li>不同词的one-hot编码向量是正交的，在向量空间中无法表示近似关系，即使两个含义相近的词，它们的词向量点积也为0</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>Embedding (词嵌入，词向量)\\n<ul>\\n<li>用维度有限的稠密的向量来表示所有的词汇，传统的Transformer中，词嵌入有512维；BERT中，词嵌入有768维和1024维两个版本。</li>\\n<li>one-hot与embedding的关系\\n<ul>\\n<li>词向量就是one hot的全连接层的权重矩阵参数</li>\\n<li>例如，对于一个有30000个词汇量的词典V，每一个词都是30000维的一个稀疏向量。对于每一个词，我们给他乘一个30000*512的权重矩阵，最终就得到一个512维的向量了</li>\\n</ul>\\n</li>\\n<li>词向量算法代表：word2vec\\n<ul>\\n<li>word2vec是一个具有代表性的词嵌入学习模型，它构建了一个简化的浅层神经网络来学习词表示</li>\\n<li>两种训练方式/两种模型\\n<ul>\\n<li>连续词袋模型 CBOW （Continuous Bag of Words）\\n<ul>\\n<li>CBOW模型根据上下文单词的平均向量来预测中心单词</li>\\n<li>思想：如果两个词的上下文很相似，那么这两个词很相似</li>\\n</ul>\\n</li>\\n<li>Skip-gram模型\\n<ul>\\n<li>根据一个中心单词来预测它周围的上下文单词</li>\\n<li>思想：如果两个词很相似，那么这两个词的上下文也会很相似</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{e as data};
