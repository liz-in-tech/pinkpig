import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as r,c as l,e as i}from"./app-K2payoM4.js";const o={},t=i('<h1 id="推理引擎-推理框架-推理加速引擎" tabindex="-1"><a class="header-anchor" href="#推理引擎-推理框架-推理加速引擎" aria-hidden="true">#</a> 推理引擎 / 推理框架 / 推理加速引擎</h1><ul><li>vllm <ul><li>continus batching，而不是static batching</li><li>PagedAttention对kv cache的有效管理</li></ul></li><li>Text Generation Inference (TGI) <ul><li>HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理</li><li>Continuous batching</li><li>flash-attention 和 Paged Attention</li><li>项目架构： <ul><li>launcher 启动器（下载模型、启动server、启动router）</li><li>router 和 serve 负责主要的逻辑处理与模型调用 <ul><li>router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理</li><li>server (Python gRPC服务) ： 在每个显卡上都启动了一个 server</li></ul></li></ul></li></ul></li><li>faster transformer (NVIDIA FasterTransformer (FT)) <ul><li>是一个用于实现基于Transformer的神经网络推理的加速引擎</li><li>英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了</li><li>与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理</li><li>在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等</li><li>除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点</li></ul></li><li>TensorRT-LLM 英伟达家的</li><li>Triton</li></ul>',2),n=[t];function a(s,c){return r(),l("div",null,n)}const T=e(o,[["render",a],["__file","03_推理框架.html.vue"]]);export{T as default};
