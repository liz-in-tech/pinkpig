import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as l,c as e,e as n}from"./app-CV1ZlhnZ.js";const a={},r=n('<h1 id="llm的构建过程" tabindex="-1"><a class="header-anchor" href="#llm的构建过程" aria-hidden="true">#</a> LLM的构建过程</h1><h2 id="_1-pre-training-预训练" tabindex="-1"><a class="header-anchor" href="#_1-pre-training-预训练" aria-hidden="true">#</a> 1. Pre-Training（预训练）</h2><p>使用与下游任务无关的大规模数据进行模型参数的初始训练，从而找到模型参数较好的初始值</p><p>OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。</p><p>现有大模型技术路径 : “解码器架构 + 预测下一个词”</p><ul><li>数据处理 <ul><li>0.收集</li><li>1.清洗 （去除掉可能包含有毒有害的内容）</li><li>2.Tokenization（词元化）</li><li>3.Batch（分批）</li></ul></li><li>数据规模 <ul><li>目前的开源模型普遍采用 2∼3T 规模的词元进行预训练，并有趋势进一步扩大这一规模。</li></ul></li><li>算力资源 <ul><li>这一过程对于算力需求量极高，一般来说训练百亿模型至少需要百卡规模的算力集群（如 A100 80G）联合训练数月时间（与具体的算力资源相关）；而训练千亿模型则需要千卡甚至万卡规模的算力集群，对于算力资源的消耗非常惊人。</li></ul></li><li>人才需求 <ul><li>尽管整体的预训练技术框架非常直观，但是实施过程中涉及到大量需要深入探索的经验性技术，如数据如何进行配比、如何进行学习率的调整、如何早期发现模型的异常行为等。预训练过程需要考虑各种实施细节，而这些细节有很多并没有公开发表的经验可循，需要研发人员具有丰富的训练经验和异常处理能力，避免大规模训练开始以后进行回退和反复迭代，从而减少算力资源的浪费，提升训练成功的几率。大语言模型的研发看似是一个算力需求型的工程，实际上相关人才是最重要的。可以说，一个大语言模型项目的核心训练人员的能力最后会决定模型的整体水平。</li></ul></li></ul><h2 id="_2-fine-tuning-微调-指令微调-有监督微调" tabindex="-1"><a class="header-anchor" href="#_2-fine-tuning-微调-指令微调-有监督微调" aria-hidden="true">#</a> 2. Fine-tuning（微调，指令微调，有监督微调）</h2><p>目前来说，比较广泛使用的微调技术是“指令微调”（也叫做有监督微调，Supervised Fine-tuning, SFT）</p><p>这种模仿示例数据进行学习的过程本质属于机器学习中的模仿学习（Imitation Learning）。给定一个特定任务，虽然可能存在很多解答方式，模仿学习旨在加强对于标准答案（即师傅的示范动作）的复刻学习。</p><p>微调不会教会LLM预训练学到的知识和能力，主要起到了对模型能力的激发作用，而不是知识注入作用</p><ul><li>数据规模 <ul><li>远小于预训练</li><li>通常来说，数十万到百万规模的指令微调数据能够有效地激发语言模型的通用任务解决能力，甚至有些工作认为数千条或者数万条高质量指令数据也能达到不错的微调效果。</li></ul></li><li>算力资源 <ul><li>需求相对较小</li><li>一般情况下，若干台单机八卡（A100-80G）的服务器就能在一天或数天的时间内完成百亿模型的指令微调，当指令数据规模较大的时候可以进一步增加所需要的算力资源。</li></ul></li></ul><h2 id="_3-alignment-对齐-人类对齐-对齐微调" tabindex="-1"><a class="header-anchor" href="#_3-alignment-对齐-人类对齐-对齐微调" aria-hidden="true">#</a> 3. Alignment（对齐，人类对齐，对齐微调）</h2><p>OpenAI在 2022 年初发布了 InstructGPT 的学术论文，系统地介绍了如何将语言模型进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 RLHF（Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加强模型的对齐能力。</p><p>RLHF算法</p><ul><li>在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。为此，需要标注人员针对大语言模型所生成的多条输出进行偏好排序，并使用偏好数据训练奖励模型，用于判断模型的输出质量。</li><li>目前还有很多工作试图通过消除奖励模型的使用，或其他使用 SFT 方式来达到与 RLHF 相似的效果，从而简化模型的对齐过程。</li></ul><p>算力资源</p><ul><li>多于指令微调，远小于预训练</li></ul>',17),t=[r];function u(d,h){return l(),e("div",null,t)}const o=i(a,[["render",u],["__file","02_LLM构建过程.html.vue"]]);export{o as default};
