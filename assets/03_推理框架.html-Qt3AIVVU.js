const e=JSON.parse('{"key":"v-b0c84cb0","path":"/llm/04_llm_reasoning/03_%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6.html","title":"推理引擎 / 推理框架 / 推理加速引擎","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"推理引擎 / 推理框架 / 推理加速引擎 概览 vllm continus batching，而不是static batching PagedAttention对kv cache的有效管理 Text Generation Inference (TGI) HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理 Continuous batching flash-attention 和 Paged Attention 项目架构： launcher 启动器（下载模型、启动server、启动router） router 和 serve 负责主要的逻辑处理与模型调用 router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理 server (Python gRPC服务) ： 在每个显卡上都启动了一个 server faster transformer (NVIDIA FasterTransformer (FT)) 是一个用于实现基于Transformer的神经网络推理的加速引擎 英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了 与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理 在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等 除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点 TensorRT-LLM 英伟达家的 Triton","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/04_llm_reasoning/03_%E6%8E%A8%E7%90%86%E6%A1%86%E6%9E%B6.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"推理引擎 / 推理框架 / 推理加速引擎"}],["meta",{"property":"og:description","content":"推理引擎 / 推理框架 / 推理加速引擎 概览 vllm continus batching，而不是static batching PagedAttention对kv cache的有效管理 Text Generation Inference (TGI) HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理 Continuous batching flash-attention 和 Paged Attention 项目架构： launcher 启动器（下载模型、启动server、启动router） router 和 serve 负责主要的逻辑处理与模型调用 router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理 server (Python gRPC服务) ： 在每个显卡上都启动了一个 server faster transformer (NVIDIA FasterTransformer (FT)) 是一个用于实现基于Transformer的神经网络推理的加速引擎 英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了 与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理 在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等 除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点 TensorRT-LLM 英伟达家的 Triton"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-04-23T13:53:11.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-04-23T13:53:11.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"推理引擎 / 推理框架 / 推理加速引擎\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-04-23T13:53:11.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"概览","slug":"概览","link":"#概览","children":[]},{"level":2,"title":"vLLM","slug":"vllm","link":"#vllm","children":[{"level":3,"title":"1.1 使用","slug":"_1-1-使用","link":"#_1-1-使用","children":[]},{"level":3,"title":"vLLM架构","slug":"vllm架构","link":"#vllm架构","children":[]},{"level":3,"title":"1.2 核心优化点","slug":"_1-2-核心优化点","link":"#_1-2-核心优化点","children":[]},{"level":3,"title":"1.3 优点","slug":"_1-3-优点","link":"#_1-3-优点","children":[]},{"level":3,"title":"1.4 缺点","slug":"_1-4-缺点","link":"#_1-4-缺点","children":[]}]},{"level":2,"title":"Text Generation Inference (TGI)","slug":"text-generation-inference-tgi","link":"#text-generation-inference-tgi","children":[]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"git":{"createdTime":1743233581000,"updatedTime":1745416391000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro-6.local","commits":1}]},"readingTime":{"minutes":6.96,"words":2087},"filePathRelative":"llm/04_llm_reasoning/03_推理框架.md","localizedDate":"March 29, 2025","excerpt":"<h1> 推理引擎 / 推理框架 / 推理加速引擎</h1>\\n<h2> 概览</h2>\\n<figure><figcaption></figcaption></figure>\\n<ul>\\n<li>vllm\\n<ul>\\n<li>continus batching，而不是static batching</li>\\n<li>PagedAttention对kv cache的有效管理</li>\\n</ul>\\n</li>\\n<li>Text Generation Inference (TGI)\\n<ul>\\n<li>HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理</li>\\n<li>Continuous batching</li>\\n<li>flash-attention 和 Paged Attention</li>\\n<li>项目架构：\\n<ul>\\n<li>launcher 启动器（下载模型、启动server、启动router）</li>\\n<li>router 和 serve 负责主要的逻辑处理与模型调用\\n<ul>\\n<li>router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理</li>\\n<li>server (Python gRPC服务) ： 在每个显卡上都启动了一个 server</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>faster transformer (NVIDIA FasterTransformer (FT))\\n<ul>\\n<li>是一个用于实现基于Transformer的神经网络推理的加速引擎</li>\\n<li>英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了</li>\\n<li>与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理</li>\\n<li>在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等</li>\\n<li>除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点</li>\\n</ul>\\n</li>\\n<li>TensorRT-LLM 英伟达家的</li>\\n<li>Triton</li>\\n</ul>","autoDesc":true}');export{e as data};
