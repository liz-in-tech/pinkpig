import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as i,c as e,e as a}from"./app-D6wNGShW.js";const r={},h=a('<h1 id="llm-关键技术概览" tabindex="-1"><a class="header-anchor" href="#llm-关键技术概览" aria-hidden="true">#</a> LLM 关键技术概览</h1><h2 id="_1-规模扩展" tabindex="-1"><a class="header-anchor" href="#_1-规模扩展" aria-hidden="true">#</a> 1. 规模扩展</h2><p>LLM的一个关键成功因素</p><ul><li>扩展方向 <ul><li>参数扩展</li><li>数据（高质量数据）扩展</li></ul></li></ul><p>关键 : 实现规模扩展的关键在于模型架构的可扩展性。Transformer 模型的可扩展性非常强，对于硬件并行优化的支持也比较友好，特别适合大语言模型的研发，很多工作也在进一步针对其进行优化与改进。</p><h2 id="_2-数据工程" tabindex="-1"><a class="header-anchor" href="#_2-数据工程" aria-hidden="true">#</a> 2. 数据工程</h2><p>在通用的预训练范式下，模型能力本质上是来源于所见过的训练数据，因此数据工程就变得极为重要，不是简单的扩大数据规模就能够实现的。</p><p>数据工程主要包括三个方面</p><ul><li>对数据进行全面的采集，拓宽高质量的数据来源</li><li>对数据精细的清洗，尽量提升用于大模型训练的数据质量</li><li>设计有效的数据配比与数据课程，加强模型对于数据语义信息的利用效率</li></ul><p>这三个方面的数据工程技术直接决定了最后大语言模型的性能水平。</p><h2 id="_3-高效预训练" tabindex="-1"><a class="header-anchor" href="#_3-高效预训练" aria-hidden="true">#</a> 3. 高效预训练</h2><ul><li>使用大规模分布式训练算法优化LLM的神经网络参数 <ul><li>在训练过程中，需要联合使用各种并行策略以及效率优化方法，包括 3D 并行（数据并行、流水线并行、张量并行）、ZeRO（内存冗余消除技术）等</li></ul></li><li>使用分布式优化框架 <ul><li>用分布式优化框架来简化并行算法的实现与部署，其中具有代表性的分布式训练软件包括 DeepSpeed和 Megatron-LM，它们能够有效支持千卡甚至万卡的联合训练</li></ul></li><li>使用优化技巧 <ul><li>研发过程也需要关注较为实用的优化技巧，提升训练稳定性和优化效率，如混合精度训练</li></ul></li><li>使用沙盒测试 由于大语言模型的训练需要耗费大量的算力资源，通常需要开展基于小模型的沙盒测试实验，进而确定面向大模型的最终训练策略</li></ul><h2 id="_4-能力激发" tabindex="-1"><a class="header-anchor" href="#_4-能力激发" aria-hidden="true">#</a> 4. 能力激发</h2><p>设计合适的指令微调以及提示策略进行激发或诱导模型的任务求解能力</p><p>2种实现途径</p><ul><li>指令微调 <ul><li>使用自然语言表达的任务描述以及期望的任务输出对于大语言模型进行指令微调</li><li>指令微调无法向大模型注入新的知识，而是训练大模型学会利用自身所掌握的知识与信息进行任务的求解</li></ul></li><li>提示学习 <ul><li>设计合适的提示策略去诱导大语言模型生成正确的问题答案</li><li>多种高级提示策略，包括上下文学习、思维链提示等</li><li>提示工程已经成为利用大语言模型能力的一个重要技术途径</li></ul></li></ul><h2 id="_5-人类对齐" tabindex="-1"><a class="header-anchor" href="#_5-人类对齐" aria-hidden="true">#</a> 5. 人类对齐</h2><p>较好地符合人类的价值观，避免生成有偏见、泄露隐私甚至对人类有害的内容。</p><p>3H对齐标准</p><ul><li>Helpfulness（有用性）</li><li>Honesty（诚实性）</li><li>Harmlessness（无害性）</li></ul><p>实践</p><ul><li>OpenAI 提出了基于人类反馈的强化学习算法（Reinforcement Learning from Human Feedback, RLHF）</li><li>由于强化学习算法的优化过程较为复杂，最近学术界开始涌现出一批使用监督微调的对齐方式，从而简化 RLHF 优化过程的算法，如 DPO 算法等</li></ul><h2 id="_6-工具使用" tabindex="-1"><a class="header-anchor" href="#_6-工具使用" aria-hidden="true">#</a> 6. 工具使用</h2><p>受限能力扩展</p><ul><li>非自然语言形式的任务（如数值计算）</li><li>预训练数据之外的（超过数据时间范围以及覆盖内容的）信息</li></ul><p>GPT 系列模型通过插件机制来形成系统性的工具调用方式</p><p>2种实现途径</p><ul><li>指令微调</li><li>提示学习</li></ul>',28),n=[h];function d(u,t){return i(),e("div",null,n)}const p=l(r,[["render",d],["__file","01_LLM关键技术概览.html.vue"]]);export{p as default};
