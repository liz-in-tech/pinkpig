import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{o,c as t,a as e,b as i}from"./app-eTVGMpLM.js";const n={},r=e("h1",{id:"对齐-强化学习",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#对齐-强化学习","aria-hidden":"true"},"#"),i(" 对齐 & 强化学习")],-1),a=e("ul",null,[e("li",null,"策略梯度 pg"),e("li",null,"近端策略优化 ppo (Proximal Policy Optimization) : 迭代更新策略以最大化奖励，同时保持接近初始行为。它使用奖励模型来对响应进行评分，并需要仔细调整超参数，包括学习率、批量大小和 PPO 剪辑范围"),e("li",null,"rlhf"),e("li",null,"dpo（Direct Preference Optimization）：直接偏好优化 直接优化策略，以最大化选择响应而非拒绝响应的可能性。它不需要奖励建模，这使得它在计算上比 PPO 更高效，但在质量方面略差。"),e("li",null,"orpo"),e("li",null,"拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。")],-1),c=[r,a];function _(s,d){return o(),t("div",null,c)}const p=l(n,[["render",_],["__file","05_对齐.html.vue"]]);export{p as default};
