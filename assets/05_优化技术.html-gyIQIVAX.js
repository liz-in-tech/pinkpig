import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as i,c as t,e as a}from"./app-CV1ZlhnZ.js";const u="/pinkpig/assets/kv_cache-ruAgjzId.png",e="/pinkpig/assets/mqa_gqa-omwORPHY.png",n={},o=a('<h1 id="优化技术" tabindex="-1"><a class="header-anchor" href="#优化技术" aria-hidden="true">#</a> 优化技术</h1><h2 id="_1-优化技术概览" tabindex="-1"><a class="header-anchor" href="#_1-优化技术概览" aria-hidden="true">#</a> 1. 优化技术概览</h2><p>优化点都是从时间和空间两方面来考虑</p><ul><li>全过程优化 <ul><li>注意力优化（计算优化） <ul><li>KV Cache：缓存KV，避免重复计算</li><li>MQA 多查询注意力：KV共享</li><li>GQA 分组查询注意力：分组KV共享</li><li>Flash Attention V1、V2</li><li>PagedAttention</li></ul></li><li>批次管理优化 <ul><li>动态批处理 / 连续批处理 /continus batching / In-flight batching</li></ul></li><li>分布式/并行化 <ul><li>将模型分布在多个 GPU 上</li><li>在多 GPU/节点 上分担计算和存储压力</li></ul></li><li>量化/混合精度 <ul><li>将模型参数从高精度转换为低精度，在不显著影响效果的前提下降低计算负担</li></ul></li><li>剪枝/稀疏化Sparsity/正则化技术 <ul><li>移除不重要的权重或神经元，减少参数数量</li><li>正则化技术：dropout &amp; L1 &amp; L2</li></ul></li><li>早退机制 / 提前停止</li></ul></li><li>训练优化 <ul><li>解决的问题 <ul><li>适配到已有的GPU资源，使模型可在资源受限情况下训练（空间）</li><li>加速训练收敛，降低训练时间，提速（时间）</li><li>稳定训练过程，防止梯度爆炸和梯度消失</li><li>高吞吐量，提高资源利用率，减少浪费</li></ul></li><li>主要技术和方法 <ul><li>激活值重新计算</li><li>梯度累积</li><li>超参数调整 <ul><li>动态学习率调整：使用调整超参数的现代优化器（AdamW、Lion），使用余弦退火、衰减、预热 warm-up策略，帮助模型更快收敛或改善稳定性</li><li>批大小的选择</li></ul></li><li>通用优化 <ul><li>注意力优化</li><li>批次管理优化</li><li>分布式/并行化</li><li>混合精度训练</li><li>剪枝/正则化技术：dropout &amp; L1 &amp; L2</li><li>早退机制/提前停止 <ul><li>在验证损失不再改善时停止训练，节省计算资源</li></ul></li></ul></li></ul></li></ul></li><li>推理优化 <ul><li>解决的问题 - 减少模型大小、内存占用和计算需求，使模型可部署在资源受限设备（如移动设备）（空间） - 降低推理时间，提速，确保低延迟，优化用户体验（时间） - 高吞吐量，提高资源利用率，减少浪费</li><li>主要技术和方法 <ul><li>训练后模型压缩优化 <ul><li>量化 Quantization / 混合精度 <ul><li>降低模型权重和激活值的精度（如从32位浮点数到8位整数），减少内存占用和计算成本</li></ul></li><li>剪枝 / 稀疏 Sparsity / 正则化技术 <ul><li>移除不重要的权重或神经元，减少参数数量，降低推理时间</li></ul></li><li>蒸馏 Distillation / 知识蒸馏 <ul><li>训练较小模型以模仿较大模型的行为，保留大部分性能，同时显著减少模型大小</li></ul></li></ul></li><li>解码策略优化 <ul><li>推测解码（Speculative Decoding） <ul><li>生成草稿token序列，并行验证保留可采纳的多个token，加速自回归解码</li></ul></li><li>非自回归解码（Non-autoregressive Decoding） <ul><li>一次生成多个token，减少序列依赖，提升速度</li></ul></li><li>早退机制（Early Exiting） <ul><li>在满足条件时提前结束推理，节省计算</li></ul></li><li>级联解码（Cascade Inference） <ul><li>把多个模型按效率排序（越往后的模型越大生成质量越高），依次进行生成，直到符合想要的质量后停止</li></ul></li></ul></li><li>通用优化 <ul><li>注意力优化</li><li>批次管理优化</li><li>混合精度推理</li><li>分布式/并行化</li><li>剪枝</li><li>早退</li></ul></li></ul></li></ul></li></ul><h2 id="_2-注意力机制优化" tabindex="-1"><a class="header-anchor" href="#_2-注意力机制优化" aria-hidden="true">#</a> 2. 注意力机制优化</h2><ul><li>传统 <ul><li>缩放点积注意力 SDPA， scaled dot-product attention</li><li>多头注意力 MHA，multi-head attention <ul><li>当使用八个并行注意力头时，每个注意力头的维度都会减少（例如 d_model/8）。这使得计算成本与单头注意力相似</li></ul></li></ul></li><li>kv cache （用空间换时间）: <ul><li>解码的时候为了减少冗余计算KV值，所以将之间计算出的KV缓存了下来</li><li>缓存KV后，每次只需要计算一个token的KV值</li><li>存在的问题：内存需求随着批量大小和序列长度线性增长，限制了可服务的吞吐量，并对长上下文输入提出了挑战</li></ul></li><li>MQA 和 GQA 等优化通过减少存储的key头和value头的数量来帮助减少 KV 缓存所需的内存 <ul><li>MQA 多查询注意力 <ul><li>在多个注意力头之间共享键和值</li></ul></li><li>GQA 分组查询注意力 <ul><li>最初使用 MHA 训练的模型可以使用原始训练计算的一小部分通过 GQA 进行“升级训练”。它们获得接近 MHA 的质量，同时保持接近 MQA 的计算效率。</li></ul></li></ul></li><li>Flash Attention V1、V2 <ul><li>修改某些计算的顺序，以更好地利用 GPU 的内存层次结构</li><li>优化注意力机制，将其复杂度从二次方转化为线性，从而加快训练和推理速度</li><li>FlashAttention 使用“平铺”一次性完全计算并写出最终矩阵的一小部分，而不是分步对整个矩阵进行部分计算，写出中间的中间值。</li></ul></li><li>PagedAttention <ul><li>KV缓存的分页高效管理</li><li>将连续的键和值存储在内存中的不连续空间中</li><li>这些块的大小是固定的，消除了因不同请求需要不同分配等挑战而产生的低效率。这极大地限制了内存浪费，从而实现了更大的批量大小（从而提高了吞吐量）</li></ul></li></ul><figure><img src="'+u+'" alt="KV Cache" tabindex="0" loading="lazy"><figcaption>KV Cache</figcaption></figure><p>下图显示多头注意力有多个键值头（左）。分组查询注意力（中心）的键值头多于一个，但少于查询头的数量，这是内存需求和模型质量之间的平衡。多查询注意力（右）具有单个键值头，有助于节省内存。</p><figure><img src="'+e+'" alt="MQA &amp; GQA" tabindex="0" loading="lazy"><figcaption>MQA &amp; GQA</figcaption></figure><h2 id="_3-动态批处理-连续批处理-continus-batching-in-flight-batching" tabindex="-1"><a class="header-anchor" href="#_3-动态批处理-连续批处理-continus-batching-in-flight-batching" aria-hidden="true">#</a> 3. 动态批处理 / 连续批处理 /continus batching / In-flight batching</h2><ul><li>同时执行多个不同的请求</li><li>传统批处理/static batching：对于批次中的每个请求，LLM 可能会生成不同数量的tokens，并且不同tokens有不同的执行时间。因此，批次中的所有请求都必须等待最长token的处理完成，而生成长度的巨大差异可能会加剧这种情况</li><li>通过动态批处理，服务器运行时会立即从批处理中剔除已完成的序列，而不是等待整个批处理完成后再继续处理下一组请求。然后，它开始执行新请求，而其他请求仍在进行中。因此，动态批处理可以极大地提高实际用例中 GPU 的整体利用率。</li></ul>',11),c=[o];function r(h,s){return i(),t("div",null,c)}const _=l(n,[["render",r],["__file","05_优化技术.html.vue"]]);export{_ as default};
