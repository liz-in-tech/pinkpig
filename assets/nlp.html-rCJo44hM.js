const e=JSON.parse('{"key":"v-2313fd7f","path":"/llm/nlp/nlp.html","title":"NLP","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"NLP nlp 基本任务 Segment(分词，token化) 重要概念 token 在自然语言处理中，Token（词元）通常指的是将句子或文本拆分成单个独立的单词或标点符号。这个过程称为分词或标记化，它是自然语言处理中的一项重要预处理任务。 猫｜在｜卧室｜吃｜鱼 就是一个拆分成词元的过程。 在表示英文的时候，Token（词元）和单词不一样，Token是一个比单词更小的分割单位。 what 将文本拆分成单个独立的单词或标点符号","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/nlp/nlp.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"NLP"}],["meta",{"property":"og:description","content":"NLP nlp 基本任务 Segment(分词，token化) 重要概念 token 在自然语言处理中，Token（词元）通常指的是将句子或文本拆分成单个独立的单词或标点符号。这个过程称为分词或标记化，它是自然语言处理中的一项重要预处理任务。 猫｜在｜卧室｜吃｜鱼 就是一个拆分成词元的过程。 在表示英文的时候，Token（词元）和单词不一样，Token是一个比单词更小的分割单位。 what 将文本拆分成单个独立的单词或标点符号"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-02-16T13:11:34.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-02-16T13:11:34.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"NLP\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-16T13:11:34.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"nlp","slug":"nlp-1","link":"#nlp-1","children":[{"level":3,"title":"基本任务","slug":"基本任务","link":"#基本任务","children":[]}]},{"level":2,"title":"向量与模型","slug":"向量与模型","link":"#向量与模型","children":[{"level":3,"title":"什么是向量","slug":"什么是向量","link":"#什么是向量","children":[]},{"level":3,"title":"LLM与向量","slug":"llm与向量","link":"#llm与向量","children":[]},{"level":3,"title":"向量的生成","slug":"向量的生成","link":"#向量的生成","children":[]},{"level":3,"title":"向量的存储方式","slug":"向量的存储方式","link":"#向量的存储方式","children":[]}]},{"level":2,"title":"神经网络","slug":"神经网络","link":"#神经网络","children":[{"level":3,"title":"直观","slug":"直观","link":"#直观","children":[]},{"level":3,"title":"分层结构","slug":"分层结构","link":"#分层结构","children":[]},{"level":3,"title":"本质","slug":"本质","link":"#本质","children":[]},{"level":3,"title":"神经网络的训练","slug":"神经网络的训练","link":"#神经网络的训练","children":[]},{"level":3,"title":"示例","slug":"示例","link":"#示例","children":[]}]},{"level":2,"title":"RNNs","slug":"rnns","link":"#rnns","children":[{"level":3,"title":"优点","slug":"优点","link":"#优点","children":[]},{"level":3,"title":"缺点","slug":"缺点","link":"#缺点","children":[]},{"level":3,"title":"适用","slug":"适用","link":"#适用","children":[]},{"level":3,"title":"典型模型","slug":"典型模型","link":"#典型模型","children":[]}]},{"level":2,"title":"seq2seq结构/Encoder-Decoder结构(编码-解码器)","slug":"seq2seq结构-encoder-decoder结构-编码-解码器","link":"#seq2seq结构-encoder-decoder结构-编码-解码器","children":[{"level":3,"title":"构成","slug":"构成-1","link":"#构成-1","children":[]}]},{"level":2,"title":"Attention(注意力机制)","slug":"attention-注意力机制","link":"#attention-注意力机制","children":[{"level":3,"title":"朴素seq2seq的信息瓶颈","slug":"朴素seq2seq的信息瓶颈","link":"#朴素seq2seq的信息瓶颈","children":[]},{"level":3,"title":"注意力机制就是希望打破这个信息瓶颈。宏观来看，Attention直接把Decoder中的每一个隐层，和Encoder中的每一个隐层，都连接起来了！","slug":"注意力机制就是希望打破这个信息瓶颈。宏观来看-attention直接把decoder中的每一个隐层-和encoder中的每一个隐层-都连接起来了","link":"#注意力机制就是希望打破这个信息瓶颈。宏观来看-attention直接把decoder中的每一个隐层-和encoder中的每一个隐层-都连接起来了","children":[]},{"level":3,"title":"向量的点积(Dot Product)==矩阵的乘法","slug":"向量的点积-dot-product-矩阵的乘法","link":"#向量的点积-dot-product-矩阵的乘法","children":[]},{"level":3,"title":"Attention机制的步骤","slug":"attention机制的步骤","link":"#attention机制的步骤","children":[]},{"level":3,"title":"Attention机制带来的好处","slug":"attention机制带来的好处","link":"#attention机制带来的好处","children":[]}]},{"level":2,"title":"Transformer","slug":"transformer","link":"#transformer","children":[{"level":3,"title":"为什么发布Transformer的文章名字叫Attention Is All You Need 呢？","slug":"为什么发布transformer的文章名字叫attention-is-all-you-need-呢","link":"#为什么发布transformer的文章名字叫attention-is-all-you-need-呢","children":[]},{"level":3,"title":"优点","slug":"优点-1","link":"#优点-1","children":[]},{"level":3,"title":"整体结构","slug":"整体结构-1","link":"#整体结构-1","children":[]},{"level":3,"title":"模型整体的前向传播步骤","slug":"模型整体的前向传播步骤","link":"#模型整体的前向传播步骤","children":[]},{"level":3,"title":"超参数","slug":"超参数","link":"#超参数","children":[]}]},{"level":2,"title":"EncoderDecoder张量维度转换","slug":"encoderdecoder张量维度转换","link":"#encoderdecoder张量维度转换","children":[{"level":3,"title":"超参数","slug":"超参数-1","link":"#超参数-1","children":[]},{"level":3,"title":"可训练参数（28layerd^2+27layerd+2d+vocabd","slug":"可训练参数-28layerd-2-27layerd-2d-vocabd","link":"#可训练参数-28layerd-2-27layerd-2d-vocabd","children":[]},{"level":3,"title":"编码器","slug":"编码器","link":"#编码器","children":[]},{"level":3,"title":"解码器","slug":"解码器","link":"#解码器","children":[]}]},{"level":2,"title":"PTM,Pre-trained Model(预训练模型)","slug":"ptm-pre-trained-model-预训练模型","link":"#ptm-pre-trained-model-预训练模型","children":[{"level":3,"title":"预训练思想","slug":"预训练思想","link":"#预训练思想","children":[]},{"level":3,"title":"预训练模型用于下游任务的两种策略","slug":"预训练模型用于下游任务的两种策略","link":"#预训练模型用于下游任务的两种策略","children":[]}]},{"level":2,"title":"GPT(Generative Pretrained Transformer)","slug":"gpt-generative-pretrained-transformer","link":"#gpt-generative-pretrained-transformer","children":[{"level":3,"title":"GPT是一个12层的Transformer解码器","slug":"gpt是一个12层的transformer解码器","link":"#gpt是一个12层的transformer解码器","children":[]},{"level":3,"title":"","slug":"-2","link":"#-2","children":[]},{"level":3,"title":"GPT1-GPT4：逐步扩大模型参数和训练语料规模","slug":"gpt1-gpt4-逐步扩大模型参数和训练语料规模","link":"#gpt1-gpt4-逐步扩大模型参数和训练语料规模","children":[]}]},{"level":2,"title":"Language Model,LM(语言模型)","slug":"language-model-lm-语言模型","link":"#language-model-lm-语言模型","children":[{"level":3,"title":"起源","slug":"起源","link":"#起源","children":[]},{"level":3,"title":"定义","slug":"定义","link":"#定义","children":[]}]},{"level":2,"title":"BERT(Pre-training of Deep Bidirectional Transformers for Language Understanding)","slug":"bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding","link":"#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding","children":[]}],"git":{"createdTime":1739711494000,"updatedTime":1739711494000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":49.68,"words":14904},"filePathRelative":"llm/nlp/nlp.md","localizedDate":"February 16, 2025","excerpt":"<h1> NLP</h1>\\n<h2> nlp</h2>\\n<h3> 基本任务</h3>\\n<h4> Segment(分词，token化)</h4>\\n<h5> 重要概念</h5>\\n<h6> token</h6>\\n<p>在自然语言处理中，Token（词元）通常指的是将句子或文本拆分成单个独立的单词或标点符号。这个过程称为分词或标记化，它是自然语言处理中的一项重要预处理任务。</p>\\n<p>猫｜在｜卧室｜吃｜鱼 就是一个拆分成词元的过程。</p>\\n<p>在表示英文的时候，Token（词元）和单词不一样，Token是一个比单词更小的分割单位。</p>\\n<h5> what</h5>\\n<p>将文本拆分成单个独立的单词或标点符号</p>","autoDesc":true}');export{e as data};
