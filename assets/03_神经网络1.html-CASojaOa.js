import{_ as o}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as t,c as n,e as r}from"./app-CV1ZlhnZ.js";const l={},s=r('<h1 id="神经网络常见算子" tabindex="-1"><a class="header-anchor" href="#神经网络常见算子" aria-hidden="true">#</a> 神经网络常见算子</h1><p>Q: 设计神经网络的时候，必须/常见用哪些算子进行组合？ 什么时候用什么</p><p>A: Okay, 在设计神经网络时，你会组合使用各种算子（在 PyTorch 中通常体现为 <code>torch.nn</code> 里的模块或 <code>torch</code> 下的函数）来构建模型架构。选择哪些算子以及如何组合它们，完全取决于你要解决的 <strong>问题类型</strong>、<strong>数据特性</strong> 以及你想要构建的 <strong>网络结构</strong>。</p><p>以下是一些最核心和常见的算子（模块/函数）及其典型用途：</p><p><strong>1. 核心层 (Core Layers):</strong></p><ul><li><strong><code>torch.nn.Linear(in_features, out_features)</code>:</strong><ul><li><strong>作用:</strong> 实现全连接层（也叫密集层），执行一个线性变换 <code>Y = X @ W.T + b</code>。</li><li><strong>何时使用:</strong><ul><li>多层感知机 (MLP) 的基本构建块。</li><li>在卷积网络（CNN）的末端，将提取的特征图（通常先展平）映射到最终的输出类别或回归值。</li><li>在 Transformer 等模型中用于各种线性投影（如 QKV 投影、前馈网络部分）。</li><li>处理非结构化或已展平的特征向量。</li></ul></li></ul></li><li><strong><code>torch.nn.Conv1d / Conv2d / Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...)</code>:</strong><ul><li><strong>作用:</strong> 实现卷积操作。通过滑动一个（或多个）可学习的卷积核（滤波器）在输入数据上进行运算，提取局部模式。</li><li><strong>何时使用:</strong><ul><li><strong><code>Conv2d</code>:</strong> 处理具有空间结构的数据，最典型的是 <strong>图像</strong>。用于提取边缘、纹理、形状等视觉特征。计算机视觉任务（分类、检测、分割）的核心。</li><li><strong><code>Conv1d</code>:</strong> 处理 <strong>序列数据</strong>，如文本（将词嵌入视为通道）、时间序列信号、音频波形等。可以捕捉局部的时间/序列模式。</li><li><strong><code>Conv3d</code>:</strong> 处理 <strong>体积数据</strong> 或时空数据，如医学影像（CT/MRI）、视频（帧序列）。</li></ul></li></ul></li><li><strong><code>torch.nn.RNN / LSTM / GRU(input_size, hidden_size, num_layers, batch_first=False, ...)</code>:</strong><ul><li><strong>作用:</strong> 实现循环神经网络层，用于处理 <strong>序列数据</strong>。它们具有内部状态（记忆），可以捕捉序列中的时间依赖关系。</li><li><strong>何时使用:</strong><ul><li>自然语言处理 (NLP)：文本分类、机器翻译、情感分析、语言建模。</li><li>时间序列预测：股票价格、天气预报。</li><li>语音识别、音乐生成。</li><li><strong>LSTM / GRU</strong> 是 RNN 的改进版，能更好地处理长序列中的长期依赖问题（缓解梯度消失/爆炸）。GRU 参数更少，计算稍快；LSTM 通常效果更稳健。</li></ul></li></ul></li></ul><p><strong>2. 激活函数 (Activation Functions):</strong></p><ul><li><strong><code>torch.nn.ReLU()</code> / <code>torch.relu(input)</code>:</strong><ul><li><strong>作用:</strong> 修正线性单元，<code>f(x) = max(0, x)</code>。引入非线性，使网络能够学习更复杂的函数。</li><li><strong>何时使用:</strong> <strong>最常用</strong> 的激活函数，几乎用于所有类型的网络（CNN、MLP）中的隐藏层之后。计算简单，收敛快，能有效缓解梯度消失。</li></ul></li><li><strong><code>torch.nn.LeakyReLU(negative_slope=0.01)</code> / <code>torch.nn.PReLU()</code> / <code>torch.nn.ELU()</code>:</strong><ul><li><strong>作用:</strong> ReLU 的变体，解决了 ReLU 在负数区域输出为 0 可能导致神经元“死亡”的问题。</li><li><strong>何时使用:</strong> 当怀疑 ReLU 导致训练困难时，可以尝试这些替代品。PReLU 的负斜率是可学习的。</li></ul></li><li><strong><code>torch.nn.Sigmoid()</code> / <code>torch.sigmoid(input)</code>:</strong><ul><li><strong>作用:</strong> 将输出压缩到 (0, 1) 之间，<code>f(x) = 1 / (1 + exp(-x))</code>。</li><li><strong>何时使用:</strong><ul><li><strong>二元分类</strong> 问题的 <strong>最后一层</strong>，输出表示属于正类的概率。</li><li>在需要将数值映射到概率或 (0, 1) 区间的地方（如某些门控机制 LSTM/GRU 内部）。</li><li><strong>注意:</strong> 在隐藏层中已较少使用，因其容易导致梯度消失。</li></ul></li></ul></li><li><strong><code>torch.nn.Tanh()</code> / <code>torch.tanh(input)</code>:</strong><ul><li><strong>作用:</strong> 将输出压缩到 (-1, 1) 之间。</li><li><strong>何时使用:</strong><ul><li>在某些 RNN 变体（如 LSTM）内部使用。</li><li>当希望输出中心化在 0 附近时，可能比 Sigmoid 效果好。</li><li>隐藏层中使用频率也低于 ReLU 及其变体。</li></ul></li></ul></li><li><strong><code>torch.nn.Softmax(dim=None)</code> / <code>torch.softmax(input, dim=None)</code>:</strong><ul><li><strong>作用:</strong> 将一个向量转换为概率分布，所有元素的和为 1，且每个元素都在 (0, 1) 之间。</li><li><strong>何时使用:</strong> <strong>多元分类</strong> 问题的 <strong>最后一层</strong>，输出表示属于每个类别的概率。<code>dim</code> 参数指定在哪一维上进行 Softmax 操作（通常是类别那一维）。</li></ul></li><li><strong><code>torch.nn.GELU()</code> / <code>torch.nn.SiLU()</code> (Swish):</strong><ul><li><strong>作用:</strong> 更平滑的非线性激活函数，在 Transformer 等现代模型中表现优异。</li><li><strong>何时使用:</strong> 常用于 Transformer 模型（如 BERT, GPT）的隐藏层。</li></ul></li></ul><p><strong>3. 池化层 (Pooling Layers):</strong></p><ul><li><strong><code>torch.nn.MaxPool1d / MaxPool2d / MaxPool3d(kernel_size, stride=None, padding=0, ...)</code>:</strong><ul><li><strong>作用:</strong> 最大池化。在输入的一个区域内取最大值作为输出。</li><li><strong>何时使用:</strong><ul><li><strong><code>MaxPool2d</code>:</strong> 在 CNN 中，通常放在卷积层之后。</li><li><strong>目的:</strong><ul><li><strong>降低特征图的空间维度</strong>（降采样），减少计算量和参数数量。</li><li><strong>增大感受野</strong> (Receptive Field)。</li><li>提供一定程度的 <strong>平移不变性</strong>，使网络对特征位置的小变化不那么敏感。</li><li>提取最显著的特征（最大值）。</li></ul></li></ul></li></ul></li><li><strong><code>torch.nn.AvgPool1d / AvgPool2d / AvgPool3d(kernel_size, stride=None, padding=0, ...)</code>:</strong><ul><li><strong>作用:</strong> 平均池化。在输入的一个区域内取平均值作为输出。</li><li><strong>何时使用:</strong><ul><li>与 MaxPool 类似，用于降采样和减少维度。</li><li>相比 MaxPool，它保留了更多背景信息，特征更平滑。有时用于替代 MaxPool，或在网络末端（如全局平均池化）使用。</li></ul></li></ul></li><li><strong><code>torch.nn.AdaptiveAvgPool2d(output_size)</code> / <code>torch.nn.AdaptiveMaxPool2d(output_size)</code>:</strong><ul><li><strong>作用:</strong> 自适应池化。无论输入特征图的大小如何，都将其池化到指定的 <code>output_size</code>。</li><li><strong>何时使用:</strong> 非常常用！特别是在 CNN 的卷积部分和最终的全连接层之间。例如，使用 <code>AdaptiveAvgPool2d((1, 1))</code> 可以将任意大小的特征图转换为 <code>(batch_size, channels, 1, 1)</code>，然后可以将其展平 (<code>flatten</code>) 送入 <code>nn.Linear</code> 层，这样网络就可以处理不同分辨率的输入图像。</li></ul></li></ul><p><strong>4. 归一化层 (Normalization Layers):</strong></p><ul><li><strong><code>torch.nn.BatchNorm1d / BatchNorm2d / BatchNorm3d(num_features)</code>:</strong><ul><li><strong>作用:</strong> 批归一化。对一个 mini-batch 内的数据在通道维度上进行归一化（使其均值为 0，方差为 1），然后通过可学习的缩放（gamma）和平移（beta）参数进行调整。</li><li><strong>何时使用:</strong> <strong>非常常用</strong>！ <ul><li>通常放在卷积层或全连接层 <strong>之后</strong>，激活函数 <strong>之前</strong>。</li><li><strong>目的:</strong><ul><li>加速模型训练收敛。</li><li>稳定训练过程，降低对初始化和学习率的敏感度。</li><li>具有一定的正则化效果。</li></ul></li><li><strong>注意:</strong> 对 batch size 比较敏感，batch size 过小时效果可能下降。</li></ul></li></ul></li><li><strong><code>torch.nn.LayerNorm(normalized_shape)</code>:</strong><ul><li><strong>作用:</strong> 层归一化。在每个样本内部，对指定的维度（通常是所有特征维度）进行归一化。</li><li><strong>何时使用:</strong><ul><li>在 <strong>RNN / LSTM / GRU</strong> 中广泛使用，因为它不依赖于 batch 统计量。</li><li>在 <strong>Transformer</strong> 模型中是核心组件。</li><li>当 batch size 很小或变化较大时，是 BatchNorm 的良好替代品。</li></ul></li></ul></li><li><strong><code>torch.nn.InstanceNorm1d / InstanceNorm2d / InstanceNorm3d(num_features)</code>:</strong><ul><li><strong>作用:</strong> 实例归一化。在每个样本的 <strong>每个通道</strong> 上独立进行归一化。</li><li><strong>何时使用:</strong><ul><li>常见于 <strong>风格迁移</strong> 等图像生成任务，因为它能消除图像实例间的对比度信息，有助于分离内容和风格。</li></ul></li></ul></li></ul><p><strong>5. 正则化与辅助层 (Regularization &amp; Utility Layers):</strong></p><ul><li><strong><code>torch.nn.Dropout(p=0.5)</code>:</strong><ul><li><strong>作用:</strong> 在训练期间，以概率 <code>p</code> 随机将输入张量中的部分元素置为 0，并对剩余元素进行缩放 (<code>1/(1-p)</code>) 以保持期望值不变。在评估（<code>.eval()</code> 模式）时不生效。</li><li><strong>何时使用:</strong><ul><li>作为 <strong>正则化</strong> 手段，防止模型 <strong>过拟合</strong>。</li><li>通常放在全连接层或卷积层之后。</li></ul></li></ul></li><li><strong><code>torch.nn.Flatten(start_dim=1, end_dim=-1)</code> / <code>torch.flatten(input, start_dim=1, end_dim=-1)</code>:</strong><ul><li><strong>作用:</strong> 将输入张量的连续维度展平成一维。</li><li><strong>何时使用:</strong><ul><li>在 CNN 中，将卷积层输出的特征图 (<code>batch, channels, height, width</code>) 展平成向量 (<code>batch, features</code>)，以便送入后续的全连接层。</li></ul></li></ul></li></ul><p><strong>6. 损失函数 (Loss Functions):</strong></p><ul><li><strong><code>torch.nn.CrossEntropyLoss()</code>:</strong><ul><li><strong>作用:</strong> 结合了 <code>LogSoftmax</code> 和 <code>NLLLoss</code> (负对数似然损失)。用于度量分类模型的预测概率分布与真实标签之间的差异。</li><li><strong>何时使用:</strong> <strong>最常用</strong> 的 <strong>多元分类</strong> 损失函数。输入应为未经 Softmax 的原始 logits，目标为类别索引。</li></ul></li><li><strong><code>torch.nn.BCELoss()</code> / <code>torch.nn.BCEWithLogitsLoss()</code>:</strong><ul><li><strong>作用:</strong> 二元交叉熵损失。</li><li><strong>何时使用:</strong><ul><li><strong>二元分类</strong> 问题。</li><li><strong>多标签分类</strong> 问题（每个样本可以属于多个类别）。</li><li><code>BCELoss</code> 需要输入经过 Sigmoid 激活后的概率值。</li><li><code>BCEWithLogitsLoss</code> <strong>更推荐</strong>，它结合了 Sigmoid 和 BCELoss，输入应为未经 Sigmoid 的原始 logits，数值上更稳定。</li></ul></li></ul></li><li><strong><code>torch.nn.MSELoss()</code>:</strong><ul><li><strong>作用:</strong> 均方误差损失 (Mean Squared Error)。计算预测值和目标值之间差的平方的平均值。</li><li><strong>何时使用:</strong> <strong>回归</strong> 问题（预测连续值）。</li></ul></li><li><strong><code>torch.nn.L1Loss()</code>:</strong><ul><li><strong>作用:</strong> 平均绝对误差损失 (Mean Absolute Error)。计算预测值和目标值之间差的绝对值的平均值。</li><li><strong>何时使用:</strong> <strong>回归</strong> 问题。相比 MSELoss，对异常值不那么敏感。</li></ul></li></ul><p><strong>7. 张量操作函数 (Tensor Operations):</strong></p><ul><li><code>torch.cat(tensors, dim=0)</code>: 沿指定维度拼接张量。</li><li><code>torch.stack(tensors, dim=0)</code>: 沿新创建的维度堆叠张量。</li><li><code>tensor.view(*shape)</code> / <code>torch.reshape(input, shape)</code>: 改变张量的形状（不改变数据总数）。</li><li><code>tensor.permute(*dims)</code> / <code>torch.permute(input, dims)</code>: 交换张量的维度。</li><li><code>tensor.transpose(dim0, dim1)</code>: 交换指定的两个维度。</li><li>索引和切片 (<code>tensor[:, 0, ...]</code>)：选择张量的子集。</li><li><code>torch.add</code>, <code>torch.mul</code>, <code>torch.matmul</code>, etc.：基本的数学运算，有时直接使用，但通常封装在 <code>nn.Module</code> 里。</li></ul><p><strong>选择策略总结:</strong></p><ol><li><strong>看数据类型和任务：</strong><ul><li>图像/空间数据 -&gt; CNN (<code>Conv2d</code>, <code>MaxPool2d</code>/<code>AvgPool2d</code>, <code>BatchNorm2d</code>/<code>InstanceNorm2d</code>)</li><li>序列数据 (文本/时间序列) -&gt; RNN/LSTM/GRU 或 <code>Conv1d</code> 或 Transformer</li><li>表格/向量数据 -&gt; MLP (<code>Linear</code>)</li><li>分类任务 -&gt; <code>Softmax</code> (多元) / <code>Sigmoid</code> (二元) + <code>CrossEntropyLoss</code> / <code>BCELoss</code></li><li>回归任务 -&gt; 通常最后一层是 <code>Linear</code> + <code>MSELoss</code> / <code>L1Loss</code></li></ul></li><li><strong>构建网络深度和非线性：</strong> 堆叠核心层 (<code>Linear</code>, <code>Conv*</code>) 并插入 <strong>激活函数</strong> (<code>ReLU</code> 是首选) 来增加模型容量和学习复杂模式的能力。</li><li><strong>控制特征维度和稳定性：</strong> 使用 <strong>池化层</strong> (<code>MaxPool</code>, <code>AvgPool</code>, <code>AdaptiveAvgPool</code>) 来降低维度、减少计算量；使用 <strong>归一化层</strong> (<code>BatchNorm</code>, <code>LayerNorm</code>) 来稳定和加速训练。</li><li><strong>防止过拟合：</strong> 加入 <strong><code>Dropout</code></strong> 层。</li><li><strong>连接不同部分：</strong> 使用 <code>Flatten</code>, <code>view</code>/<code>reshape</code>, <code>cat</code>/<code>stack</code> 等张量操作来调整形状和组合特征。</li><li><strong>现代架构趋势：</strong> 对于许多任务，Transformer 架构（依赖于自注意力机制 <code>nn.MultiheadAttention</code> 和 <code>LayerNorm</code>、<code>Linear</code>、<code>GELU</code> 等）正变得越来越流行，尤其是在 NLP 和逐渐扩展到 CV 领域。残差连接（Residual Connections，通常通过简单的 <code>torch.add</code> 实现）也是现代深度网络（如 ResNet）的重要组成部分，有助于训练非常深的网络。</li></ol><p>最终的网络设计往往是一个结合理论知识和实验迭代的过程。从经典的、经过验证的架构（如 LeNet, AlexNet, VGG, ResNet, U-Net, LSTMs, Transformers）开始学习和修改是一个很好的起点。</p>',21),e=[s];function i(c,d){return t(),n("div",null,e)}const u=o(l,[["render",i],["__file","03_神经网络1.html.vue"]]);export{u as default};
