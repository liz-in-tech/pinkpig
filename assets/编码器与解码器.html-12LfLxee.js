const e=JSON.parse('{"key":"v-78d1ab22","path":"/llm/02_llm_architecture/%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8.html","title":"编码器和解码器","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"编码器和解码器 三种结构 seq2seq结构/Encoder-Decoder结构 Encoder Only 结构 Decoder Only 结构 自然语言理解和自然语言生成 编码器架构被认为更适合去解决自然语言理解任务（如完形填空等） 解码器架构更适合解决自然语言生成任务（如文本摘要等） 1. Decoder Only训练时和推理时有区别 训练时：接受Encoder给出的表示向量和“标准答案”作为输入。 推理时：接受Encoder给出的表示向量和Decoder上一步的预测结果作为输入。这种使用上一步预测结果作为下一步输入的方式，我们称为自回归(Auto-regressive)。GPT就是一个自回归的文本生成模型。自回归形式天然适合Generative的场景，这不难理解，就像是我们写文章是从左往右边写边想的一样。","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/02_llm_architecture/%E7%BC%96%E7%A0%81%E5%99%A8%E4%B8%8E%E8%A7%A3%E7%A0%81%E5%99%A8.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"编码器和解码器"}],["meta",{"property":"og:description","content":"编码器和解码器 三种结构 seq2seq结构/Encoder-Decoder结构 Encoder Only 结构 Decoder Only 结构 自然语言理解和自然语言生成 编码器架构被认为更适合去解决自然语言理解任务（如完形填空等） 解码器架构更适合解决自然语言生成任务（如文本摘要等） 1. Decoder Only训练时和推理时有区别 训练时：接受Encoder给出的表示向量和“标准答案”作为输入。 推理时：接受Encoder给出的表示向量和Decoder上一步的预测结果作为输入。这种使用上一步预测结果作为下一步输入的方式，我们称为自回归(Auto-regressive)。GPT就是一个自回归的文本生成模型。自回归形式天然适合Generative的场景，这不难理解，就像是我们写文章是从左往右边写边想的一样。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"编码器和解码器\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Decoder Only训练时和推理时有区别","slug":"_1-decoder-only训练时和推理时有区别","link":"#_1-decoder-only训练时和推理时有区别","children":[]},{"level":2,"title":"2. 一些典型的大语言模型的详细配置","slug":"_2-一些典型的大语言模型的详细配置","link":"#_2-一些典型的大语言模型的详细配置","children":[]},{"level":2,"title":"3. seq2seq结构/Encoder-Decoder结构","slug":"_3-seq2seq结构-encoder-decoder结构","link":"#_3-seq2seq结构-encoder-decoder结构","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":1.54,"words":463},"filePathRelative":"llm/02_llm_architecture/编码器与解码器.md","localizedDate":"March 28, 2025","excerpt":"<h1> 编码器和解码器</h1>\\n<p>三种结构</p>\\n<ul>\\n<li>seq2seq结构/Encoder-Decoder结构</li>\\n<li>Encoder Only 结构</li>\\n<li>Decoder Only 结构</li>\\n</ul>\\n<p>自然语言理解和自然语言生成</p>\\n<ul>\\n<li>编码器架构被认为更适合去解决自然语言理解任务（如完形填空等）</li>\\n<li>解码器架构更适合解决自然语言生成任务（如文本摘要等）</li>\\n</ul>\\n<h2> 1. Decoder Only训练时和推理时有区别</h2>\\n<ul>\\n<li>训练时：接受Encoder给出的表示向量和“标准答案”作为输入。</li>\\n<li>推理时：接受Encoder给出的表示向量和Decoder上一步的预测结果作为输入。这种使用上一步预测结果作为下一步输入的方式，我们称为自回归(Auto-regressive)。GPT就是一个自回归的文本生成模型。自回归形式天然适合Generative的场景，这不难理解，就像是我们写文章是从左往右边写边想的一样。</li>\\n</ul>","autoDesc":true}');export{e as data};
