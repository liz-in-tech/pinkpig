import{_ as i}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as a,c as e,e as l}from"./app-K2payoM4.js";const n="/pinkpig/assets/llm_020-lIidEWOm.png",r="/pinkpig/assets/llm_021-oykqkVuu.png",t="/pinkpig/assets/llm_022-j7ItV1OS.png",d="/pinkpig/assets/llm_023-DEXP28GG.png",s="/pinkpig/assets/llm_024-CqUHcjqH.png",h={},c=l('<h1 id="llm推理" tabindex="-1"><a class="header-anchor" href="#llm推理" aria-hidden="true">#</a> LLM推理</h1><h2 id="_1-解码效率的定量评估指标" tabindex="-1"><a class="header-anchor" href="#_1-解码效率的定量评估指标" aria-hidden="true">#</a> 1. 解码效率的定量评估指标</h2><ul><li>GPU性能评估 <ul><li>GPU算力 : 算力是指 GPU 每秒能够进行的浮点运算次数，单位是 FLOP/s</li><li>GPU带宽 : 带宽是该显卡每秒能够进行的显存读写量，单位是 byte/s</li><li>GPU计算强度上限 : 算力和带宽的比值被称为该 GPU 的计算强度上限 𝐼𝑚𝑎𝑥，单位为 FLOP/byte</li></ul></li><li>模型性能评估 <ul><li>模型的运算量 : 运算量是指运行该模型需要的总浮点计算数，单位为 FLOP</li><li>模型的访存量 : 访存量是运行该模型的过程中所需的显存读写量，单位为 byte</li><li>模型的计算强度 : 运算量和访存量的比值被称为该模型的计算强度 𝐼，单位为 FLOP/byte</li></ul></li><li>带宽瓶颈和计算瓶颈 <ul><li>带宽瓶颈 / 显存瓶颈 <ul><li>当模型的计算强度 𝐼 小于GPU 的计算强度上限 𝐼𝑚𝑎𝑥 时，这说明 GPU 的理论最高显存读写速度低于实际运算所需速度，因此模型实际的运行效率将主要受到显存读写速度的影响，这种情况称为带宽瓶颈；</li></ul></li><li>反之，当 𝐼 大于 𝐼𝑚𝑎𝑥 时，说明 GPU 的理论最高浮点运算速度低于实际运算所需速度，因此模型的运行效率将主要受到算力的影响，这种情况称为计算瓶颈。</li></ul></li></ul><h2 id="_2-自回归生成算法" tabindex="-1"><a class="header-anchor" href="#_2-自回归生成算法" aria-hidden="true">#</a> 2. 自回归生成算法</h2><p>由于自回归算法的序列化生成特点，使得解码算法存在效率较低的问题。</p><figure><img src="'+n+'" alt="自回归生成算法" tabindex="0" loading="lazy"><figcaption>自回归生成算法</figcaption></figure><h2 id="_3-全量解码阶段与增量解码阶段" tabindex="-1"><a class="header-anchor" href="#_3-全量解码阶段与增量解码阶段" aria-hidden="true">#</a> 3. 全量解码阶段与增量解码阶段</h2><h3 id="_3-1-全量解码阶段-预填充阶段-第一次-计算瓶颈" tabindex="-1"><a class="header-anchor" href="#_3-1-全量解码阶段-预填充阶段-第一次-计算瓶颈" aria-hidden="true">#</a> 3.1. 全量解码阶段 / 预填充阶段（第一次）- 计算瓶颈</h3><p>对于输入序列，一次性地计算其状态并缓存键值矩阵 （算法 3 第 1 至 3 行）</p><p>以 LLaMA 模型为例，全量解码的运算量、访存量和计算强度：</p><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>全量解码阶段是受限于 GPU 浮点数计算能力（即计算瓶颈）</p><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_3-2-增量解码阶段-解码阶段-之后多次-带宽瓶颈-显存瓶颈" tabindex="-1"><a class="header-anchor" href="#_3-2-增量解码阶段-解码阶段-之后多次-带宽瓶颈-显存瓶颈" aria-hidden="true">#</a> 3.2. 增量解码阶段 / 解码阶段（之后多次）- 带宽瓶颈/显存瓶颈</h3><p>只计算上一步新生成词元的状态，并不断地以自回归方式生成新词元并对应更新键值缓存，直到生成结束（算法 3 第 4-9 行）</p><p>解码时需要用到之前每一个token的KV值，如果不缓存KV值，就要每次都重新计算完整的KV值</p><p><strong>解码阶段的低效问题主要出现在增量解码阶段，存在显存瓶颈</strong></p><p>以 LLaMA 模型为例，增量解码的运算量、访存量和计算强度</p><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>增量解码阶段是受限于 GPU 显存读写速度的（即显存瓶颈），这种问题通常被称为内存墙（Memory Wall）问题</p><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_4-增量解码阶段-显存瓶颈-的改进方法" tabindex="-1"><a class="header-anchor" href="#_4-增量解码阶段-显存瓶颈-的改进方法" aria-hidden="true">#</a> 4. 增量解码阶段（显存瓶颈）的改进方法</h2><h3 id="_4-1-系统优化-直接解决系统级别的内存墙问题" tabindex="-1"><a class="header-anchor" href="#_4-1-系统优化-直接解决系统级别的内存墙问题" aria-hidden="true">#</a> 4.1. 系统优化（直接解决系统级别的内存墙问题）</h3><p>针对“内存墙”问题，一个直观的解决方案是减少相关操作的访存量，从而达到提升计算强度的目的。</p><p>优化方法</p><ul><li>FlashAttention</li><li>PagedAttention</li><li>批次管理优化</li></ul><h3 id="_4-2-解码策略优化-针对自回归解码策略的改进方法" tabindex="-1"><a class="header-anchor" href="#_4-2-解码策略优化-针对自回归解码策略的改进方法" aria-hidden="true">#</a> 4.2. 解码策略优化（针对自回归解码策略的改进方法）</h3><h4 id="_4-2-1-推测解码-speculative-decoding" tabindex="-1"><a class="header-anchor" href="#_4-2-1-推测解码-speculative-decoding" aria-hidden="true">#</a> 4.2.1. 推测解码（Speculative Decoding）</h4><ul><li>推测解码不会降低大模型解码的质量，实验测试表明能够带来约两倍左右的解码加速，是目前使用较多的解码策略优化方案。</li><li>生成草稿token序列，然后进行并行验证草稿token序列的哪部分可以被采纳</li></ul><h4 id="_4-2-2-非自回归解码-non-autoregressive-decoding" tabindex="-1"><a class="header-anchor" href="#_4-2-2-非自回归解码-non-autoregressive-decoding" aria-hidden="true">#</a> 4.2.2. 非自回归解码（Non-autoregressive Decoding）</h4><ul><li>非自回归解码 <ul><li>基于输入并行地一次性生成所有的词元</li><li>这种方法的生成质量和自回归方法有一定差距</li></ul></li><li>半自回归解码 <ul><li>每次生成一组词元</li><li>Medusa 在 Vicuna 模型的基础上，额外训练了两个预测头来分别预测第2个和第3个词，因此可以达到一次生成3个次元的效果</li></ul></li></ul><h4 id="_4-2-3-早退机制-early-exiting" tabindex="-1"><a class="header-anchor" href="#_4-2-3-早退机制-early-exiting" aria-hidden="true">#</a> 4.2.3. 早退机制（Early Exiting）</h4><ul><li>可能不需要经过所有层的计算，模型就可以较为可靠地预测下一个词的生成。基于这种想法，研究人员提出了基于早退机制的生成方式。</li><li>早退机制 <ul><li>传统早退机制 <ul><li>设置早退判断条件，条件满足时，结束网络层的前向传播，不进行后续层的计算，直接生成结果</li></ul></li><li>混合深度方法 <ul><li>动态调整每一层的计算量</li><li>与传统早退机制直接跳过后续所有层相比，混合深度方法有选择性的跳过了部分层，因此可以更好地利用模型中不同层的特性</li></ul></li></ul></li></ul><h4 id="_4-2-4-级联解码-cascade-inference" tabindex="-1"><a class="header-anchor" href="#_4-2-4-级联解码-cascade-inference" aria-hidden="true">#</a> 4.2.4. 级联解码（Cascade Inference）</h4><ul><li>考虑到不能请求的难易度不同，分别使用不同规模的模型来处理请求，从而最小化解码时间</li><li>多个模型排序（模型从小到大，效率从高到低），将请求依次给排好序的模型进行生成任务，引入一个专门训练的二分类模型来判断生成结果是否符合任务要求，如果结果可靠则不再给之后的模型进行生成，否则让之后的模型进行处理</li><li>用户可以根据需求设定分类器的判断阈值，从而平衡生成效率和生成质量</li></ul>',35),o=[c];function u(p,_){return a(),e("div",null,o)}const m=i(h,[["render",u],["__file","02_LLM推理.html.vue"]]);export{m as default};
