import{_ as l}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as i,c as e,e as a}from"./app-eS8cJyut.js";const r="/pinkpig/assets/kv_cache-ruAgjzId.png",n="/pinkpig/assets/mqa_gqa-omwORPHY.png",u={},t=a('<h1 id="llm-roadmap" tabindex="-1"><a class="header-anchor" href="#llm-roadmap" aria-hidden="true">#</a> LLM Roadmap</h1><h2 id="_1-llm-基础" tabindex="-1"><a class="header-anchor" href="#_1-llm-基础" aria-hidden="true">#</a> 1. LLM 基础</h2><ul><li>GPU</li><li>Mathematics for ML <ul><li>线性代数 <ul><li>向量、矩阵、行列式、特征值和特征向量、向量空间和线性变换</li></ul></li><li>概率论</li><li>微积分 <ul><li>导数、积分、极限和级数、多元微积分和梯度</li></ul></li></ul></li><li>Python for ML <ul><li>Python Basics <ul><li>基本语法、数据类型、错误处理和面向对象编程</li></ul></li><li>Data Science Libraries <ul><li>NumPy 用于数值运算</li><li>Pandas 用于数据操作和分析</li><li>Matplotlib 和 Seaborn 用于数据可视化</li></ul></li><li>Data Preprocessing <ul><li>涉及特征缩放和规范化、处理缺失数据、异常值检测、分类数据编码以及将数据分成训练、验证和测试集</li></ul></li><li>ML Libraries <ul><li>熟练掌握 Scikit-learn（一个提供多种监督和非监督学习算法的库）至关重要。了解如何实现线性回归、逻辑回归、决策树、随机森林、k-最近邻 (K-NN) 和 K-均值聚类等算法非常重要。PCA 和 t-SNE 等降维技术也有助于可视化高维数据。</li></ul></li></ul></li><li>Neural Networks <ul><li>Fundamentals <ul><li>理解神经网络的结构，例如层、权重、偏差和激活函数（sigmoid、tanh、ReLU 等）。</li></ul></li><li>Training and Optimization <ul><li>熟悉反向传播和不同类型的损失函数，如均方误差 (MSE) 和交叉熵。了解各种优化算法，如梯度下降、随机梯度下降、RMSprop 和 Adam。</li><li>常用参数更新方法 <ul><li>梯度下降：在一个方向上更新和调整模型的参数，来最小化损失函数</li><li>随机梯度下降（Stochastic gradient descent，SGD） 对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快</li><li>小批量梯度下降（Mini Batch Gradient Descent）：对每个批次中的n个训练样本，这种方法只执行一次更新 <ul><li>通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。</li><li>在训练神经网络时，通常都会选择小批量梯度下降算法</li></ul></li><li>动量（Momentum）技术 <ul><li>通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练</li></ul></li><li>Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似</li></ul></li><li>梯度爆炸和梯度消失 <ul><li>导致的原因和解决 <ul><li>矩阵连乘 <ul><li>在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸</li><li>解决：对梯度做截断解决梯度爆炸问题、残差连接、normalize。由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为该问题已经解决了。</li></ul></li><li>使用了Sigmoid和tanh激活函数：换用ReLU激活函数</li></ul></li></ul></li></ul></li><li>Overfitting 过拟合 <ul><li>了解过度拟合的概念（模型在训练数据上表现良好但在看不见的数据上表现不佳），并学习各种正则化技术（dropout、L1/L2 正则化、早期停止、数据增强）来防止过度拟合。</li></ul></li><li>Implement a MLP <ul><li>使用 PyTorch 构建 MLP，也称为全连接网络</li></ul></li></ul></li><li>NLP <ul><li>Text Preprocessing / Tokenizer <ul><li>学习各种文本预处理步骤，如标记化（将文本分成单词或句子）、词干提取（将单词简化为其词根形式）、词形还原（类似于词干提取但考虑上下文）、停用词删除等</li></ul></li><li>Feature Extraction Techniques <ul><li>熟悉将文本数据转换为机器学习算法可以理解的格式的技术。主要方法包括词袋 (BoW)、词频-逆文档频率 (TF-IDF) 和 n-gram。</li></ul></li><li>Word Embeddings / Word2Vec <ul><li>词嵌入是一种词语表示方法，可以让具有相似含义的词语具有相似的表示。主要方法包括 Word2Vec、GloVe 和 FastText。</li><li>演变 <ul><li>读热编码 one-hot向量表示</li><li>Word2Vec</li></ul></li></ul></li><li>NLP三大特征抽取器 <ul><li>Transformer</li><li>RNN <ul><li>了解 RNN 的工作原理，RNN 是一种用于处理序列数据的神经网络。探索 LSTM 和 GRU，这两种 RNN 变体都能够学习长期依赖关系。</li></ul></li><li>CNN</li></ul></li><li>NLP预训练发展史：图像预训练 → word embedding → word2vec → elmo → transformer → gpt → bert → GPT 234</li></ul></li></ul><h2 id="_2-llm-架构" tabindex="-1"><a class="header-anchor" href="#_2-llm-架构" aria-hidden="true">#</a> 2. LLM 架构</h2><ul><li>了解从编码器-解码器 Transformer 到仅解码器架构（如 GPT）的演变，这些架构构成了现代人工智能的基础。重点介绍这些模型如何在高层次上处理和生成文本。 <ul><li>Encoder 在抽取序列中某一个词的特征时能够看到整个序列中所有的信息，即上文和下文同时看到</li><li>Decoder 中因为有 mask 机制的存在，使得它在编码某一个词的特征时只能看到自身和它之前的文本信息</li><li>几种主要的架构 <ul><li>以BERT为代表的encoder-only</li><li>以T5和BART为代表的encoder-decoder</li><li>以GPT为代表的decoder-only</li></ul></li></ul></li><li>为什么现在的LLM都是Decoder only的架构？ <ul><li>next token prediction 预训练，兼顾理解和生成</li><li>就生成任务而言，引入双向注意力并无实质好处。而Encoder-Decoder架构之所以能够在某些场景下表现更好，大概只是因为它多了一倍参数。所以，在同等参数量、同等推理成本下，Decoder-only架构就是最优选择了</li><li>decoder-only支持一直复用KV-Cache，对多轮对话更友好，因为每个Token的表示只和它之前的输入有关，而encoder-decoder和PrefixLM就难以做到</li></ul></li><li>Tokenizer <ul><li>了解标记化的原理 - 如何将文本转换为可以处理的数字表示。探索不同的标记化策略及其对模型性能和输出质量的影响</li></ul></li><li>Attention <ul><li>掌握注意力机制的核心概念，尤其是自我注意力及其变体。了解这些机制如何能够处理长距离依赖关系并在整个序列中保持上下文</li></ul></li><li>解码策略 (Top-k / Top-p / Temperature)</li><li>采样技术 <ul><li>探索各种文本生成方法及其权衡。将贪婪搜索和束搜索等确定性方法与温度采样和核采样等概率方法进行比较。</li></ul></li><li>Transformer <ul><li>Transformer 模型一开始是用来做 seq2seq 任务的，所以它包含 Encoder 和 Decoder 两个部分</li></ul></li><li>GPT：补全形式，目标是生成，只需要decoder机制</li><li>BERT：完形填空形式，目标是理解，只需要encoder机制</li><li>MoE</li><li>常见大模型 <ul><li>Llama</li><li>Qwen</li><li>Deepseek</li></ul></li></ul><h2 id="_3-llm-训练" tabindex="-1"><a class="header-anchor" href="#_3-llm-训练" aria-hidden="true">#</a> 3. LLM 训练</h2><h3 id="_3-1-llm-数据工程" tabindex="-1"><a class="header-anchor" href="#_3-1-llm-数据工程" aria-hidden="true">#</a> 3.1. LLM 数据工程</h3><ul><li>数据格式</li><li>数据收集</li><li>数据清洗 <ul><li>去重</li><li>过滤</li><li>选择</li><li>组合</li></ul></li><li>标记化 Tokenizer</li></ul><h3 id="_3-2-分布式训练" tabindex="-1"><a class="header-anchor" href="#_3-2-分布式训练" aria-hidden="true">#</a> 3.2. 分布式训练</h3><ul><li>使用 DeepSpeed 或 FSDP 在多个 GPU 上进行大规模训练。DeepSpeed 提供三个 ZeRO 优化阶段，通过状态分区提高内存效率。两种方法都支持梯度检查点以提高内存效率。</li><li>并行策略 : 这些策略需要优化跨 GPU 集群的网络通信和内存管理</li><li>DeepSpeed</li><li>Megatron</li></ul><h3 id="_3-3-有监督微调" tabindex="-1"><a class="header-anchor" href="#_3-3-有监督微调" aria-hidden="true">#</a> 3.3. 有监督微调</h3><p>全面微调会更新所有模型参数，但需要大量计算。LoRA 和 QLoRA 等参数高效的微调技术通过训练少量适配器参数同时保持基本权重不变来减少内存需求。QLoRA 将 4 位量化与 LoRA 相结合，以减少 VRAM 的使用。</p><h3 id="_3-4-强化学习-人类偏好对齐" tabindex="-1"><a class="header-anchor" href="#_3-4-强化学习-人类偏好对齐" aria-hidden="true">#</a> 3.4. 强化学习 &amp; 人类偏好对齐</h3><p>重点是将生成的答案与人类偏好对齐。此阶段旨在调整语气并减少毒性和幻觉。然而，提高其性能和实用性也变得越来越重要。与 SFT 不同，有许多偏好对齐算法。在这里，我们将重点介绍两个最重要的算法：DPO 和 PPO。</p><ul><li>策略梯度 pg</li><li>近端策略优化 ppo (Proximal Policy Optimization) : 迭代更新策略以最大化奖励，同时保持接近初始行为。它使用奖励模型来对响应进行评分，并需要仔细调整超参数，包括学习率、批量大小和 PPO 剪辑范围</li><li>rlhf</li><li>dpo（Direct Preference Optimization）：直接偏好优化 直接优化策略，以最大化选择响应而非拒绝响应的可能性。它不需要奖励建模，这使得它在计算上比 PPO 更高效，但在质量方面略差。</li><li>orpo</li><li>拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。</li></ul><p>监控 ：除了 SFT 指标之外，您还希望最大化所选答案和首选答案之间的差距。准确率也应逐渐提高，直至稳定下来。</p><h3 id="_3-5-训练参数" tabindex="-1"><a class="header-anchor" href="#_3-5-训练参数" aria-hidden="true">#</a> 3.5. 训练参数</h3><p>关键参数包括带调度器的学习率、批量大小、梯度累积、时期数、优化器（如 8 位 AdamW）、正则化的权重衰减以及训练稳定性的预热步骤。LoRA 还增加了三个参数：等级（通常为 16-128）、alpha（1-2x 等级）和目标模块。</p><h3 id="_3-6-训练监控" tabindex="-1"><a class="header-anchor" href="#_3-6-训练监控" aria-hidden="true">#</a> 3.6. 训练监控</h3><p>使用仪表板跟踪关键指标（损失、梯度、GPU 统计数据），针对分布式训练问题实施有针对性的日志记录，并设置性能分析以识别跨设备计算和通信中的瓶颈。</p><p>跟踪训练指标，包括损失曲线、学习率计划和梯度范数。监控常见问题，如损失峰值、梯度爆炸或性能下降。</p><h2 id="_4-llm-推理" tabindex="-1"><a class="header-anchor" href="#_4-llm-推理" aria-hidden="true">#</a> 4. LLM 推理</h2><h3 id="_4-1-llm-训练优化" tabindex="-1"><a class="header-anchor" href="#_4-1-llm-训练优化" aria-hidden="true">#</a> 4.1. LLM 训练优化</h3><ul><li>FlashAttention V1、V2 <ul><li>优化注意力机制，将其复杂度从二次方转化为线性，从而加快训练和推理速度</li></ul></li><li>混合精度训练</li><li>重计算</li><li>MQA / GQA</li><li>梯度累积</li><li>训练优化 ：使用自适应学习率进行预热、梯度剪裁和规范化以防止爆炸，使用混合精度训练提高内存效率，并使用调整超参数的现代优化器（AdamW、Lion）。</li></ul><h3 id="_4-2-推理加速优化" tabindex="-1"><a class="header-anchor" href="#_4-2-推理加速优化" aria-hidden="true">#</a> 4.2. 推理加速优化</h3><p>推理引擎 / 推理框架 / 推理加速引擎</p><ul><li>vllm <ul><li>continus batching，而不是static batching</li><li>PagedAttention对kv cache的有效管理</li></ul></li><li>Text Generation Inference (TGI) <ul><li>HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理</li><li>Continuous batching</li><li>flash-attention 和 Paged Attention</li><li>项目架构： <ul><li>launcher 启动器（下载模型、启动server、启动router）</li><li>router 和 serve 负责主要的逻辑处理与模型调用 <ul><li>router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理</li><li>server (Python gRPC服务) ： 在每个显卡上都启动了一个 server</li></ul></li></ul></li></ul></li><li>faster transformer (NVIDIA FasterTransformer (FT)) <ul><li>是一个用于实现基于Transformer的神经网络推理的加速引擎</li><li>英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了</li><li>与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理</li><li>在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等</li><li>除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点</li></ul></li><li>TensorRT-LLM 英伟达家的</li><li>Triton</li></ul><p>推理优化技术</p><p>优化动机 / 为什么要优化：推理分为第一次和之后的多次，第一次为预填充，之后叫解码。解码时需要用到之前每一个token的KV值，如果不优化，就要每次都重新计算完整的KV值</p><ul><li>kv cache （用空间换时间）: <ul><li>解码的时候为了减少冗余计算KV值，所以将之间计算出的KV缓存了下来</li><li>缓存KV后，每次只需要计算一个token的KV值</li><li>存在的问题：内存需求随着批量大小和序列长度线性增长，限制了可服务的吞吐量，并对长上下文输入提出了挑战</li></ul></li><li>模型并行化训练和推理：将模型分布在多个 GPU 上 <ul><li>PP</li><li>TP</li><li>SP</li></ul></li><li>注意力机制优化 <ul><li>传统 <ul><li>缩放点积注意力 SDPA， scaled dot-product attention</li><li>多头注意力 MHA，multi-head attention <ul><li>当使用八个并行注意力头时，每个注意力头的维度都会减少（例如 d_model/8）。这使得计算成本与单头注意力相似</li></ul></li></ul></li><li>MQA 和 GQA 等优化通过减少存储的key头和value头的数量来帮助减少 KV 缓存所需的内存 <ul><li>MQA 多查询注意力 <ul><li>在多个注意力头之间共享键和值</li></ul></li><li>GQA 分组查询注意力 <ul><li>最初使用 MHA 训练的模型可以使用原始训练计算的一小部分通过 GQA 进行“升级训练”。它们获得接近 MHA 的质量，同时保持接近 MQA 的计算效率。</li></ul></li></ul></li><li>Flash Attention <ul><li>FlashAttention 使用“平铺”一次性完全计算并写出最终矩阵的一小部分，而不是分步对整个矩阵进行部分计算，写出中间的中间值。</li></ul></li></ul></li><li>PagedAttention <ul><li>KV缓存的分页高效管理</li><li>将连续的键和值存储在内存中的不连续空间中</li><li>这些块的大小是固定的，消除了因不同请求需要不同分配等挑战而产生的低效率。这极大地限制了内存浪费，从而实现了更大的批量大小（从而提高了吞吐量）</li></ul></li><li>模型优化技术 (修改模型权重本身来减少每个 GPU 上的内存使用) <ul><li>量化 Quantization <ul><li>降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。</li></ul></li><li>剪枝 / 稀疏 Sparsity</li><li>蒸馏 Distillation <ul><li>缩小模型大小的另一种方法是通过称为蒸馏的过程将其知识转移到较小的模型。此过程涉及训练较小的模型（称为学生）来模仿较大模型（教师）的行为</li></ul></li></ul></li><li>模型服务技术 <ul><li>动态批处理 / 连续批处理 /continus batching / In-flight batching <ul><li>同时执行多个不同的请求</li><li>传统批处理/static batching：对于批次中的每个请求，LLM 可能会生成不同数量的tokens，并且不同tokens有不同的执行时间。因此，批次中的所有请求都必须等待最长token的处理完成，而生成长度的巨大差异可能会加剧这种情况</li><li>通过动态批处理，服务器运行时会立即从批处理中剔除已完成的序列，而不是等待整个批处理完成后再继续处理下一组请求。然后，它开始执行新请求，而其他请求仍在进行中。因此，动态批处理可以极大地提高实际用例中 GPU 的整体利用率。</li></ul></li><li>Speculative decoding <ul><li>使用小模型生成草稿，然后由更大的模型进行审查，以加快文本生成速度</li></ul></li></ul></li></ul><figure><img src="'+r+'" alt="KV Cache" tabindex="0" loading="lazy"><figcaption>KV Cache</figcaption></figure><p>下图显示多头注意力有多个键值头（左）。分组查询注意力（中心）的键值头多于一个，但少于查询头的数量，这是内存需求和模型质量之间的平衡。多查询注意力（右）具有单个键值头，有助于节省内存。</p><figure><img src="'+n+'" alt="MQA &amp; GQA" tabindex="0" loading="lazy"><figcaption>MQA &amp; GQA</figcaption></figure><h3 id="_4-3-llm-部署" tabindex="-1"><a class="header-anchor" href="#_4-3-llm-部署" aria-hidden="true">#</a> 4.3. LLM 部署</h3><ul><li>本地部署 <ul><li>LM Studio 、 Ollama 、 oobabooga 、 kobold.cpp 等</li></ul></li><li>演示部署 <ul><li>Gradio 和 Streamlit 等框架有助于创建应用程序原型并共享演示</li><li>您还可以轻松地在线托管它们，例如使用 Hugging Face Spaces</li></ul></li><li>服务器部署 <ul><li>大规模部署需要云（另请参阅 SkyPilot ）或本地基础架构，并且通常利用优化的文本生成框架，例如 TGI 、 vLLM 等</li></ul></li><li>边缘部署 <ul><li>在受限环境中， MLC 和 mnn- 等高性能框架可以在网络浏览器、Android 和 iOS 中部署</li></ul></li></ul><h3 id="_4-4-llm-压缩" tabindex="-1"><a class="header-anchor" href="#_4-4-llm-压缩" aria-hidden="true">#</a> 4.4. LLM 压缩</h3><ul><li>量化:分析错误模式以识别特定弱点，例如遵循复杂指令的局限性、缺乏特定知识或易受对抗性提示的影响。这可以通过更好的数据生成和训练参数来改进 <ul><li>基础技术 ：了解不同精度级别（FP32、FP16、INT8 等）以及如何使用 absmax 和零点技术执行简单量化</li><li>GGUF 和 llama.cpp ： llama.cpp 和 GGUF 格式最初设计为在 CPU 上运行，现已成为在消费级硬件上运行的最流行工具。它支持将特殊标记、词汇表和元数据存储在单个文件中</li><li>GPTQ 和 AWQ ： GPTQ / EXL2 和 AWQ 等技术引入了逐层校准，可在极低位宽下保持性能。它们使用动态缩放来减少灾难性的异常值，有选择地跳过或重新居中最重要的参数</li><li>SmoothQuant 和 ZeroQuant ：新的量化友好型转换 (SmoothQuant) 和基于编译器的优化 (ZeroQuant) 有助于在量化之前缓解异常值。它们还通过融合某些操作和优化数据流来减少硬件开销</li></ul></li><li>蒸馏</li><li>剪枝</li></ul><h3 id="_4-5-llm-评估" tabindex="-1"><a class="header-anchor" href="#_4-5-llm-评估" aria-hidden="true">#</a> 4.5. LLM 评估</h3><p>可靠地评估LLMs是一项复杂但必不可少的任务，可以指导数据生成和训练。它提供了有关改进领域的宝贵反馈，可以利用这些反馈来修改数据组合、质量和训练参数。</p><ul><li>模型评估 <ul><li>Automated benchmarks 自动评测: 使用精选数据集和指标（如 MMLU）评估特定任务上的模型。它非常适合具体任务，但抽象和创造能力较差。它也容易受到数据污染</li><li>Human evaluation 人工评测: 它涉及人工提示模型并对响应进行评分。方法包括从氛围检查到具有特定指导方针的系统注释和大规模社区投票（竞技场）。它更适合主观任务，但事实准确性较不可靠</li><li>Model-based evaluation 模型评测: 使用判断和奖励模型来评估模型输出。它与人类偏好高度相关，但受到对其自身输出的偏见和不一致的评分的影响</li><li>Feedback signal : 分析错误模式以识别特定弱点，例如遵循复杂指令的局限性、缺乏特定知识或易受对抗性提示的影响。这可以通过更好的数据生成和训练参数来改进</li></ul></li><li>LLM 幻觉 <ul><li>幻觉指的是一本正经的胡说八道：看似流畅自然的表述，实则不符合事实或者是错误的</li><li>幻觉影响了模型的可靠性和可信度，因此需要解决LLM的幻觉问题</li><li>幻觉不一定是有害的，特别是在一些需要创造力或灵感的场合，对幻觉的容忍度取决于具体的应用场景</li><li>幻觉分类 <ul><li>内在幻觉：生成的内容与源内容相矛盾</li><li>外在幻觉：生成的内容不能从源内容中得到验证，既不受源内容支持也不受其反驳</li></ul></li><li>为什么LLM会产生幻觉 <ul><li>训练数据的重复性</li><li>数据噪声的影响</li><li>解码过程中的随机性</li><li>训练与实际应用中的解码差异</li></ul></li><li>什么时候最容易产生幻觉 <ul><li>错误的上下文信息</li><li>上下文与内置知识的冲突</li><li>处理长文本</li><li>处理数值</li><li>逻辑推断障碍</li></ul></li><li>如何度量幻觉 <ul><li>人工评估：成本高</li><li>自动评估 <ul><li>命名实体误差 （命名实体）</li><li>蕴含率 （句子数量）</li></ul></li><li>模型评估</li><li>问答系统</li><li>利用信息提取系统</li></ul></li><li>如何缓解幻觉 <ul><li>创建高质量无噪声的数据集</li><li>利用外部知识验证正确性 RAG</li><li>采样多个输出并检查其一致性</li><li>修改解码策略</li></ul></li></ul></li><li>LLM 重复</li></ul><h2 id="_5-llm应用开发" tabindex="-1"><a class="header-anchor" href="#_5-llm应用开发" aria-hidden="true">#</a> 5. LLM应用开发</h2><h3 id="llm-apis" tabindex="-1"><a class="header-anchor" href="#llm-apis" aria-hidden="true">#</a> LLM APIs</h3><p>LLM API ：API 是一种便捷的部署方式。该领域分为私有领域（ OpenAI 、 Google 、 Anthropic 、 Cohere 等）和开源领域（ OpenRouter 、 Hugging Face 、 Together AI 等）。</p><h3 id="开源-llms" tabindex="-1"><a class="header-anchor" href="#开源-llms" aria-hidden="true">#</a> 开源 LLMs</h3><p>开源LLMs ： Hugging Face Hub 是查找LLMs的好地方。您可以直接在 Hugging Face Spaces 中运行其中一些，也可以在 LM Studio 等应用中下载并在本地运行它们，或者通过 CLI 使用 llama.cpp 或 Ollama 运行它们。</p><h3 id="prompt-engineering" tabindex="-1"><a class="header-anchor" href="#prompt-engineering" aria-hidden="true">#</a> Prompt Engineering</h3><p>提示工程 ：常用技术包括零样本提示、少样本提示、思路链和 ReAct。它们更适用于较大的模型，但也可以适用于较小的模型。</p><p>提示黑客 ：与提示工程相关的不同技术，包括提示注入（劫持模型答案的附加指令）、数据/提示泄露（检索其原始数据/提示）和越狱（制作提示以绕过安全功能）。</p><h3 id="agent-rag" tabindex="-1"><a class="header-anchor" href="#agent-rag" aria-hidden="true">#</a> Agent &amp; RAG</h3><ul><li>文档加载</li><li>文档拆分</li><li>嵌入模型</li><li>向量数据库</li><li>检索</li><li>LangChain 、 LlamaIndex 、 FastRAG</li><li>结构化输出 ：许多任务需要结构化输出，如严格的模板或 JSON 格式。可以使用 LMQL 、 Outlines 、 Guidance 等库来指导生成并遵循给定的结构。</li><li>评估 <ul><li>Ragas 和 DeepEval</li></ul></li></ul>',50),o=[t];function d(h,c){return i(),e("div",null,o)}const m=l(u,[["render",d],["__file","00_llm_roadmap.html.vue"]]);export{m as default};
