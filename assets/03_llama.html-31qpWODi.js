const e=JSON.parse('{"key":"v-19bd3e38","path":"/llm/02_llm_architecture/03_llama.html","title":"LLaMA","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"LLaMA 1. About 公司 : Meta AI HF : https://huggingface.co/meta-llama 论文 : https://ai.meta.com/research/publications/the-llama-3-herd-of-models/ 意义 : 自 2023 年 2 月发布以来，LLaMA 系列模型在学术界和工业界引起了广泛的关注，已经成为了最受欢迎的开源大语言模型之一，许多研究工作都是以其为基座模型进行指令微调或继续预训练，衍生出了众多变体模型，对于推动LLM技术的开源发展和研究进展做出了重要贡献","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/02_llm_architecture/03_llama.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"LLaMA"}],["meta",{"property":"og:description","content":"LLaMA 1. About 公司 : Meta AI HF : https://huggingface.co/meta-llama 论文 : https://ai.meta.com/research/publications/the-llama-3-herd-of-models/ 意义 : 自 2023 年 2 月发布以来，LLaMA 系列模型在学术界和工业界引起了广泛的关注，已经成为了最受欢迎的开源大语言模型之一，许多研究工作都是以其为基座模型进行指令微调或继续预训练，衍生出了众多变体模型，对于推动LLM技术的开源发展和研究进展做出了重要贡献"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-29T07:33:01.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-29T07:33:01.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLaMA\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-29T07:33:01.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. About","slug":"_1-about","link":"#_1-about","children":[]},{"level":2,"title":"2. LLaMA变体模型（非官方）","slug":"_2-llama变体模型-非官方","link":"#_2-llama变体模型-非官方","children":[]},{"level":2,"title":"Llama进阶","slug":"llama进阶","link":"#llama进阶","children":[]},{"level":2,"title":"1. LayerNorm(Layer Normalization，层归一化)","slug":"_1-layernorm-layer-normalization-层归一化","link":"#_1-layernorm-layer-normalization-层归一化","children":[{"level":3,"title":"1.1. LayerNorm的计算公式","slug":"_1-1-layernorm的计算公式","link":"#_1-1-layernorm的计算公式","children":[]},{"level":3,"title":"1.2. BatchNorm和LayerNorm","slug":"_1-2-batchnorm和layernorm","link":"#_1-2-batchnorm和layernorm","children":[]},{"level":3,"title":"1.3. Normalization的几何意义","slug":"_1-3-normalization的几何意义","link":"#_1-3-normalization的几何意义","children":[]},{"level":3,"title":"1.4. RMSNorm","slug":"_1-4-rmsnorm","link":"#_1-4-rmsnorm","children":[]},{"level":3,"title":"1.4. Normalization对Attention的重要性","slug":"_1-4-normalization对attention的重要性","link":"#_1-4-normalization对attention的重要性","children":[]}]},{"level":2,"title":"3. llama为什么不用encoder-decoder,而只用decoder","slug":"_3-llama为什么不用encoder-decoder-而只用decoder","link":"#_3-llama为什么不用encoder-decoder-而只用decoder","children":[]},{"level":2,"title":"4. attention计算为什么要除根号d","slug":"_4-attention计算为什么要除根号d","link":"#_4-attention计算为什么要除根号d","children":[]},{"level":2,"title":"5. attention公式的计算优化","slug":"_5-attention公式的计算优化","link":"#_5-attention公式的计算优化","children":[]},{"level":2,"title":"6. kv_cache,group+kv_cache省了哪一部分计算，省了多少","slug":"_6-kv-cache-group-kv-cache省了哪一部分计算-省了多少","link":"#_6-kv-cache-group-kv-cache省了哪一部分计算-省了多少","children":[]},{"level":2,"title":"7. 旋转位置编码的长度外推是怎么做的","slug":"_7-旋转位置编码的长度外推是怎么做的","link":"#_7-旋转位置编码的长度外推是怎么做的","children":[]},{"level":2,"title":"8. 能不能自定义特殊token","slug":"_8-能不能自定义特殊token","link":"#_8-能不能自定义特殊token","children":[]},{"level":2,"title":"9. pytorch的通用算子汇总(公式和输入输出)","slug":"_9-pytorch的通用算子汇总-公式和输入输出","link":"#_9-pytorch的通用算子汇总-公式和输入输出","children":[]}],"git":{"createdTime":1743233581000,"updatedTime":1743233581000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1}]},"readingTime":{"minutes":6.19,"words":1856},"filePathRelative":"llm/02_llm_architecture/03_llama.md","localizedDate":"March 29, 2025","excerpt":"<h1> LLaMA</h1>\\n<h2> 1. About</h2>\\n<ul>\\n<li>公司 : Meta AI</li>\\n<li>HF : https://huggingface.co/meta-llama</li>\\n<li>论文 : https://ai.meta.com/research/publications/the-llama-3-herd-of-models/</li>\\n<li>意义 : 自 2023 年 2 月发布以来，LLaMA 系列模型在学术界和工业界引起了广泛的关注，已经成为了最受欢迎的开源大语言模型之一，许多研究工作都是以其为基座模型进行指令微调或继续预训练，衍生出了众多变体模型，对于推动LLM技术的开源发展和研究进展做出了重要贡献</li>\\n</ul>","autoDesc":true}');export{e as data};
