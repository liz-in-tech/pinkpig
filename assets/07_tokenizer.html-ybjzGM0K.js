const n=JSON.parse('{"key":"v-3d95b3b8","path":"/llm/03_llm_training/07_tokenizer.html","title":"Tokenization（词元化，分词）","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"Tokenization（词元化，分词） token 词元 token 和单词不一样，token 是一个比单词更小的分割单位 tokenizer 分词器 Tokenization / Segment 词元化，token化，分词 目的 : 将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据 1. 分词方式 基于词汇word的分词方法 优点 : 分词最直观的解决方式 缺点 : 存在OOV的问题（OOV，out of vocabulary, 未登录词）(word模型的word数量太多，存在大量稀疏word，删掉它们又会导致OOV问题) 基于字符character的分词方式 对26个字母训练word2vec，每个词由其字母的embedding拼接或者求平均得到 优点：解决了OOV问题 缺点：和word级别的模型效果差不多，并没有显著优势。而且如果用RNN来训练character级别的模型也有它的问题，就是训练起来非常慢，原来的一个word，现在变成了七八个character，时间步长增加了很多，训练和预测都更久了，而且梯度消失（爆炸）的问题也会更严重 基于子词subword的分词方法 子词分词器（Subword Tokenizer）被广泛应用于基于 Transformer 的语言模型中 优点：作为character和word的折中模型，解决了OOV问题，训练也不会特别慢 三种常见方法 BPE 分词 BPE 通过合并高频字符对形成子词词汇表 BPE算法 (Byte Pair Encoding) 是上世纪提出的一种数据压缩算法，后来被广泛应用于 NLP 核心思想：BPE通过迭代地合并出现频率最高的字符或子词单元，最终形成一个最优的子词词汇表，使得常见的词可以用较少的子词单元表示，而不常见的词则可以拆分为更小的可组合单元 优点 减少 OOV 问题：通过将不常见词拆分为更小的可识别子词，提升模型泛化能力 平衡词汇表大小与覆盖率：常见词可以用较少子词表示，不常见词则拆分为组合单元 适用于多种语言：特别适合形态丰富的语言（如德语、芬兰语），以及无空格分隔的语言（如中文） 代表性模型： GPT-2 、BART 和 LLaMA WordPiece 分词 WordPiece 和BPE类似但选择合并对的方式不同 是谷歌内部非公开的分词算法 代表性模型： BERT Unigram 分词 Unigram 从大词汇表逐步移除低频子词 代表性模型：T5 和 mBART 两种实现方式 模型结构和word模型完全一样，只不过把word换成了subword 关键点：怎样得到subword BPE、WordPiece 和 Unigram 这三种常见分词方法都属于此 word和character模型的杂交模型 字典中的词用word分词模型，对于不太常见的词则用character分词模型","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/03_llm_training/07_tokenizer.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Tokenization（词元化，分词）"}],["meta",{"property":"og:description","content":"Tokenization（词元化，分词） token 词元 token 和单词不一样，token 是一个比单词更小的分割单位 tokenizer 分词器 Tokenization / Segment 词元化，token化，分词 目的 : 将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据 1. 分词方式 基于词汇word的分词方法 优点 : 分词最直观的解决方式 缺点 : 存在OOV的问题（OOV，out of vocabulary, 未登录词）(word模型的word数量太多，存在大量稀疏word，删掉它们又会导致OOV问题) 基于字符character的分词方式 对26个字母训练word2vec，每个词由其字母的embedding拼接或者求平均得到 优点：解决了OOV问题 缺点：和word级别的模型效果差不多，并没有显著优势。而且如果用RNN来训练character级别的模型也有它的问题，就是训练起来非常慢，原来的一个word，现在变成了七八个character，时间步长增加了很多，训练和预测都更久了，而且梯度消失（爆炸）的问题也会更严重 基于子词subword的分词方法 子词分词器（Subword Tokenizer）被广泛应用于基于 Transformer 的语言模型中 优点：作为character和word的折中模型，解决了OOV问题，训练也不会特别慢 三种常见方法 BPE 分词 BPE 通过合并高频字符对形成子词词汇表 BPE算法 (Byte Pair Encoding) 是上世纪提出的一种数据压缩算法，后来被广泛应用于 NLP 核心思想：BPE通过迭代地合并出现频率最高的字符或子词单元，最终形成一个最优的子词词汇表，使得常见的词可以用较少的子词单元表示，而不常见的词则可以拆分为更小的可组合单元 优点 减少 OOV 问题：通过将不常见词拆分为更小的可识别子词，提升模型泛化能力 平衡词汇表大小与覆盖率：常见词可以用较少子词表示，不常见词则拆分为组合单元 适用于多种语言：特别适合形态丰富的语言（如德语、芬兰语），以及无空格分隔的语言（如中文） 代表性模型： GPT-2 、BART 和 LLaMA WordPiece 分词 WordPiece 和BPE类似但选择合并对的方式不同 是谷歌内部非公开的分词算法 代表性模型： BERT Unigram 分词 Unigram 从大词汇表逐步移除低频子词 代表性模型：T5 和 mBART 两种实现方式 模型结构和word模型完全一样，只不过把word换成了subword 关键点：怎样得到subword BPE、WordPiece 和 Unigram 这三种常见分词方法都属于此 word和character模型的杂交模型 字典中的词用word分词模型，对于不太常见的词则用character分词模型"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Tokenization（词元化，分词）\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 分词方式","slug":"_1-分词方式","link":"#_1-分词方式","children":[]},{"level":2,"title":"2. 分词器 Tokenizer 的选择","slug":"_2-分词器-tokenizer-的选择","link":"#_2-分词器-tokenizer-的选择","children":[]},{"level":2,"title":"3. Token计算","slug":"_3-token计算","link":"#_3-token计算","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":3.78,"words":1133},"filePathRelative":"llm/03_llm_training/07_tokenizer.md","localizedDate":"March 28, 2025","excerpt":"<h1> Tokenization（词元化，分词）</h1>\\n<ul>\\n<li>token 词元\\n<ul>\\n<li>token 和单词不一样，token 是一个比单词更小的分割单位</li>\\n</ul>\\n</li>\\n<li>tokenizer 分词器</li>\\n<li>Tokenization / Segment 词元化，token化，分词</li>\\n</ul>\\n<p>目的 : 将原始文本分割成模型可识别和建模的词元序列，作为大语言模型的输入数据</p>\\n<h2> 1. 分词方式</h2>\\n<ul>\\n<li>基于词汇word的分词方法\\n<ul>\\n<li>优点 : 分词最直观的解决方式</li>\\n<li>缺点 : 存在OOV的问题（OOV，out of vocabulary, 未登录词）(word模型的word数量太多，存在大量稀疏word，删掉它们又会导致OOV问题)</li>\\n</ul>\\n</li>\\n<li>基于字符character的分词方式\\n<ul>\\n<li>对26个字母训练word2vec，每个词由其字母的embedding拼接或者求平均得到</li>\\n<li>优点：解决了OOV问题</li>\\n<li>缺点：和word级别的模型效果差不多，并没有显著优势。而且如果用RNN来训练character级别的模型也有它的问题，就是训练起来非常慢，原来的一个word，现在变成了七八个character，时间步长增加了很多，训练和预测都更久了，而且梯度消失（爆炸）的问题也会更严重</li>\\n</ul>\\n</li>\\n<li>基于子词subword的分词方法\\n<ul>\\n<li>子词分词器（Subword Tokenizer）被广泛应用于基于 Transformer 的语言模型中</li>\\n<li>优点：作为character和word的折中模型，解决了OOV问题，训练也不会特别慢</li>\\n<li>三种常见方法\\n<ul>\\n<li>BPE 分词\\n<ul>\\n<li>BPE 通过合并高频字符对形成子词词汇表</li>\\n<li>BPE算法 (Byte Pair Encoding)\\n<ul>\\n<li>是上世纪提出的一种数据压缩算法，后来被广泛应用于 NLP</li>\\n<li>核心思想：BPE通过迭代地合并出现频率最高的字符或子词单元，最终形成一个最优的子词词汇表，使得常见的词可以用较少的子词单元表示，而不常见的词则可以拆分为更小的可组合单元</li>\\n</ul>\\n</li>\\n<li>优点\\n<ul>\\n<li>减少 OOV 问题：通过将不常见词拆分为更小的可识别子词，提升模型泛化能力</li>\\n<li>平衡词汇表大小与覆盖率：常见词可以用较少子词表示，不常见词则拆分为组合单元</li>\\n<li>适用于多种语言：特别适合形态丰富的语言（如德语、芬兰语），以及无空格分隔的语言（如中文）</li>\\n</ul>\\n</li>\\n<li>代表性模型： GPT-2 、BART 和 LLaMA</li>\\n</ul>\\n</li>\\n<li>WordPiece 分词\\n<ul>\\n<li>WordPiece 和BPE类似但选择合并对的方式不同</li>\\n<li>是谷歌内部非公开的分词算法</li>\\n<li>代表性模型： BERT</li>\\n</ul>\\n</li>\\n<li>Unigram 分词\\n<ul>\\n<li>Unigram 从大词汇表逐步移除低频子词</li>\\n<li>代表性模型：T5 和 mBART</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>两种实现方式\\n<ul>\\n<li>模型结构和word模型完全一样，只不过把word换成了subword\\n<ul>\\n<li>关键点：怎样得到subword</li>\\n<li>BPE、WordPiece 和 Unigram 这三种常见分词方法都属于此</li>\\n</ul>\\n</li>\\n<li>word和character模型的杂交模型\\n<ul>\\n<li>字典中的词用word分词模型，对于不太常见的词则用character分词模型</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{n as data};
