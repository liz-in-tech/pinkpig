const e=JSON.parse('{"key":"v-71694ba2","path":"/llm/03_llm_training/02_LLM%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B.html","title":"LLM的构建过程","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"LLM的构建过程 1. Pre-Training（预训练） 使用与下游任务无关的大规模数据进行模型参数的初始训练，从而找到模型参数较好的初始值 OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。 现有大模型技术路径 : “解码器架构 + 预测下一个词” 数据处理 0.收集 1.清洗 （去除掉可能包含有毒有害的内容） 2.Tokenization（词元化） 3.Batch（分批） 数据规模 目前的开源模型普遍采用 2∼3T 规模的词元进行预训练，并有趋势进一步扩大这一规模。 算力资源 这一过程对于算力需求量极高，一般来说训练百亿模型至少需要百卡规模的算力集群（如 A100 80G）联合训练数月时间（与具体的算力资源相关）；而训练千亿模型则需要千卡甚至万卡规模的算力集群，对于算力资源的消耗非常惊人。 人才需求 尽管整体的预训练技术框架非常直观，但是实施过程中涉及到大量需要深入探索的经验性技术，如数据如何进行配比、如何进行学习率的调整、如何早期发现模型的异常行为等。预训练过程需要考虑各种实施细节，而这些细节有很多并没有公开发表的经验可循，需要研发人员具有丰富的训练经验和异常处理能力，避免大规模训练开始以后进行回退和反复迭代，从而减少算力资源的浪费，提升训练成功的几率。大语言模型的研发看似是一个算力需求型的工程，实际上相关人才是最重要的。可以说，一个大语言模型项目的核心训练人员的能力最后会决定模型的整体水平。","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/03_llm_training/02_LLM%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"LLM的构建过程"}],["meta",{"property":"og:description","content":"LLM的构建过程 1. Pre-Training（预训练） 使用与下游任务无关的大规模数据进行模型参数的初始训练，从而找到模型参数较好的初始值 OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。 现有大模型技术路径 : “解码器架构 + 预测下一个词” 数据处理 0.收集 1.清洗 （去除掉可能包含有毒有害的内容） 2.Tokenization（词元化） 3.Batch（分批） 数据规模 目前的开源模型普遍采用 2∼3T 规模的词元进行预训练，并有趋势进一步扩大这一规模。 算力资源 这一过程对于算力需求量极高，一般来说训练百亿模型至少需要百卡规模的算力集群（如 A100 80G）联合训练数月时间（与具体的算力资源相关）；而训练千亿模型则需要千卡甚至万卡规模的算力集群，对于算力资源的消耗非常惊人。 人才需求 尽管整体的预训练技术框架非常直观，但是实施过程中涉及到大量需要深入探索的经验性技术，如数据如何进行配比、如何进行学习率的调整、如何早期发现模型的异常行为等。预训练过程需要考虑各种实施细节，而这些细节有很多并没有公开发表的经验可循，需要研发人员具有丰富的训练经验和异常处理能力，避免大规模训练开始以后进行回退和反复迭代，从而减少算力资源的浪费，提升训练成功的几率。大语言模型的研发看似是一个算力需求型的工程，实际上相关人才是最重要的。可以说，一个大语言模型项目的核心训练人员的能力最后会决定模型的整体水平。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM的构建过程\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. Pre-Training（预训练）","slug":"_1-pre-training-预训练","link":"#_1-pre-training-预训练","children":[]},{"level":2,"title":"2. Fine-tuning（微调，指令微调，有监督微调）","slug":"_2-fine-tuning-微调-指令微调-有监督微调","link":"#_2-fine-tuning-微调-指令微调-有监督微调","children":[]},{"level":2,"title":"3. Alignment（对齐，人类对齐，对齐微调）","slug":"_3-alignment-对齐-人类对齐-对齐微调","link":"#_3-alignment-对齐-人类对齐-对齐微调","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":3.79,"words":1138},"filePathRelative":"llm/03_llm_training/02_LLM构建过程.md","localizedDate":"March 28, 2025","excerpt":"<h1> LLM的构建过程</h1>\\n<h2> 1. Pre-Training（预训练）</h2>\\n<p>使用与下游任务无关的大规模数据进行模型参数的初始训练，从而找到模型参数较好的初始值</p>\\n<p>OpenAI 前首席科学家 Ilya Sutskever 在公开采访中指出大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个编码世界知识的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。</p>\\n<p>现有大模型技术路径 : “解码器架构 + 预测下一个词”</p>\\n<ul>\\n<li>数据处理\\n<ul>\\n<li>0.收集</li>\\n<li>1.清洗 （去除掉可能包含有毒有害的内容）</li>\\n<li>2.Tokenization（词元化）</li>\\n<li>3.Batch（分批）</li>\\n</ul>\\n</li>\\n<li>数据规模\\n<ul>\\n<li>目前的开源模型普遍采用 2∼3T 规模的词元进行预训练，并有趋势进一步扩大这一规模。</li>\\n</ul>\\n</li>\\n<li>算力资源\\n<ul>\\n<li>这一过程对于算力需求量极高，一般来说训练百亿模型至少需要百卡规模的算力集群（如 A100 80G）联合训练数月时间（与具体的算力资源相关）；而训练千亿模型则需要千卡甚至万卡规模的算力集群，对于算力资源的消耗非常惊人。</li>\\n</ul>\\n</li>\\n<li>人才需求\\n<ul>\\n<li>尽管整体的预训练技术框架非常直观，但是实施过程中涉及到大量需要深入探索的经验性技术，如数据如何进行配比、如何进行学习率的调整、如何早期发现模型的异常行为等。预训练过程需要考虑各种实施细节，而这些细节有很多并没有公开发表的经验可循，需要研发人员具有丰富的训练经验和异常处理能力，避免大规模训练开始以后进行回退和反复迭代，从而减少算力资源的浪费，提升训练成功的几率。大语言模型的研发看似是一个算力需求型的工程，实际上相关人才是最重要的。可以说，一个大语言模型项目的核心训练人员的能力最后会决定模型的整体水平。</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{e as data};
