const t=JSON.parse('{"key":"v-7cdb0286","path":"/llm/04_llm_reasoning/06_LLM%E5%8E%8B%E7%BC%A9.html","title":"低资源部署策略/模型压缩技术 (修改模型权重本身来减少每个 GPU 上的内存使用)","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"低资源部署策略/模型压缩技术 (修改模型权重本身来减少每个 GPU 上的内存使用) 1. 模型量化 / Quantization / 混合精度 降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。 2. 模型剪枝 / 稀疏 Sparsity / 正则化 与模型量化不同，模型蒸馏和模型剪枝则通过精简模型的结构，进而减少参数的数量。 3. 模型蒸馏 / Distillation","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/04_llm_reasoning/06_LLM%E5%8E%8B%E7%BC%A9.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"低资源部署策略/模型压缩技术 (修改模型权重本身来减少每个 GPU 上的内存使用)"}],["meta",{"property":"og:description","content":"低资源部署策略/模型压缩技术 (修改模型权重本身来减少每个 GPU 上的内存使用) 1. 模型量化 / Quantization / 混合精度 降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。 2. 模型剪枝 / 稀疏 Sparsity / 正则化 与模型量化不同，模型蒸馏和模型剪枝则通过精简模型的结构，进而减少参数的数量。 3. 模型蒸馏 / Distillation"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-29T07:33:01.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-29T07:33:01.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"低资源部署策略/模型压缩技术 (修改模型权重本身来减少每个 GPU 上的内存使用)\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-29T07:33:01.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 模型量化 / Quantization / 混合精度","slug":"_1-模型量化-quantization-混合精度","link":"#_1-模型量化-quantization-混合精度","children":[]},{"level":2,"title":"2. 模型剪枝 / 稀疏 Sparsity / 正则化","slug":"_2-模型剪枝-稀疏-sparsity-正则化","link":"#_2-模型剪枝-稀疏-sparsity-正则化","children":[]},{"level":2,"title":"3. 模型蒸馏 / Distillation","slug":"_3-模型蒸馏-distillation","link":"#_3-模型蒸馏-distillation","children":[]}],"git":{"createdTime":1743233581000,"updatedTime":1743233581000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1}]},"readingTime":{"minutes":0.94,"words":282},"filePathRelative":"llm/04_llm_reasoning/06_LLM压缩.md","localizedDate":"March 29, 2025","excerpt":"<h1> 低资源部署策略/模型压缩技术 (修改模型权重本身来减少每个 GPU 上的内存使用)</h1>\\n<h2> 1. 模型量化 / Quantization / 混合精度</h2>\\n<p>降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。</p>\\n<h2> 2. 模型剪枝 / 稀疏 Sparsity / 正则化</h2>\\n<p>与模型量化不同，模型蒸馏和模型剪枝则通过精简模型的结构，进而减少参数的数量。</p>\\n<h2> 3. 模型蒸馏 / Distillation</h2>","autoDesc":true}');export{t as data};
