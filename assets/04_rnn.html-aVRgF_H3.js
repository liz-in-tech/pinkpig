import{_ as a}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as i,c as e,e as n}from"./app-CV1ZlhnZ.js";const r="/pinkpig/assets/nlp_026-_LNoFfj7.png",t="/pinkpig/assets/nlp_027-uhyOkBfV.png",p="/pinkpig/assets/nlp_028-MAmJjJey.png",s="/pinkpig/assets/nlp_029-sa921UPD.png",d="/pinkpig/assets/nlp_030-LoY13KCl.png",h="/pinkpig/assets/nlp_031-Cxn7q_G6.png",c="/pinkpig/assets/nlp_032-wk_NWX-Z.png",o="/pinkpig/assets/nlp_033-OukAWRmr.png",l="/pinkpig/assets/nlp_034-GTRGNKRm.png",g="/pinkpig/assets/nlp_035-7Ob2RTsx.png",_="/pinkpig/assets/nlp_036-VOkOlLzA.png",f="/pinkpig/assets/nlp_037-lalIcPi_.png",u={},m=n('<h1 id="rnns" tabindex="-1"><a class="header-anchor" href="#rnns" aria-hidden="true">#</a> RNNs</h1><h2 id="_1-优点" tabindex="-1"><a class="header-anchor" href="#_1-优点" aria-hidden="true">#</a> 1. 优点</h2><p>模型有了记忆力</p><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_2-缺点" tabindex="-1"><a class="header-anchor" href="#_2-缺点" aria-hidden="true">#</a> 2. 缺点</h2><p>只能串行训练，无法并行训练</p><h2 id="_3-适用" tabindex="-1"><a class="header-anchor" href="#_3-适用" aria-hidden="true">#</a> 3. 适用</h2><p>循环神经网络非常适合用于具有序列特征的数据的识别</p><h2 id="_4-典型模型" tabindex="-1"><a class="header-anchor" href="#_4-典型模型" aria-hidden="true">#</a> 4. 典型模型</h2><h3 id="_4-1-native-rnn-传统rnn" tabindex="-1"><a class="header-anchor" href="#_4-1-native-rnn-传统rnn" aria-hidden="true">#</a> 4.1. Native RNN(传统RNN)</h3><h4 id="_4-1-1-公式-模型结构就是公式的表示" tabindex="-1"><a class="header-anchor" href="#_4-1-1-公式-模型结构就是公式的表示" aria-hidden="true">#</a> 4.1.1. 公式: 模型结构就是公式的表示</h4><figure><img src="'+t+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_4-1-2-优点" tabindex="-1"><a class="header-anchor" href="#_4-1-2-优点" aria-hidden="true">#</a> 4.1.2. 优点</h4><p>模型有了记忆</p><h4 id="_4-1-3-缺点" tabindex="-1"><a class="header-anchor" href="#_4-1-3-缺点" aria-hidden="true">#</a> 4.1.3. 缺点</h4><p>记忆力过强，每个时间步的隐藏状态都包含了全部历史时刻的信息</p><p>导致的问题</p><ul><li>信息价值逐渐降低 <ul><li>因此随着时间步的增加，隐藏状态中的信息会越来越多，其中有价值的信息含量的比率会越来越少</li><li>为了解决这个问题，我们要想办法给Native RNN引入遗忘机制</li></ul></li><li>存在梯度消失问题 <ul><li>处理长序列时存在梯度消失和梯度爆炸的问题</li><li>梯度爆炸相对而言比较好解决，通过梯度裁剪、调整学习率等方法就可以很好的控制，但是处理梯度消失就非常的麻烦（所以后面我们也会更多的关注梯度消失的问题）。当梯度消失时，网络就很难学习到序列中的长期依赖关系。</li><li>梯度消失会带来哪些问题呢？一个很明显的问题就是参数更新更多的受到临近词的影响，那些和当前时刻t较远的词对当前的参数更新影响很小。</li></ul></li></ul><h3 id="_4-2-lstm" tabindex="-1"><a class="header-anchor" href="#_4-2-lstm" aria-hidden="true">#</a> 4.2. LSTM</h3><p>LSTM是Native RNN的变体，对Native RNN做了优化</p><p>Native RNN中，h代表记忆；LSTM中，c代表记忆</p><figure><img src="'+s+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_4-2-1-公式-别被图吓到-不难哦-图就是公式的表示" tabindex="-1"><a class="header-anchor" href="#_4-2-1-公式-别被图吓到-不难哦-图就是公式的表示" aria-hidden="true">#</a> 4.2.1. 公式：别被图吓到，不难哦，图就是公式的表示</h4><figure><img src="'+d+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Note：因为xmind写不了公式，这里用括号表示下标</p><p>1.拼接x(t)和h(t-1)为一个新的更长的向量</p><p>2.计算出4个向量z,z(f),z(i),z(o)</p><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_4-2-2-3个阶段" tabindex="-1"><a class="header-anchor" href="#_4-2-2-3个阶段" aria-hidden="true">#</a> 4.2.2. 3个阶段</h4><h5 id="_4-2-2-1-忘记阶段" tabindex="-1"><a class="header-anchor" href="#_4-2-2-1-忘记阶段" aria-hidden="true">#</a> 4.2.2.1. 忘记阶段</h5><p>对上一个节点传进来的输入进行选择性忘记</p><p>忘记门控：z(f)</p><p>这个能力通过学习W(f)参数来实现</p><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="_4-2-2-2-选择记忆阶段" tabindex="-1"><a class="header-anchor" href="#_4-2-2-2-选择记忆阶段" aria-hidden="true">#</a> 4.2.2.2. 选择记忆阶段</h5><p>记忆门控：z和z(i)</p><p>这个能力通过学习W和W(i)参数来实现</p><h5 id="_4-2-2-3-输出阶段" tabindex="-1"><a class="header-anchor" href="#_4-2-2-3-输出阶段" aria-hidden="true">#</a> 4.2.2.3. 输出阶段</h5><p>决定哪些将会被当成当前状态的输出</p><p>输出门控：z(o)</p><p>这个能力通过学习W(o)参数来实现</p><h4 id="_4-2-3-如何从图里看公式-从输出看到输入" tabindex="-1"><a class="header-anchor" href="#_4-2-3-如何从图里看公式-从输出看到输入" aria-hidden="true">#</a> 4.2.3. 如何从图里看公式：从输出看到输入</h4><p>不要从输入看到输出，这样会很混乱，而应该找准一个公式后，看哪些箭头指向了输出，从输出看到输入</p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="_4-2-4-优-缓解了梯度问题" tabindex="-1"><a class="header-anchor" href="#_4-2-4-优-缓解了梯度问题" aria-hidden="true">#</a> 4.2.4. 优：缓解了梯度问题</h5><figure><img src="'+_+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="_4-2-5-缺" tabindex="-1"><a class="header-anchor" href="#_4-2-5-缺" aria-hidden="true">#</a> 4.2.5. 缺</h5><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_4-3-gru" tabindex="-1"><a class="header-anchor" href="#_4-3-gru" aria-hidden="true">#</a> 4.3. GRU</h3><p>GRU是LSTM的变体，对LSTM做了优化</p><p>GRU很聪明的一点就在于，它采用了一些巧妙的构思，让一个门控同时可以进行遗忘和选择记忆，而LSTM则要使用多个门控，因此GRU很多时候都会更加实用。</p>',54),x=[m];function b(N,z){return i(),e("div",null,x)}const y=a(u,[["render",b],["__file","04_rnn.html.vue"]]);export{y as default};
