const n=JSON.parse('{"key":"v-57134c01","path":"/llm/04_llm_reasoning/05_%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.html","title":"优化技术","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"优化技术 1. 优化技术概览 优化点都是从时间和空间两方面来考虑 全过程优化 注意力优化（计算优化） KV Cache：缓存KV，避免重复计算 MQA 多查询注意力：KV共享 GQA 分组查询注意力：分组KV共享 Flash Attention V1、V2 PagedAttention 批次管理优化 动态批处理 / 连续批处理 /continus batching / In-flight batching 分布式/并行化 将模型分布在多个 GPU 上 在多 GPU/节点 上分担计算和存储压力 量化/混合精度 将模型参数从高精度转换为低精度，在不显著影响效果的前提下降低计算负担 剪枝/稀疏化Sparsity/正则化技术 移除不重要的权重或神经元，减少参数数量 正则化技术：dropout &amp; L1 &amp; L2 早退机制 / 提前停止 训练优化 解决的问题 适配到已有的GPU资源，使模型可在资源受限情况下训练（空间） 加速训练收敛，降低训练时间，提速（时间） 稳定训练过程，防止梯度爆炸和梯度消失 高吞吐量，提高资源利用率，减少浪费 主要技术和方法 激活值重新计算 梯度累积 超参数调整 动态学习率调整：使用调整超参数的现代优化器（AdamW、Lion），使用余弦退火、衰减、预热 warm-up策略，帮助模型更快收敛或改善稳定性 批大小的选择 通用优化 注意力优化 批次管理优化 分布式/并行化 混合精度训练 剪枝/正则化技术：dropout &amp; L1 &amp; L2 早退机制/提前停止 在验证损失不再改善时停止训练，节省计算资源 推理优化 解决的问题 - 减少模型大小、内存占用和计算需求，使模型可部署在资源受限设备（如移动设备）（空间） - 降低推理时间，提速，确保低延迟，优化用户体验（时间） - 高吞吐量，提高资源利用率，减少浪费 主要技术和方法 训练后模型压缩优化 量化 Quantization / 混合精度 降低模型权重和激活值的精度（如从32位浮点数到8位整数），减少内存占用和计算成本 剪枝 / 稀疏 Sparsity / 正则化技术 移除不重要的权重或神经元，减少参数数量，降低推理时间 蒸馏 Distillation / 知识蒸馏 训练较小模型以模仿较大模型的行为，保留大部分性能，同时显著减少模型大小 解码策略优化 推测解码（Speculative Decoding） 生成草稿token序列，并行验证保留可采纳的多个token，加速自回归解码 非自回归解码（Non-autoregressive Decoding） 一次生成多个token，减少序列依赖，提升速度 早退机制（Early Exiting） 在满足条件时提前结束推理，节省计算 级联解码（Cascade Inference） 把多个模型按效率排序（越往后的模型越大生成质量越高），依次进行生成，直到符合想要的质量后停止 通用优化 注意力优化 批次管理优化 混合精度推理 分布式/并行化 剪枝 早退","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/04_llm_reasoning/05_%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"优化技术"}],["meta",{"property":"og:description","content":"优化技术 1. 优化技术概览 优化点都是从时间和空间两方面来考虑 全过程优化 注意力优化（计算优化） KV Cache：缓存KV，避免重复计算 MQA 多查询注意力：KV共享 GQA 分组查询注意力：分组KV共享 Flash Attention V1、V2 PagedAttention 批次管理优化 动态批处理 / 连续批处理 /continus batching / In-flight batching 分布式/并行化 将模型分布在多个 GPU 上 在多 GPU/节点 上分担计算和存储压力 量化/混合精度 将模型参数从高精度转换为低精度，在不显著影响效果的前提下降低计算负担 剪枝/稀疏化Sparsity/正则化技术 移除不重要的权重或神经元，减少参数数量 正则化技术：dropout &amp; L1 &amp; L2 早退机制 / 提前停止 训练优化 解决的问题 适配到已有的GPU资源，使模型可在资源受限情况下训练（空间） 加速训练收敛，降低训练时间，提速（时间） 稳定训练过程，防止梯度爆炸和梯度消失 高吞吐量，提高资源利用率，减少浪费 主要技术和方法 激活值重新计算 梯度累积 超参数调整 动态学习率调整：使用调整超参数的现代优化器（AdamW、Lion），使用余弦退火、衰减、预热 warm-up策略，帮助模型更快收敛或改善稳定性 批大小的选择 通用优化 注意力优化 批次管理优化 分布式/并行化 混合精度训练 剪枝/正则化技术：dropout &amp; L1 &amp; L2 早退机制/提前停止 在验证损失不再改善时停止训练，节省计算资源 推理优化 解决的问题 - 减少模型大小、内存占用和计算需求，使模型可部署在资源受限设备（如移动设备）（空间） - 降低推理时间，提速，确保低延迟，优化用户体验（时间） - 高吞吐量，提高资源利用率，减少浪费 主要技术和方法 训练后模型压缩优化 量化 Quantization / 混合精度 降低模型权重和激活值的精度（如从32位浮点数到8位整数），减少内存占用和计算成本 剪枝 / 稀疏 Sparsity / 正则化技术 移除不重要的权重或神经元，减少参数数量，降低推理时间 蒸馏 Distillation / 知识蒸馏 训练较小模型以模仿较大模型的行为，保留大部分性能，同时显著减少模型大小 解码策略优化 推测解码（Speculative Decoding） 生成草稿token序列，并行验证保留可采纳的多个token，加速自回归解码 非自回归解码（Non-autoregressive Decoding） 一次生成多个token，减少序列依赖，提升速度 早退机制（Early Exiting） 在满足条件时提前结束推理，节省计算 级联解码（Cascade Inference） 把多个模型按效率排序（越往后的模型越大生成质量越高），依次进行生成，直到符合想要的质量后停止 通用优化 注意力优化 批次管理优化 混合精度推理 分布式/并行化 剪枝 早退"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-04-23T13:53:11.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-04-23T13:53:11.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"优化技术\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-04-23T13:53:11.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 优化技术概览","slug":"_1-优化技术概览","link":"#_1-优化技术概览","children":[]},{"level":2,"title":"2. 注意力机制优化","slug":"_2-注意力机制优化","link":"#_2-注意力机制优化","children":[]},{"level":2,"title":"3. 动态批处理 / 连续批处理 /continus batching / In-flight batching","slug":"_3-动态批处理-连续批处理-continus-batching-in-flight-batching","link":"#_3-动态批处理-连续批处理-continus-batching-in-flight-batching","children":[]}],"git":{"createdTime":1743233581000,"updatedTime":1745416391000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro-6.local","commits":1}]},"readingTime":{"minutes":5.44,"words":1632},"filePathRelative":"llm/04_llm_reasoning/05_优化技术.md","localizedDate":"March 29, 2025","excerpt":"<h1> 优化技术</h1>\\n<h2> 1. 优化技术概览</h2>\\n<p>优化点都是从时间和空间两方面来考虑</p>\\n<ul>\\n<li>全过程优化\\n<ul>\\n<li>注意力优化（计算优化）\\n<ul>\\n<li>KV Cache：缓存KV，避免重复计算</li>\\n<li>MQA 多查询注意力：KV共享</li>\\n<li>GQA 分组查询注意力：分组KV共享</li>\\n<li>Flash Attention V1、V2</li>\\n<li>PagedAttention</li>\\n</ul>\\n</li>\\n<li>批次管理优化\\n<ul>\\n<li>动态批处理 / 连续批处理 /continus batching / In-flight batching</li>\\n</ul>\\n</li>\\n<li>分布式/并行化\\n<ul>\\n<li>将模型分布在多个 GPU 上</li>\\n<li>在多 GPU/节点 上分担计算和存储压力</li>\\n</ul>\\n</li>\\n<li>量化/混合精度\\n<ul>\\n<li>将模型参数从高精度转换为低精度，在不显著影响效果的前提下降低计算负担</li>\\n</ul>\\n</li>\\n<li>剪枝/稀疏化Sparsity/正则化技术\\n<ul>\\n<li>移除不重要的权重或神经元，减少参数数量</li>\\n<li>正则化技术：dropout &amp; L1 &amp; L2</li>\\n</ul>\\n</li>\\n<li>早退机制 / 提前停止</li>\\n</ul>\\n</li>\\n<li>训练优化\\n<ul>\\n<li>解决的问题\\n<ul>\\n<li>适配到已有的GPU资源，使模型可在资源受限情况下训练（空间）</li>\\n<li>加速训练收敛，降低训练时间，提速（时间）</li>\\n<li>稳定训练过程，防止梯度爆炸和梯度消失</li>\\n<li>高吞吐量，提高资源利用率，减少浪费</li>\\n</ul>\\n</li>\\n<li>主要技术和方法\\n<ul>\\n<li>激活值重新计算</li>\\n<li>梯度累积</li>\\n<li>超参数调整\\n<ul>\\n<li>动态学习率调整：使用调整超参数的现代优化器（AdamW、Lion），使用余弦退火、衰减、预热 warm-up策略，帮助模型更快收敛或改善稳定性</li>\\n<li>批大小的选择</li>\\n</ul>\\n</li>\\n<li>通用优化\\n<ul>\\n<li>注意力优化</li>\\n<li>批次管理优化</li>\\n<li>分布式/并行化</li>\\n<li>混合精度训练</li>\\n<li>剪枝/正则化技术：dropout &amp; L1 &amp; L2</li>\\n<li>早退机制/提前停止\\n<ul>\\n<li>在验证损失不再改善时停止训练，节省计算资源</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>推理优化\\n<ul>\\n<li>解决的问题\\n- 减少模型大小、内存占用和计算需求，使模型可部署在资源受限设备（如移动设备）（空间）\\n- 降低推理时间，提速，确保低延迟，优化用户体验（时间）\\n- 高吞吐量，提高资源利用率，减少浪费</li>\\n<li>主要技术和方法\\n<ul>\\n<li>训练后模型压缩优化\\n<ul>\\n<li>量化 Quantization / 混合精度\\n<ul>\\n<li>降低模型权重和激活值的精度（如从32位浮点数到8位整数），减少内存占用和计算成本</li>\\n</ul>\\n</li>\\n<li>剪枝 / 稀疏 Sparsity / 正则化技术\\n<ul>\\n<li>移除不重要的权重或神经元，减少参数数量，降低推理时间</li>\\n</ul>\\n</li>\\n<li>蒸馏 Distillation / 知识蒸馏\\n<ul>\\n<li>训练较小模型以模仿较大模型的行为，保留大部分性能，同时显著减少模型大小</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>解码策略优化\\n<ul>\\n<li>推测解码（Speculative Decoding）\\n<ul>\\n<li>生成草稿token序列，并行验证保留可采纳的多个token，加速自回归解码</li>\\n</ul>\\n</li>\\n<li>非自回归解码（Non-autoregressive Decoding）\\n<ul>\\n<li>一次生成多个token，减少序列依赖，提升速度</li>\\n</ul>\\n</li>\\n<li>早退机制（Early Exiting）\\n<ul>\\n<li>在满足条件时提前结束推理，节省计算</li>\\n</ul>\\n</li>\\n<li>级联解码（Cascade Inference）\\n<ul>\\n<li>把多个模型按效率排序（越往后的模型越大生成质量越高），依次进行生成，直到符合想要的质量后停止</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>通用优化\\n<ul>\\n<li>注意力优化</li>\\n<li>批次管理优化</li>\\n<li>混合精度推理</li>\\n<li>分布式/并行化</li>\\n<li>剪枝</li>\\n<li>早退</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{n as data};
