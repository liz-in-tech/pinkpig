const e=JSON.parse('{"key":"v-1b945999","path":"/llm/06_llm_experience/llm%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E4%BB%A5%E5%8F%8AGPU%E7%9A%84%E9%80%89%E6%8B%A9.html","title":"GPU显存占用计算以及GPU的选择","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"GPU显存占用计算以及GPU的选择 从第一性原则出发，要回答的第一个问题就是，为什么要计算大模型占用的显存资源？一句话概括：显存太小，模型无法运行；显存太大，浪费金钱。所以从成本的角度来看，很有必要分析计算大模型的资源占用 当你手头想要部署某个开源大模型，你的老板可能会问你，需要多大的资源？这时候需要你来确定使用哪种GPU来运行模型。 显存占用计算在线工具 https://rahulschand.github.io/gpu_poor/ https://huggingface.co/spaces/hf-accelerate/model-memory-usage https://vram.asmirnov.xyz/ https://huggingface.co/spaces/Vokturz/can-it-run-llm?ref=blog.runpod.io","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/06_llm_experience/llm%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97%E4%BB%A5%E5%8F%8AGPU%E7%9A%84%E9%80%89%E6%8B%A9.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"GPU显存占用计算以及GPU的选择"}],["meta",{"property":"og:description","content":"GPU显存占用计算以及GPU的选择 从第一性原则出发，要回答的第一个问题就是，为什么要计算大模型占用的显存资源？一句话概括：显存太小，模型无法运行；显存太大，浪费金钱。所以从成本的角度来看，很有必要分析计算大模型的资源占用 当你手头想要部署某个开源大模型，你的老板可能会问你，需要多大的资源？这时候需要你来确定使用哪种GPU来运行模型。 显存占用计算在线工具 https://rahulschand.github.io/gpu_poor/ https://huggingface.co/spaces/hf-accelerate/model-memory-usage https://vram.asmirnov.xyz/ https://huggingface.co/spaces/Vokturz/can-it-run-llm?ref=blog.runpod.io"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-05-30T04:24:27.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-05-30T04:24:27.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"GPU显存占用计算以及GPU的选择\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-05-30T04:24:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"显存占用计算在线工具","slug":"显存占用计算在线工具","link":"#显存占用计算在线工具","children":[]},{"level":2,"title":"1. 大模型常见规格","slug":"_1-大模型常见规格","link":"#_1-大模型常见规格","children":[]},{"level":2,"title":"参数量估算 (n B)","slug":"参数量估算-n-b","link":"#参数量估算-n-b","children":[]},{"level":2,"title":"2. 模型文件有多大 (2n GB)","slug":"_2-模型文件有多大-2n-gb","link":"#_2-模型文件有多大-2n-gb","children":[]},{"level":2,"title":"3. nB 模型推理需要多少显存 (2n GB)","slug":"_3-nb-模型推理需要多少显存-2n-gb","link":"#_3-nb-模型推理需要多少显存-2n-gb","children":[]},{"level":2,"title":"4. nB 模型训练需要多少显存","slug":"_4-nb-模型训练需要多少显存","link":"#_4-nb-模型训练需要多少显存","children":[{"level":3,"title":"4.1. gege version3","slug":"_4-1-gege-version3","link":"#_4-1-gege-version3","children":[]},{"level":3,"title":"4.2. GPU 显存计算  version2","slug":"_4-2-gpu-显存计算-version2","link":"#_4-2-gpu-显存计算-version2","children":[]},{"level":3,"title":"4.3 version3","slug":"_4-3-version3","link":"#_4-3-version3","children":[]},{"level":3,"title":"4.4. 混合精度（详见blog和deepspeed处）","slug":"_4-4-混合精度-详见blog和deepspeed处","link":"#_4-4-混合精度-详见blog和deepspeed处","children":[]},{"level":3,"title":"4.5. 实践案例","slug":"_4-5-实践案例","link":"#_4-5-实践案例","children":[]}]},{"level":2,"title":"5. 能否用4 * v100 32G训练vicuna 65b？","slug":"_5-能否用4-v100-32g训练vicuna-65b","link":"#_5-能否用4-v100-32g训练vicuna-65b","children":[]},{"level":2,"title":"6. 全参数微调所需显存量的考虑因素","slug":"_6-全参数微调所需显存量的考虑因素","link":"#_6-全参数微调所需显存量的考虑因素","children":[]},{"level":2,"title":"7.样本量规模增大，训练出现OOM错解决方案","slug":"_7-样本量规模增大-训练出现oom错解决方案","link":"#_7-样本量规模增大-训练出现oom错解决方案","children":[]},{"level":2,"title":"1. 哥哥现有GPU资源","slug":"_1-哥哥现有gpu资源","link":"#_1-哥哥现有gpu资源","children":[]},{"level":2,"title":"显存含义","slug":"显存含义","link":"#显存含义","children":[]},{"level":2,"title":"Cuda Cores和Tensor Cores","slug":"cuda-cores和tensor-cores","link":"#cuda-cores和tensor-cores","children":[]},{"level":2,"title":"不同GPU对精度的支持力度","slug":"不同gpu对精度的支持力度","link":"#不同gpu对精度的支持力度","children":[]},{"level":2,"title":"2. 选择 GPU 用于 AI 机器学习和 LLM 的关键因素","slug":"_2-选择-gpu-用于-ai-机器学习和-llm-的关键因素","link":"#_2-选择-gpu-用于-ai-机器学习和-llm-的关键因素","children":[]},{"level":2,"title":"3. NVIDIA GPU 参数速查表","slug":"_3-nvidia-gpu-参数速查表","link":"#_3-nvidia-gpu-参数速查表","children":[]},{"level":2,"title":"4. GPU介绍","slug":"_4-gpu介绍","link":"#_4-gpu介绍","children":[]}],"git":{"createdTime":1745416391000,"updatedTime":1748579067000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-6.local","commits":2}]},"readingTime":{"minutes":15.19,"words":4558},"filePathRelative":"llm/06_llm_experience/llm显存占用计算以及GPU的选择.md","localizedDate":"April 23, 2025","excerpt":"<h1> GPU显存占用计算以及GPU的选择</h1>\\n<p>从第一性原则出发，要回答的第一个问题就是，为什么要计算大模型占用的显存资源？一句话概括：显存太小，模型无法运行；显存太大，浪费金钱。所以从成本的角度来看，很有必要分析计算大模型的资源占用</p>\\n<p>当你手头想要部署某个开源大模型，你的老板可能会问你，需要多大的资源？这时候需要你来确定使用哪种GPU来运行模型。</p>\\n<h2> 显存占用计算在线工具</h2>\\n<ul>\\n<li>https://rahulschand.github.io/gpu_poor/</li>\\n<li>https://huggingface.co/spaces/hf-accelerate/model-memory-usage</li>\\n<li>https://vram.asmirnov.xyz/</li>\\n<li>https://huggingface.co/spaces/Vokturz/can-it-run-llm?ref=blog.runpod.io</li>\\n</ul>","autoDesc":true}');export{e as data};
