const t=JSON.parse('{"key":"v-6110c2d1","path":"/llm/01_llm_basic/attention.html","title":"Attention(注意力机制)","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"Attention(注意力机制) 1. 朴素seq2seq的信息瓶颈 用Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。 这里存在一个问题：Encoder的最后一个隐状态（Context Vector）承载了源句子的所有信息，成为整个模型的“信息”瓶颈。 2. 注意力机制就是希望打破这个信息瓶颈。宏观来看，Attention直接把Decoder中的每一个隐层，和Encoder中的每一个隐层，都连接起来了！","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/01_llm_basic/attention.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"Attention(注意力机制)"}],["meta",{"property":"og:description","content":"Attention(注意力机制) 1. 朴素seq2seq的信息瓶颈 用Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。 这里存在一个问题：Encoder的最后一个隐状态（Context Vector）承载了源句子的所有信息，成为整个模型的“信息”瓶颈。 2. 注意力机制就是希望打破这个信息瓶颈。宏观来看，Attention直接把Decoder中的每一个隐层，和Encoder中的每一个隐层，都连接起来了！"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Attention(注意力机制)\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 朴素seq2seq的信息瓶颈","slug":"_1-朴素seq2seq的信息瓶颈","link":"#_1-朴素seq2seq的信息瓶颈","children":[]},{"level":2,"title":"2. 注意力机制就是希望打破这个信息瓶颈。宏观来看，Attention直接把Decoder中的每一个隐层，和Encoder中的每一个隐层，都连接起来了！","slug":"_2-注意力机制就是希望打破这个信息瓶颈。宏观来看-attention直接把decoder中的每一个隐层-和encoder中的每一个隐层-都连接起来了","link":"#_2-注意力机制就是希望打破这个信息瓶颈。宏观来看-attention直接把decoder中的每一个隐层-和encoder中的每一个隐层-都连接起来了","children":[]},{"level":2,"title":"3. 向量的点积(Dot Product)==矩阵的乘法","slug":"_3-向量的点积-dot-product-矩阵的乘法","link":"#_3-向量的点积-dot-product-矩阵的乘法","children":[{"level":3,"title":"3.1. 向量的点积","slug":"_3-1-向量的点积","link":"#_3-1-向量的点积","children":[]},{"level":3,"title":"3.2. 矩阵的乘法","slug":"_3-2-矩阵的乘法","link":"#_3-2-矩阵的乘法","children":[]}]},{"level":2,"title":"4. QKV : 每个词有q,k,v(query,key,value)","slug":"_4-qkv-每个词有q-k-v-query-key-value","link":"#_4-qkv-每个词有q-k-v-query-key-value","children":[]},{"level":2,"title":"5. Attention机制的步骤","slug":"_5-attention机制的步骤","link":"#_5-attention机制的步骤","children":[{"level":3,"title":"5.1. 注意力得分(Attention Score)（计算相关性）","slug":"_5-1-注意力得分-attention-score-计算相关性","link":"#_5-1-注意力得分-attention-score-计算相关性","children":[]},{"level":3,"title":"5.2. 注意力分布(Attention Distribution)（获取概率密度分布）","slug":"_5-2-注意力分布-attention-distribution-获取概率密度分布","link":"#_5-2-注意力分布-attention-distribution-获取概率密度分布","children":[]},{"level":3,"title":"5.3. 注意力输出(Attention Output)（求期望值）","slug":"_5-3-注意力输出-attention-output-求期望值","link":"#_5-3-注意力输出-attention-output-求期望值","children":[]}]},{"level":2,"title":"6. Self-Attention(自注意力)","slug":"_6-self-attention-自注意力","link":"#_6-self-attention-自注意力","children":[]},{"level":2,"title":"7. Multi-Head Attention(多头注意力)","slug":"_7-multi-head-attention-多头注意力","link":"#_7-multi-head-attention-多头注意力","children":[]},{"level":2,"title":"8. Attention机制带来的好处","slug":"_8-attention机制带来的好处","link":"#_8-attention机制带来的好处","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":3.57,"words":1072},"filePathRelative":"llm/01_llm_basic/attention.md","localizedDate":"March 28, 2025","excerpt":"<h1> Attention(注意力机制)</h1>\\n<h2> 1. 朴素seq2seq的信息瓶颈</h2>\\n<figure><figcaption></figcaption></figure>\\n<p>用Encoder RNN的最后一个神经元的隐状态作为Decoder RNN的初始隐状态。\\n这里存在一个问题：Encoder的最后一个隐状态（Context Vector）承载了源句子的所有信息，成为整个模型的“信息”瓶颈。</p>\\n<h2> 2. 注意力机制就是希望打破这个信息瓶颈。宏观来看，Attention直接把Decoder中的每一个隐层，和Encoder中的每一个隐层，都连接起来了！</h2>","autoDesc":true}');export{t as data};
