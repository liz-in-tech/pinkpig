const e=JSON.parse('{"key":"v-2b1465da","path":"/llm/03_llm_training/05_%E5%AF%B9%E9%BD%90.html","title":"对齐 / 偏好对齐","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"对齐 / 偏好对齐 Note: 不是所有对齐都用的强化学习，dpo直接偏好对齐就是避免使用强化学习的偏好对齐方法 在训练过程中产生新的数据和行为的是强化学习，反之不是强化学习 策略梯度 pg rlhf-ppo 近端策略优化 ppo (Proximal Policy Optimization) grpo 拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/03_llm_training/05_%E5%AF%B9%E9%BD%90.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"对齐 / 偏好对齐"}],["meta",{"property":"og:description","content":"对齐 / 偏好对齐 Note: 不是所有对齐都用的强化学习，dpo直接偏好对齐就是避免使用强化学习的偏好对齐方法 在训练过程中产生新的数据和行为的是强化学习，反之不是强化学习 策略梯度 pg rlhf-ppo 近端策略优化 ppo (Proximal Policy Optimization) grpo 拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-04-23T13:53:11.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-04-23T13:53:11.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"对齐 / 偏好对齐\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-04-23T13:53:11.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"偏好对齐","slug":"偏好对齐","link":"#偏好对齐","children":[{"level":3,"title":"RFT训练（Reinforcement Fine-Tuning）","slug":"rft训练-reinforcement-fine-tuning","link":"#rft训练-reinforcement-fine-tuning","children":[]},{"level":3,"title":"DPO训练 (Direct Preference Optimization) 直接偏好优化","slug":"dpo训练-direct-preference-optimization-直接偏好优化","link":"#dpo训练-direct-preference-optimization-直接偏好优化","children":[]},{"level":3,"title":"KTO训练","slug":"kto训练","link":"#kto训练","children":[]},{"level":3,"title":"SimPO训练 (Simple Preference Optimization)","slug":"simpo训练-simple-preference-optimization","link":"#simpo训练-simple-preference-optimization","children":[]},{"level":3,"title":"RLHF训练","slug":"rlhf训练","link":"#rlhf训练","children":[]}]},{"level":2,"title":"模型 & 训练方式 & 超参数","slug":"模型-训练方式-超参数","link":"#模型-训练方式-超参数","children":[]},{"level":2,"title":"强化学习","slug":"强化学习","link":"#强化学习","children":[]},{"level":2,"title":"NLP中的强化学习","slug":"nlp中的强化学习","link":"#nlp中的强化学习","children":[]},{"level":2,"title":"RLHF中的四个重要角色","slug":"rlhf中的四个重要角色","link":"#rlhf中的四个重要角色","children":[]},{"level":2,"title":"KL散度","slug":"kl散度","link":"#kl散度","children":[]},{"level":2,"title":"优势（Advantage）","slug":"优势-advantage","link":"#优势-advantage","children":[]},{"level":2,"title":"RLHF-PPO的训练过程","slug":"rlhf-ppo的训练过程","link":"#rlhf-ppo的训练过程","children":[]},{"level":2,"title":"RLHF中的loss计算","slug":"rlhf中的loss计算","link":"#rlhf中的loss计算","children":[]},{"level":2,"title":"马尔可夫决策过程（Markov decision process，MDP）","slug":"马尔可夫决策过程-markov-decision-process-mdp","link":"#马尔可夫决策过程-markov-decision-process-mdp","children":[{"level":3,"title":"马尔可夫过程","slug":"马尔可夫过程","link":"#马尔可夫过程","children":[]},{"level":3,"title":"马尔可夫奖励过程（Markov reward process, MRP）","slug":"马尔可夫奖励过程-markov-reward-process-mrp","link":"#马尔可夫奖励过程-markov-reward-process-mrp","children":[]},{"level":3,"title":"价值函数","slug":"价值函数","link":"#价值函数","children":[]},{"level":3,"title":"马尔可夫决策过程（Markov decision process，MDP）","slug":"马尔可夫决策过程-markov-decision-process-mdp-1","link":"#马尔可夫决策过程-markov-decision-process-mdp-1","children":[]},{"level":3,"title":"策略的占用度量（occupancy measure）","slug":"策略的占用度量-occupancy-measure","link":"#策略的占用度量-occupancy-measure","children":[]},{"level":3,"title":"基于价值的最优策略","slug":"基于价值的最优策略","link":"#基于价值的最优策略","children":[]}]},{"level":2,"title":"策略梯度算法（Policy Gradient，PG）","slug":"策略梯度算法-policy-gradient-pg","link":"#策略梯度算法-policy-gradient-pg","children":[]},{"level":2,"title":"Actor-Critic算法","slug":"actor-critic算法","link":"#actor-critic算法","children":[]},{"level":2,"title":"PPO算法","slug":"ppo算法","link":"#ppo算法","children":[]},{"level":2,"title":"Reference","slug":"reference","link":"#reference","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1745416391000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro-6.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":40.01,"words":12004},"filePathRelative":"llm/03_llm_training/05_对齐.md","localizedDate":"March 28, 2025","excerpt":"<h1> 对齐 / 偏好对齐</h1>\\n<p>Note: 不是所有对齐都用的强化学习，dpo直接偏好对齐就是避免使用强化学习的偏好对齐方法</p>\\n<ul>\\n<li>\\n<p>在训练过程中产生新的数据和行为的是强化学习，反之不是强化学习</p>\\n</li>\\n<li>\\n<p>策略梯度 pg</p>\\n</li>\\n<li>\\n<p>rlhf-ppo</p>\\n</li>\\n<li>\\n<p>近端策略优化 ppo (Proximal Policy Optimization)</p>\\n</li>\\n<li>\\n<p>grpo</p>\\n</li>\\n<li>\\n<p>拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。</p>\\n</li>\\n</ul>","autoDesc":true}');export{e as data};
