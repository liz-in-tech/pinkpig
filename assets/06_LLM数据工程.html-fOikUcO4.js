const e=JSON.parse('{"key":"v-f3800172","path":"/llm/03_llm_training/06_LLM%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B.html","title":"LLM 数据工程","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"LLM 数据工程 数据集 &amp; 数据采集 数据清洗 质量过滤 敏感内容过滤 数据去重 tokenizer 词元化 / 分词 数据调度（Data Scheduling）：数据配比 &amp; 数据课程 1. 数据集 &amp; 数据采集 1.0. 数据类型 预训练 纯文本：⽤于预训练Post-pretrain。特定⾏业⽅向/场景下的⼤规模⽆标注数据语料。 微调 Prompt+Response：⽤于文本生成场景的有监督微调SFT。单轮或多轮的⽂本对话数据，提问与回答⼀⼀对应。 Role(user+assistant)：⽤于文本生成场景的有监督微调SFT。单轮或多轮的文本对话数据，提问与回答—一对应，支持多角色类型与Function Call工具调用。 多模态 Prompt+图片：⽤于图像生成场景的有监督微调SFT，文本提问与图片回答—一对应。 Prompt+Image+Response：用于图像理解场景的有监督微调SFT，单轮或多轮的图文混合对话数据，支持单图或多图场景。 对齐 Prompt+Chosen/Rejected：用于偏好对齐KTO。单轮或多轮的⽂本对话数据，每个提示语存在对应的正或负偏好回答。 Prompt+Chosen+Rejected：用于偏好对齐DPO。单轮或多轮的⽂本对话数据，每个提示语存在对应的正负偏好回答。 Prompt+多Response排序：⽤于RLHF奖励模型微调。单轮或多轮的⽂本对话数据，单个提问对应多个回答，多个回答间带有先后排序关系。 Prompt集：⽤于RLHF强化学习微调。","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/03_llm_training/06_LLM%E6%95%B0%E6%8D%AE%E5%B7%A5%E7%A8%8B.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"LLM 数据工程"}],["meta",{"property":"og:description","content":"LLM 数据工程 数据集 &amp; 数据采集 数据清洗 质量过滤 敏感内容过滤 数据去重 tokenizer 词元化 / 分词 数据调度（Data Scheduling）：数据配比 &amp; 数据课程 1. 数据集 &amp; 数据采集 1.0. 数据类型 预训练 纯文本：⽤于预训练Post-pretrain。特定⾏业⽅向/场景下的⼤规模⽆标注数据语料。 微调 Prompt+Response：⽤于文本生成场景的有监督微调SFT。单轮或多轮的⽂本对话数据，提问与回答⼀⼀对应。 Role(user+assistant)：⽤于文本生成场景的有监督微调SFT。单轮或多轮的文本对话数据，提问与回答—一对应，支持多角色类型与Function Call工具调用。 多模态 Prompt+图片：⽤于图像生成场景的有监督微调SFT，文本提问与图片回答—一对应。 Prompt+Image+Response：用于图像理解场景的有监督微调SFT，单轮或多轮的图文混合对话数据，支持单图或多图场景。 对齐 Prompt+Chosen/Rejected：用于偏好对齐KTO。单轮或多轮的⽂本对话数据，每个提示语存在对应的正或负偏好回答。 Prompt+Chosen+Rejected：用于偏好对齐DPO。单轮或多轮的⽂本对话数据，每个提示语存在对应的正负偏好回答。 Prompt+多Response排序：⽤于RLHF奖励模型微调。单轮或多轮的⽂本对话数据，单个提问对应多个回答，多个回答间带有先后排序关系。 Prompt集：⽤于RLHF强化学习微调。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-04-23T13:53:11.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-04-23T13:53:11.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM 数据工程\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-04-23T13:53:11.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. 数据集 & 数据采集","slug":"_1-数据集-数据采集","link":"#_1-数据集-数据采集","children":[]},{"level":2,"title":"1.0. 数据类型","slug":"_1-0-数据类型","link":"#_1-0-数据类型","children":[{"level":3,"title":"1.1. 数据集","slug":"_1-1-数据集","link":"#_1-1-数据集","children":[]},{"level":3,"title":"1.1.1. 常用的预训练数据集","slug":"_1-1-1-常用的预训练数据集","link":"#_1-1-1-常用的预训练数据集","children":[]},{"level":3,"title":"1.1.2. 常用的微调数据集","slug":"_1-1-2-常用的微调数据集","link":"#_1-1-2-常用的微调数据集","children":[]},{"level":3,"title":"1.1.3. 常用的对齐数据集","slug":"_1-1-3-常用的对齐数据集","link":"#_1-1-3-常用的对齐数据集","children":[]},{"level":3,"title":"1.2. 数据采集","slug":"_1-2-数据采集","link":"#_1-2-数据采集","children":[]},{"level":3,"title":"1.3. 评估数据的好坏","slug":"_1-3-评估数据的好坏","link":"#_1-3-评估数据的好坏","children":[]}]},{"level":2,"title":"2. 数据清洗","slug":"_2-数据清洗","link":"#_2-数据清洗","children":[{"level":3,"title":"2.1. 质量过滤","slug":"_2-1-质量过滤","link":"#_2-1-质量过滤","children":[]},{"level":3,"title":"2.2. 敏感内容过滤","slug":"_2-2-敏感内容过滤","link":"#_2-2-敏感内容过滤","children":[]},{"level":3,"title":"2.3. 数据去重 Deduplicating the data","slug":"_2-3-数据去重-deduplicating-the-data","link":"#_2-3-数据去重-deduplicating-the-data","children":[]}]},{"level":2,"title":"3. 数据调度（Data Scheduling）：数据配比 & 数据课程","slug":"_3-数据调度-data-scheduling-数据配比-数据课程","link":"#_3-数据调度-data-scheduling-数据配比-数据课程","children":[{"level":3,"title":"3.1. 数据配比","slug":"_3-1-数据配比","link":"#_3-1-数据配比","children":[]},{"level":3,"title":"3.2. 数据课程，Data Curriculum（各数据源用于训练的顺序）","slug":"_3-2-数据课程-data-curriculum-各数据源用于训练的顺序","link":"#_3-2-数据课程-data-curriculum-各数据源用于训练的顺序","children":[]}]},{"level":2,"title":"4. 数据对预训练效果的影响","slug":"_4-数据对预训练效果的影响","link":"#_4-数据对预训练效果的影响","children":[]}],"git":{"createdTime":1743175521000,"updatedTime":1745416391000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-6.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":13.39,"words":4016},"filePathRelative":"llm/03_llm_training/06_LLM数据工程.md","localizedDate":"March 28, 2025","excerpt":"<h1> LLM 数据工程</h1>\\n<figure><figcaption></figcaption></figure>\\n<ul>\\n<li>数据集 &amp; 数据采集</li>\\n<li>数据清洗\\n<ul>\\n<li>质量过滤</li>\\n<li>敏感内容过滤</li>\\n<li>数据去重</li>\\n</ul>\\n</li>\\n<li>tokenizer 词元化 / 分词</li>\\n<li>数据调度（Data Scheduling）：数据配比 &amp; 数据课程</li>\\n</ul>\\n<h2> 1. 数据集 &amp; 数据采集</h2>\\n<h2> 1.0. 数据类型</h2>\\n<ul>\\n<li>预训练\\n<ul>\\n<li>纯文本：⽤于预训练Post-pretrain。特定⾏业⽅向/场景下的⼤规模⽆标注数据语料。</li>\\n</ul>\\n</li>\\n<li>微调\\n<ul>\\n<li>Prompt+Response：⽤于文本生成场景的有监督微调SFT。单轮或多轮的⽂本对话数据，提问与回答⼀⼀对应。</li>\\n<li>Role(user+assistant)：⽤于文本生成场景的有监督微调SFT。单轮或多轮的文本对话数据，提问与回答—一对应，支持多角色类型与Function Call工具调用。</li>\\n<li>多模态\\n<ul>\\n<li>Prompt+图片：⽤于图像生成场景的有监督微调SFT，文本提问与图片回答—一对应。</li>\\n<li>Prompt+Image+Response：用于图像理解场景的有监督微调SFT，单轮或多轮的图文混合对话数据，支持单图或多图场景。</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>对齐\\n<ul>\\n<li>Prompt+Chosen/Rejected：用于偏好对齐KTO。单轮或多轮的⽂本对话数据，每个提示语存在对应的正或负偏好回答。</li>\\n<li>Prompt+Chosen+Rejected：用于偏好对齐DPO。单轮或多轮的⽂本对话数据，每个提示语存在对应的正负偏好回答。</li>\\n<li>Prompt+多Response排序：⽤于RLHF奖励模型微调。单轮或多轮的⽂本对话数据，单个提问对应多个回答，多个回答间带有先后排序关系。</li>\\n<li>Prompt集：⽤于RLHF强化学习微调。</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{e as data};
