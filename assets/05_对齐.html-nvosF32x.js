const t=JSON.parse('{"key":"v-2b1465da","path":"/llm/03_llm_training/05_%E5%AF%B9%E9%BD%90.html","title":"对齐 & 强化学习","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"对齐 &amp; 强化学习 策略梯度 pg 近端策略优化 ppo (Proximal Policy Optimization) : 迭代更新策略以最大化奖励，同时保持接近初始行为。它使用奖励模型来对响应进行评分，并需要仔细调整超参数，包括学习率、批量大小和 PPO 剪辑范围 rlhf dpo（Direct Preference Optimization）：直接偏好优化 直接优化策略，以最大化选择响应而非拒绝响应的可能性。它不需要奖励建模，这使得它在计算上比 PPO 更高效，但在质量方面略差。 orpo 拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/03_llm_training/05_%E5%AF%B9%E9%BD%90.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"对齐 & 强化学习"}],["meta",{"property":"og:description","content":"对齐 &amp; 强化学习 策略梯度 pg 近端策略优化 ppo (Proximal Policy Optimization) : 迭代更新策略以最大化奖励，同时保持接近初始行为。它使用奖励模型来对响应进行评分，并需要仔细调整超参数，包括学习率、批量大小和 PPO 剪辑范围 rlhf dpo（Direct Preference Optimization）：直接偏好优化 直接优化策略，以最大化选择响应而非拒绝响应的可能性。它不需要奖励建模，这使得它在计算上比 PPO 更高效，但在质量方面略差。 orpo 拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-29T07:33:01.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-29T07:33:01.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"对齐 & 强化学习\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-29T07:33:01.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[],"git":{"createdTime":1743175521000,"updatedTime":1743233581000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro-2.local","commits":1},{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":0.76,"words":229},"filePathRelative":"llm/03_llm_training/05_对齐.md","localizedDate":"March 28, 2025","excerpt":"<h1> 对齐 &amp; 强化学习</h1>\\n<ul>\\n<li>策略梯度 pg</li>\\n<li>近端策略优化 ppo (Proximal Policy Optimization) : 迭代更新策略以最大化奖励，同时保持接近初始行为。它使用奖励模型来对响应进行评分，并需要仔细调整超参数，包括学习率、批量大小和 PPO 剪辑范围</li>\\n<li>rlhf</li>\\n<li>dpo（Direct Preference Optimization）：直接偏好优化 直接优化策略，以最大化选择响应而非拒绝响应的可能性。它不需要奖励建模，这使得它在计算上比 PPO 更高效，但在质量方面略差。</li>\\n<li>orpo</li>\\n<li>拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。</li>\\n</ul>","autoDesc":true}');export{t as data};
