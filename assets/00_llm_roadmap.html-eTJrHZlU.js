const l=JSON.parse('{"key":"v-8ec2695a","path":"/llm/00_llm_roadmap.html","title":"LLM Roadmap","lang":"en-US","frontmatter":{"icon":"lightbulb","description":"LLM Roadmap 1. LLM 基础 GPU Mathematics for ML 线性代数 向量、矩阵、行列式、特征值和特征向量、向量空间和线性变换 概率论 微积分 导数、积分、极限和级数、多元微积分和梯度 Python for ML Python Basics 基本语法、数据类型、错误处理和面向对象编程 Data Science Libraries NumPy 用于数值运算 Pandas 用于数据操作和分析 Matplotlib 和 Seaborn 用于数据可视化 Data Preprocessing 涉及特征缩放和规范化、处理缺失数据、异常值检测、分类数据编码以及将数据分成训练、验证和测试集 ML Libraries 熟练掌握 Scikit-learn（一个提供多种监督和非监督学习算法的库）至关重要。了解如何实现线性回归、逻辑回归、决策树、随机森林、k-最近邻 (K-NN) 和 K-均值聚类等算法非常重要。PCA 和 t-SNE 等降维技术也有助于可视化高维数据。 Neural Networks Fundamentals 理解神经网络的结构，例如层、权重、偏差和激活函数（sigmoid、tanh、ReLU 等）。 Training and Optimization 熟悉反向传播和不同类型的损失函数，如均方误差 (MSE) 和交叉熵。了解各种优化算法，如梯度下降、随机梯度下降、RMSprop 和 Adam。 常用参数更新方法 梯度下降：在一个方向上更新和调整模型的参数，来最小化损失函数 随机梯度下降（Stochastic gradient descent，SGD） 对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快 小批量梯度下降（Mini Batch Gradient Descent）：对每个批次中的n个训练样本，这种方法只执行一次更新 通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。 在训练神经网络时，通常都会选择小批量梯度下降算法 动量（Momentum）技术 通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练 Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似 梯度爆炸和梯度消失 导致的原因和解决 矩阵连乘 在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸 解决：对梯度做截断解决梯度爆炸问题、残差连接、normalize。由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为该问题已经解决了。 使用了Sigmoid和tanh激活函数：换用ReLU激活函数 Overfitting 过拟合 了解过度拟合的概念（模型在训练数据上表现良好但在看不见的数据上表现不佳），并学习各种正则化技术（dropout、L1/L2 正则化、早期停止、数据增强）来防止过度拟合。 Implement a MLP 使用 PyTorch 构建 MLP，也称为全连接网络 NLP Text Preprocessing / Tokenizer 学习各种文本预处理步骤，如标记化（将文本分成单词或句子）、词干提取（将单词简化为其词根形式）、词形还原（类似于词干提取但考虑上下文）、停用词删除等 Feature Extraction Techniques 熟悉将文本数据转换为机器学习算法可以理解的格式的技术。主要方法包括词袋 (BoW)、词频-逆文档频率 (TF-IDF) 和 n-gram。 Word Embeddings / Word2Vec 词嵌入是一种词语表示方法，可以让具有相似含义的词语具有相似的表示。主要方法包括 Word2Vec、GloVe 和 FastText。 演变 读热编码 one-hot向量表示 Word2Vec NLP三大特征抽取器 Transformer RNN 了解 RNN 的工作原理，RNN 是一种用于处理序列数据的神经网络。探索 LSTM 和 GRU，这两种 RNN 变体都能够学习长期依赖关系。 CNN NLP预训练发展史：图像预训练 → word embedding → word2vec → elmo → transformer → gpt → bert → GPT 234","head":[["meta",{"property":"og:url","content":"https://liz-in-tech.github.io/pinkpig/llm/00_llm_roadmap.html"}],["meta",{"property":"og:site_name","content":"Liz"}],["meta",{"property":"og:title","content":"LLM Roadmap"}],["meta",{"property":"og:description","content":"LLM Roadmap 1. LLM 基础 GPU Mathematics for ML 线性代数 向量、矩阵、行列式、特征值和特征向量、向量空间和线性变换 概率论 微积分 导数、积分、极限和级数、多元微积分和梯度 Python for ML Python Basics 基本语法、数据类型、错误处理和面向对象编程 Data Science Libraries NumPy 用于数值运算 Pandas 用于数据操作和分析 Matplotlib 和 Seaborn 用于数据可视化 Data Preprocessing 涉及特征缩放和规范化、处理缺失数据、异常值检测、分类数据编码以及将数据分成训练、验证和测试集 ML Libraries 熟练掌握 Scikit-learn（一个提供多种监督和非监督学习算法的库）至关重要。了解如何实现线性回归、逻辑回归、决策树、随机森林、k-最近邻 (K-NN) 和 K-均值聚类等算法非常重要。PCA 和 t-SNE 等降维技术也有助于可视化高维数据。 Neural Networks Fundamentals 理解神经网络的结构，例如层、权重、偏差和激活函数（sigmoid、tanh、ReLU 等）。 Training and Optimization 熟悉反向传播和不同类型的损失函数，如均方误差 (MSE) 和交叉熵。了解各种优化算法，如梯度下降、随机梯度下降、RMSprop 和 Adam。 常用参数更新方法 梯度下降：在一个方向上更新和调整模型的参数，来最小化损失函数 随机梯度下降（Stochastic gradient descent，SGD） 对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快 小批量梯度下降（Mini Batch Gradient Descent）：对每个批次中的n个训练样本，这种方法只执行一次更新 通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。 在训练神经网络时，通常都会选择小批量梯度下降算法 动量（Momentum）技术 通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练 Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似 梯度爆炸和梯度消失 导致的原因和解决 矩阵连乘 在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸 解决：对梯度做截断解决梯度爆炸问题、残差连接、normalize。由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为该问题已经解决了。 使用了Sigmoid和tanh激活函数：换用ReLU激活函数 Overfitting 过拟合 了解过度拟合的概念（模型在训练数据上表现良好但在看不见的数据上表现不佳），并学习各种正则化技术（dropout、L1/L2 正则化、早期停止、数据增强）来防止过度拟合。 Implement a MLP 使用 PyTorch 构建 MLP，也称为全连接网络 NLP Text Preprocessing / Tokenizer 学习各种文本预处理步骤，如标记化（将文本分成单词或句子）、词干提取（将单词简化为其词根形式）、词形还原（类似于词干提取但考虑上下文）、停用词删除等 Feature Extraction Techniques 熟悉将文本数据转换为机器学习算法可以理解的格式的技术。主要方法包括词袋 (BoW)、词频-逆文档频率 (TF-IDF) 和 n-gram。 Word Embeddings / Word2Vec 词嵌入是一种词语表示方法，可以让具有相似含义的词语具有相似的表示。主要方法包括 Word2Vec、GloVe 和 FastText。 演变 读热编码 one-hot向量表示 Word2Vec NLP三大特征抽取器 Transformer RNN 了解 RNN 的工作原理，RNN 是一种用于处理序列数据的神经网络。探索 LSTM 和 GRU，这两种 RNN 变体都能够学习长期依赖关系。 CNN NLP预训练发展史：图像预训练 → word embedding → word2vec → elmo → transformer → gpt → bert → GPT 234"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-03-28T15:25:21.000Z"}],["meta",{"property":"article:author","content":"Liz"}],["meta",{"property":"article:modified_time","content":"2025-03-28T15:25:21.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM Roadmap\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-03-28T15:25:21.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Liz\\",\\"url\\":\\"https://github.com/liz-in-tech\\"}]}"]]},"headers":[{"level":2,"title":"1. LLM 基础","slug":"_1-llm-基础","link":"#_1-llm-基础","children":[]},{"level":2,"title":"2. LLM 架构","slug":"_2-llm-架构","link":"#_2-llm-架构","children":[]},{"level":2,"title":"3. LLM 训练","slug":"_3-llm-训练","link":"#_3-llm-训练","children":[{"level":3,"title":"3.1. LLM 数据工程","slug":"_3-1-llm-数据工程","link":"#_3-1-llm-数据工程","children":[]},{"level":3,"title":"3.2. 分布式训练","slug":"_3-2-分布式训练","link":"#_3-2-分布式训练","children":[]},{"level":3,"title":"3.3. 有监督微调","slug":"_3-3-有监督微调","link":"#_3-3-有监督微调","children":[]},{"level":3,"title":"3.4. 强化学习 & 人类偏好对齐","slug":"_3-4-强化学习-人类偏好对齐","link":"#_3-4-强化学习-人类偏好对齐","children":[]},{"level":3,"title":"3.5. 训练参数","slug":"_3-5-训练参数","link":"#_3-5-训练参数","children":[]},{"level":3,"title":"3.6. 训练监控","slug":"_3-6-训练监控","link":"#_3-6-训练监控","children":[]}]},{"level":2,"title":"4. LLM 推理","slug":"_4-llm-推理","link":"#_4-llm-推理","children":[{"level":3,"title":"4.1. LLM 训练优化","slug":"_4-1-llm-训练优化","link":"#_4-1-llm-训练优化","children":[]},{"level":3,"title":"4.2. 推理加速优化","slug":"_4-2-推理加速优化","link":"#_4-2-推理加速优化","children":[]},{"level":3,"title":"4.3. LLM 部署","slug":"_4-3-llm-部署","link":"#_4-3-llm-部署","children":[]},{"level":3,"title":"4.4. LLM 压缩","slug":"_4-4-llm-压缩","link":"#_4-4-llm-压缩","children":[]},{"level":3,"title":"4.5. LLM 评估","slug":"_4-5-llm-评估","link":"#_4-5-llm-评估","children":[]}]},{"level":2,"title":"5. LLM应用开发","slug":"_5-llm应用开发","link":"#_5-llm应用开发","children":[{"level":3,"title":"LLM APIs","slug":"llm-apis","link":"#llm-apis","children":[]},{"level":3,"title":"开源 LLMs","slug":"开源-llms","link":"#开源-llms","children":[]},{"level":3,"title":"Prompt Engineering","slug":"prompt-engineering","link":"#prompt-engineering","children":[]},{"level":3,"title":"Agent & RAG","slug":"agent-rag","link":"#agent-rag","children":[]}]}],"git":{"createdTime":1743175521000,"updatedTime":1743175521000,"contributors":[{"name":"liz","email":"liz@MacBook-Pro.local","commits":1}]},"readingTime":{"minutes":17.97,"words":5392},"filePathRelative":"llm/00_llm_roadmap.md","localizedDate":"March 28, 2025","excerpt":"<h1> LLM Roadmap</h1>\\n<h2> 1. LLM 基础</h2>\\n<ul>\\n<li>GPU</li>\\n<li>Mathematics for ML\\n<ul>\\n<li>线性代数\\n<ul>\\n<li>向量、矩阵、行列式、特征值和特征向量、向量空间和线性变换</li>\\n</ul>\\n</li>\\n<li>概率论</li>\\n<li>微积分\\n<ul>\\n<li>导数、积分、极限和级数、多元微积分和梯度</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>Python for ML\\n<ul>\\n<li>Python Basics\\n<ul>\\n<li>基本语法、数据类型、错误处理和面向对象编程</li>\\n</ul>\\n</li>\\n<li>Data Science Libraries\\n<ul>\\n<li>NumPy 用于数值运算</li>\\n<li>Pandas 用于数据操作和分析</li>\\n<li>Matplotlib 和 Seaborn 用于数据可视化</li>\\n</ul>\\n</li>\\n<li>Data Preprocessing\\n<ul>\\n<li>涉及特征缩放和规范化、处理缺失数据、异常值检测、分类数据编码以及将数据分成训练、验证和测试集</li>\\n</ul>\\n</li>\\n<li>ML Libraries\\n<ul>\\n<li>熟练掌握 Scikit-learn（一个提供多种监督和非监督学习算法的库）至关重要。了解如何实现线性回归、逻辑回归、决策树、随机森林、k-最近邻 (K-NN) 和 K-均值聚类等算法非常重要。PCA 和 t-SNE 等降维技术也有助于可视化高维数据。</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>Neural Networks\\n<ul>\\n<li>Fundamentals\\n<ul>\\n<li>理解神经网络的结构，例如层、权重、偏差和激活函数（sigmoid、tanh、ReLU 等）。</li>\\n</ul>\\n</li>\\n<li>Training and Optimization\\n<ul>\\n<li>熟悉反向传播和不同类型的损失函数，如均方误差 (MSE) 和交叉熵。了解各种优化算法，如梯度下降、随机梯度下降、RMSprop 和 Adam。</li>\\n<li>常用参数更新方法\\n<ul>\\n<li>梯度下降：在一个方向上更新和调整模型的参数，来最小化损失函数</li>\\n<li>随机梯度下降（Stochastic gradient descent，SGD） 对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快</li>\\n<li>小批量梯度下降（Mini Batch Gradient Descent）：对每个批次中的n个训练样本，这种方法只执行一次更新\\n<ul>\\n<li>通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。</li>\\n<li>在训练神经网络时，通常都会选择小批量梯度下降算法</li>\\n</ul>\\n</li>\\n<li>动量（Momentum）技术\\n<ul>\\n<li>通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练</li>\\n</ul>\\n</li>\\n<li>Adam算法即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似</li>\\n</ul>\\n</li>\\n<li>梯度爆炸和梯度消失\\n<ul>\\n<li>导致的原因和解决\\n<ul>\\n<li>矩阵连乘\\n<ul>\\n<li>在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸</li>\\n<li>解决：对梯度做截断解决梯度爆炸问题、残差连接、normalize。由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为该问题已经解决了。</li>\\n</ul>\\n</li>\\n<li>使用了Sigmoid和tanh激活函数：换用ReLU激活函数</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>Overfitting 过拟合\\n<ul>\\n<li>了解过度拟合的概念（模型在训练数据上表现良好但在看不见的数据上表现不佳），并学习各种正则化技术（dropout、L1/L2 正则化、早期停止、数据增强）来防止过度拟合。</li>\\n</ul>\\n</li>\\n<li>Implement a MLP\\n<ul>\\n<li>使用 PyTorch 构建 MLP，也称为全连接网络</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>NLP\\n<ul>\\n<li>Text Preprocessing / Tokenizer\\n<ul>\\n<li>学习各种文本预处理步骤，如标记化（将文本分成单词或句子）、词干提取（将单词简化为其词根形式）、词形还原（类似于词干提取但考虑上下文）、停用词删除等</li>\\n</ul>\\n</li>\\n<li>Feature Extraction Techniques\\n<ul>\\n<li>熟悉将文本数据转换为机器学习算法可以理解的格式的技术。主要方法包括词袋 (BoW)、词频-逆文档频率 (TF-IDF) 和 n-gram。</li>\\n</ul>\\n</li>\\n<li>Word Embeddings / Word2Vec\\n<ul>\\n<li>词嵌入是一种词语表示方法，可以让具有相似含义的词语具有相似的表示。主要方法包括 Word2Vec、GloVe 和 FastText。</li>\\n<li>演变\\n<ul>\\n<li>读热编码 one-hot向量表示</li>\\n<li>Word2Vec</li>\\n</ul>\\n</li>\\n</ul>\\n</li>\\n<li>NLP三大特征抽取器\\n<ul>\\n<li>Transformer</li>\\n<li>RNN\\n<ul>\\n<li>了解 RNN 的工作原理，RNN 是一种用于处理序列数据的神经网络。探索 LSTM 和 GRU，这两种 RNN 变体都能够学习长期依赖关系。</li>\\n</ul>\\n</li>\\n<li>CNN</li>\\n</ul>\\n</li>\\n<li>NLP预训练发展史：图像预训练 → word embedding → word2vec → elmo → transformer → gpt → bert → GPT 234</li>\\n</ul>\\n</li>\\n</ul>","autoDesc":true}');export{l as data};
