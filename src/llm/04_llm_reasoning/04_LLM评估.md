---
icon: lightbulb
---
# LLM 评估
- 模型评估
    - Automated benchmarks 自动评测: 使用精选数据集和指标（如 MMLU）评估特定任务上的模型。它非常适合具体任务，但抽象和创造能力较差。它也容易受到数据污染
    - Human evaluation 人工评测: 它涉及人工提示模型并对响应进行评分。方法包括从氛围检查到具有特定指导方针的系统注释和大规模社区投票（竞技场）。它更适合主观任务，但事实准确性较不可靠
    - Model-based evaluation 模型评测: 使用判断和奖励模型来评估模型输出。它与人类偏好高度相关，但受到对其自身输出的偏见和不一致的评分的影响
    - Feedback signal : 分析错误模式以识别特定弱点，例如遵循复杂指令的局限性、缺乏特定知识或易受对抗性提示的影响。这可以通过更好的数据生成和训练参数来改进
- LLM 幻觉
    - 幻觉指的是一本正经的胡说八道：看似流畅自然的表述，实则不符合事实或者是错误的
    - 幻觉影响了模型的可靠性和可信度，因此需要解决LLM的幻觉问题
    - 幻觉不一定是有害的，特别是在一些需要创造力或灵感的场合，对幻觉的容忍度取决于具体的应用场景
    - 幻觉分类
        - 内在幻觉：生成的内容与源内容相矛盾
        - 外在幻觉：生成的内容不能从源内容中得到验证，既不受源内容支持也不受其反驳
    - 为什么LLM会产生幻觉
        - 训练数据的重复性
        - 数据噪声的影响
        - 解码过程中的随机性
        - 训练与实际应用中的解码差异
    - 什么时候最容易产生幻觉
        - 错误的上下文信息
        - 上下文与内置知识的冲突
        - 处理长文本
        - 处理数值
        - 逻辑推断障碍
    - 如何度量幻觉
        - 人工评估：成本高
        - 自动评估
            - 命名实体误差 （命名实体是否存在）
            - 蕴含率 （句子数量占比）
        - 模型评估
        - 问答系统
        - 利用信息提取系统
    - 如何缓解幻觉
        - 创建高质量无噪声的数据集
        - 利用外部知识验证正确性 RAG
        - 采样多个输出并检查其一致性
        - 修改解码策略
- LLM 重复