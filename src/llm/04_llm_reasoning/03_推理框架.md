---
icon: lightbulb
---
# 推理引擎 / 推理框架 / 推理加速引擎
- vllm
    - continus batching，而不是static batching
    - PagedAttention对kv cache的有效管理
- Text Generation Inference (TGI)
    - HuggingFace 推出的一个项目，作为支持 HuggingFace Inference API 和 Hugging Chat 上的LLM 推理的工具，旨在支持大型语言模型的优化推理
    - Continuous batching
    - flash-attention 和 Paged Attention
    - 项目架构：
        - launcher 启动器（下载模型、启动server、启动router）
        - router 和 serve 负责主要的逻辑处理与模型调用
            - router : 是一个webserver 负责接收请求，然后放在 buffer 中，等收集到一定量的数据后，一个 batch 一个 batch 的以 rpc 的方式发送给 server 的去处理
            - server (Python gRPC服务) ： 在每个显卡上都启动了一个 server
- faster transformer (NVIDIA FasterTransformer (FT)) 
    - 是一个用于实现基于Transformer的神经网络推理的加速引擎
    - 英伟达新推出了TensorRT-LLM，相对来说更加易用，后续FasterTransformer将不再为维护了
    - 与 NVIDIA TensorRT 等其他编译器相比，FT 的最大特点是它支持以分布式方式进行 Transformer 大模型推理
    - 在底层，节点间或节点内通信依赖于 MPI 、 NVIDIA NCCL、Gloo等
    - 除了使用 C ++ 作为后端部署，FasterTransformer 还集成了 TensorFlow（使用 TensorFlow op）、PyTorch （使用 Pytorch op）和 Triton 作为后端框架进行部署。当前，TensorFlow op 仅支持单 GPU，而 PyTorch op 和 Triton 后端都支持多 GPU 和多节点
- TensorRT-LLM 英伟达家的
- Triton