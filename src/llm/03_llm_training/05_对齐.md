---
icon: lightbulb
---
# 对齐 & 强化学习

- 策略梯度 pg
- 近端策略优化 ppo (Proximal Policy Optimization) : 迭代更新策略以最大化奖励，同时保持接近初始行为。它使用奖励模型来对响应进行评分，并需要仔细调整超参数，包括学习率、批量大小和 PPO 剪辑范围
- rlhf
- dpo（Direct Preference Optimization）：直接偏好优化 直接优化策略，以最大化选择响应而非拒绝响应的可能性。它不需要奖励建模，这使得它在计算上比 PPO 更高效，但在质量方面略差。
- orpo
- 拒绝抽样 ：对于每个提示，使用经过训练的模型生成多个响应，并对其进行评分以推断选择/拒绝的答案。这会创建符合策略的数据，其中两个响应都来自正在训练的模型，从而提高对齐稳定性。
