# GPU显存计算
## 1. 大模型常见规格
一般模型的规格会体现在模型的名称上，例如 LLaMA2-13b，13b 就是其模型**参数量**的大小，意思是130亿的参数量。

如何查看
- huggingface上找到相应组织后，看模型的Collection

各模型汇总
- Llama
    - Llama 3.3：70B
    - Llama 3.2：1B，3B，11B（vision），90B（vision）
    - Llama 3.1：8B，70B，405B
    - Llama 3：8B，70B
- Qwen
    - Qwen 2.5：0.5B、1.5B、3B、7B、14B、32B、72B
    - QwQ：32B
- Deepseek
    - DeepSeek LLM：7B，67B  
    - DeepSeek V3：671B  
    - DeepSeek R1：671B，蒸馏版：（1.5B，7B，8B，14B，32B，70B）
- Gemini
    - Gemma 3：1B，4B，12B，27B

规格总结：
- 小型（<10B）：0.5B，1B，1.5B，3B，4B，7B，8B
- 中型（10B-100B）：11B，12B，14B，27B，32B，67B，70B，72B，90B
- 大型（>100B）：405B，671B

## 2. 模型文件有多大

大模型的文件大小与其参数量有关，通常大模型是以**半精度**存储的（一般放出来的模型文件都是fp16的）， nB 的模型文件大概是 2n GB多一些，例如 13B 的模型文件大小大约是 27GB 左右。

## 3. nB 模型推理需要多少显存
fp16加载到显存里做推理也是占 2n GB，和文件大小一致

## 4. nB 模型训练需要多少显存
基础显存：模型参数2n+梯度2n+优化器12n = 16nG 显存

激活值占用显存，和max len、batch size有关

解释：优化器部分必须用fp32（似乎fp16会导致训练不稳定），所以应该是 2+2+12=16，参考 ZeRO 论文

举例：7B 的 vicuna 在 fsdp 下总共 160G 显存勉强可以训练
    - 按照上面计算出 7 * 16 = 112G 是基础显存
    - 全量训练准备 20nG 大概是最低要求，除非内存充足，显存不够 offload 内存补


### 4.1. GPU 显存计算  version2

7B 
- 70亿 * 4 byte
  - 70亿参数
  - 每个参数 4 byte
  - 计算出总共多少GB，记为 a GB

全量微调 （需要5a GB）
- 模型本身 *1
- Gradient *1 
- Optimizer States * 2
- 变量 *1

LoRA (略比aGB大，训练的参数都不到原参数量的1%)  

Trainable: 20971520 | total: 7262703616 | Percentage: 0.2888%
- 模型本身 *1
- Gradient *1 * 1% 
- Optimizer States * 2 * 1%

### 4.2. gege version3

推理*2,训练*8

bf16

每个参数占2个字节

推理只有模型参数

训练，参数1，梯度1,优化器2,激活值不计，4份参数大小 * 2 = 8

## 5. 能否用4 * v100 32G训练vicuna 65b？
不能。

首先，llama 65b 的权重需要5 * v100 32G 才能完整加载到GPU

其次，vicuna 使用 flash-attention 加速训练，暂不支持 v100，需要 turing 架构之后的显卡

一般来说推理模型需要的显存约等于模型文件大小，全参训练需要的显存约为推理所需显存的三倍到四倍，正常来说，在不量化的情况下4张 v100 显卡推理 65b 的模型都会有一些吃力，无法进行训练，需要通过 LoRA 或者 QLoRA 采用低秩分解的方式才可以训练。

