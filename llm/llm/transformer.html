<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://liz-in-tech.github.io/pinkpig/llm/llm/transformer.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="Transformer源码解读"><meta property="og:description" content="Transformer源码解读 About 模型总体架构 超参数 张量维度转换 可训练参数量 源码"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2024-10-29T16:42:17.000Z"><meta property="article:author" content="Liz"><meta property="article:tag" content="LLM"><meta property="article:published_time" content="2024-05-24T00:00:00.000Z"><meta property="article:modified_time" content="2024-10-29T16:42:17.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Transformer源码解读","image":[""],"datePublished":"2024-05-24T00:00:00.000Z","dateModified":"2024-10-29T16:42:17.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/pinkpig/blogger.png"><title>Transformer源码解读 | Liz</title><meta name="description" content="Transformer源码解读 About 模型总体架构 超参数 张量维度转换 可训练参数量 源码">
    <link rel="preload" href="/pinkpig/assets/style-WcGEFLwN.css" as="style"><link rel="stylesheet" href="/pinkpig/assets/style-WcGEFLwN.css">
    <link rel="modulepreload" href="/pinkpig/assets/app-MeCKi1WJ.js"><link rel="modulepreload" href="/pinkpig/assets/transformer.html-Bksj3dyy.js"><link rel="modulepreload" href="/pinkpig/assets/transformer.html-nbDjBHKV.js"><link rel="modulepreload" href="/pinkpig/assets/plugin-vue_export-helper-x3n3nnut.js">
    <link rel="prefetch" href="/pinkpig/assets/index.html-ibkdGA84.js" as="script"><link rel="prefetch" href="/pinkpig/assets/intro.html-VEF0ZusF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/blog_and_pinkpig.html-8NGYArRG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/tools.html-07OuYGth.js" as="script"><link rel="prefetch" href="/pinkpig/assets/youtube_channel.html-QpTsrp5k.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSAPP.html-DfrX1UAP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Netty.html-p8gk6fJ_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/RPC.html-7C88YC2N.js" as="script"><link rel="prefetch" href="/pinkpig/assets/操作系统.html-1QgCNUH3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/浏览器技能.html-BDECznX2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/网络.html-36AG3eKy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/计算机技能.html-H-ZX4k04.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AntDesign.html-5NgDnkhT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSS.html-DWW8u9ge.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Expo.html-lJQuUz4Q.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Frontend.html-a7NJdFWI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/HTML.html-9wodgV5c.js" as="script"><link rel="prefetch" href="/pinkpig/assets/JavaScript.html-lvtDI2LR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Practice.html-lui81Yar.js" as="script"><link rel="prefetch" href="/pinkpig/assets/React.html-b8iVrI2O.js" as="script"><link rel="prefetch" href="/pinkpig/assets/npm.html-cf-i0MG_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/commen_mistakes.html-mGgYvOuL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/grammar.html-H-3z-vXH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english3.html--aWr1mmm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english_detail.html-MQS8h2D6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/pronunciation.html-d7en6ro-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/sentence_pattern_and_expression.html-bv8MyCdJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llm_resources.html-i9D8ymtB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/open_interpreter.html-OonIxWRM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Git使用手册.html--rWVZDjM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Keymap.html-oa0K3y1z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Problem_and_plugin.html-S07wVB3Y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Markdown.html-p0NdC-RI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Maven--java包管理工具.html-1fgii2ME.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Poetry--python包管理工具.html-k_s-bX6Y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/photoshop.html-kcZA9JjG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/quick_recovery.html-57YZ5vgN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/solve_trick.html-R1zd6M1x.js" as="script"><link rel="prefetch" href="/pinkpig/assets/多层迷宫.html-5S0uvLI1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/算法提升.html-3eGbfge7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/经典题汇总（每个细分类限定10题以内）.html-q2-dePgM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java8学习笔记.html-gBdmNduj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/基础.html-1_ocEQ4I.js" as="script"><link rel="prefetch" href="/pinkpig/assets/集合.html-B3Wp3Upx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/juc.html-gl7Xq0UJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/jvm.html-ZAK0WB8C.js" as="script"><link rel="prefetch" href="/pinkpig/assets/spring.html-D6cEh02G.js" as="script"><link rel="prefetch" href="/pinkpig/assets/careers.html-yiJQBxx-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/common.html-8eP_H2he.js" as="script"><link rel="prefetch" href="/pinkpig/assets/communication.html-Ws1GaxQj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computers.html-E9ZVn5M0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/describing_something.html-qB30C2Nz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/dreams.html-KfGof6S4.js" as="script"><link rel="prefetch" href="/pinkpig/assets/graduating.html-P8cgR_ex.js" as="script"><link rel="prefetch" href="/pinkpig/assets/greetings.html-5eDIKNUU.js" as="script"><link rel="prefetch" href="/pinkpig/assets/hobbies.html-j7YJS0Uj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/immigration.html-N3b_zhs7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/introducing_someone.html-8g3aWQlk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/phone.html-83IoFm_-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/routine.html-HbuB0pac.js" as="script"><link rel="prefetch" href="/pinkpig/assets/time_and_weather.html-iKp7YvRN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/traits.html-2kgfa3hS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/agent.html-6lb4xXi7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computer_use.html-peJhAY_z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/learning_resources.html-kWSqgTvd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/same.html-VkYZOA7A.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain.html-DiLu1TNX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain1.html-N8fuBmmQ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain_source_code.html-RdRBg4zJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langgraph.html-epN2amRI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langsmith.html-NA1uMsEv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/streamlit.html-DaOhEev2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain.html-v5I8Kifm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain_source_code.html-OCzKdjxH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llama.html-9cDrSRNR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llama_advanced.html-naBkMCZ4.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llm_summary.html-6bAj0cAx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/streamlit.html-EAzWk0RM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/rag_opensource.html-j7xWlgzI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/GraphQL.html-zq9chJmn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MicroService.html-gqQ9Z22K.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MybatisPlus.html-sXrDLPET.js" as="script"><link rel="prefetch" href="/pinkpig/assets/mq.html-NsDzST8S.js" as="script"><link rel="prefetch" href="/pinkpig/assets/SQL.html-h6ZRnzFD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/mysql.html-NJkB6CdL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/redis.html-BLmZTn6L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Docker.html-eiI8tiKy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/K8S.html-hVxZM-yZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/linux.html-6BddWckV.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_python_environment.html-ECBOX7G8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_python_data_type.html-SsgcwWnj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_python_operator.html-WW78TP0B.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_python_method.html-gy5ccBYv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_python_builtin_module.html-W4QAcEOF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_python_popular_package.html-4W72Pdg1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_ai_concept.html-FUCs1g0L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_neural_net_train.html-nC9FySn1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_pytorch_operation.html-yukjJeR7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_pytorch_practice_nn.html-6bU2zAkC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_linear_nn.html-V-M3_Tfb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_heterogeneous_graph.html-ljs05wDt.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AI_evolution.html-dPUJTcbb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/0.时空复杂度.html-H3Qs0219.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.分治思想_递归实现.html-ZR8C9fAr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.二进制_位运算.html-IPYNI70m.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.排序.html-oDCsHciu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.二分查找.html-6XjBZtvX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.动态规划_贪心.html-JlAVeSTu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.字符串.html-H_Kw0VHp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.数学.html-JB3Fd046.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.算法技巧.html-0zv2JKDG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.数组.html-2pX8ASkd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.链表.html-MDTUDdof.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.栈.html-VhlnLXDW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.队列.html-5RW94C74.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.堆（优先队列）.html-9UtOuX_z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.树.html-eY_fw8EL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.图.html-C6zmof_l.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.哈希表（散列表）.html-rEshAQ6r.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java语言基础.html-zkZgse3g.js" as="script"><link rel="prefetch" href="/pinkpig/assets/python算法刷题语法快速恢复.html-MwR2aQKw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/404.html-ELYBSFNe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-0a3wtf0M.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZtH9hb0Y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ndBqxueE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FhfXXQqd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Y18Gk6ug.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PYFEr-gw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-n6sTAWGG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-SpiN3Ual.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-lySh48SJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-QLcoUYFo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-OkAhqWR6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-WjAYn9Z-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ayy3EF9I.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-s_gR8OaW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ErRX4wOI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-kLIVwRrx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dwZlzFOb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-oeeST0kk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-MDiPagDx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-RmQTNmcI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-uTiIdO0F.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-TrYac60Q.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-OWFxg9Im.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PfTXE4w9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--tUsRM8W.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ik3y9HJ8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2LBMckbl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-avJpz5gY.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-aX5SAJZk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-7kqkDjc1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-11KJ3gwb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-LQuERQvK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-6iizFISA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-qSCg3dme.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-3AOtZ1O7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-NUrjn_rx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZkjR0Tnl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-4fJRFXlS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-P-PEBKdE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Drd3Fdts.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-0Zni0yS2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-iJmuibHr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-bUj5_ism.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-x4TH_xzP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-yZYnH68c.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-M3KBctL2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-8WjFejoc.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-KzgtbF_l.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-xtYkLxjR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-xfwIgr3s.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-AVn-_fFT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-V6IelPdJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-wvgAZYvE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Bp62bPd5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FTMRL3lu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Ob7dvR-M.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2oWEcuGm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Di6h8Ky1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-5GuLlprQ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dbwd_1YG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dQ2QFe76.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-DKIbv2Jr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-kmPKSOrX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PjKCTcgX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-j0J10-eI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-WHhL-UfW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FHbtXQBu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-mcSwmMGo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/intro.html-Hw2s8EfE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/blog_and_pinkpig.html-dQGTgxMm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/tools.html-6njTGhHx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/youtube_channel.html-HfA5X0lc.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSAPP.html-SbnbGuEi.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Netty.html--IVhMv2E.js" as="script"><link rel="prefetch" href="/pinkpig/assets/RPC.html-xlbZc6Mg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/操作系统.html-4CP8_cq0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/浏览器技能.html-kTNrjwf5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/网络.html-99A5I10w.js" as="script"><link rel="prefetch" href="/pinkpig/assets/计算机技能.html-8FFp2Bc2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AntDesign.html-lmYPxH4M.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSS.html-g51BB3HW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Expo.html-BKKDhLVj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Frontend.html-M6IXIEti.js" as="script"><link rel="prefetch" href="/pinkpig/assets/HTML.html-C3GMQwcl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/JavaScript.html-R057wnmH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Practice.html-D_K0lUoI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/React.html-3oUi1oU-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/npm.html-8sxxtInn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/commen_mistakes.html-DRbEqBqB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/grammar.html-KQo2r4Gh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english3.html-G354M3MW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english_detail.html-fllXnXfY.js" as="script"><link rel="prefetch" href="/pinkpig/assets/pronunciation.html-HDXFigus.js" as="script"><link rel="prefetch" href="/pinkpig/assets/sentence_pattern_and_expression.html-YALvHoOS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llm_resources.html-xx31kedv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/open_interpreter.html-F-KRFhjH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Git使用手册.html-G5nq0H63.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Keymap.html-wdXt7WQh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Problem_and_plugin.html-ENk2lRX5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Markdown.html-eNdlQGkl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Maven--java包管理工具.html-GP1Pc5DW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Poetry--python包管理工具.html-pFVsrtmR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/photoshop.html-Thxc-ELl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/quick_recovery.html-5wiObx1i.js" as="script"><link rel="prefetch" href="/pinkpig/assets/solve_trick.html-OBq2MCxG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/多层迷宫.html-dK26C_34.js" as="script"><link rel="prefetch" href="/pinkpig/assets/算法提升.html-GXPrTbCj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/经典题汇总（每个细分类限定10题以内）.html-fzLLbZ7d.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java8学习笔记.html-eCe9L_8t.js" as="script"><link rel="prefetch" href="/pinkpig/assets/基础.html-cqZzqTlF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/集合.html-O9555Nz_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/juc.html-zO2Bkqmo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/jvm.html-9DAJ-qSH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/spring.html-Yf2BMwrd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/careers.html-QkP99RrL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/common.html-AJ_qDhPM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/communication.html-rhe1raki.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computers.html-yDDHUNP2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/describing_something.html-XydjZVZs.js" as="script"><link rel="prefetch" href="/pinkpig/assets/dreams.html--ss056Hc.js" as="script"><link rel="prefetch" href="/pinkpig/assets/graduating.html-3xVjgthx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/greetings.html-DPZIXjeZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/hobbies.html-fefsQrwr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/immigration.html-J7gllX4t.js" as="script"><link rel="prefetch" href="/pinkpig/assets/introducing_someone.html-LNt1snPa.js" as="script"><link rel="prefetch" href="/pinkpig/assets/phone.html-JpJvVwTQ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/routine.html-2zHOHtdE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/time_and_weather.html-YoEBRhvn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/traits.html-rw1WJzdF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/agent.html-E7Kw2Krk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computer_use.html-IjZ6_iW3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/learning_resources.html-NtkueTzH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/same.html-yDK6pmqZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain.html-4KpBoKCf.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain1.html-41CgNdfg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain_source_code.html-y-MYP424.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langgraph.html-arott71g.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langsmith.html-hI216jJZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/streamlit.html-_VNe3AxF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain.html-EPKHRRDT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain_source_code.html-AmI1nO6L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llama.html-gmRwjPMk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llama_advanced.html-pa4rDeJA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llm_summary.html-vsW5QUkr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/streamlit.html-T2DkQf24.js" as="script"><link rel="prefetch" href="/pinkpig/assets/rag_opensource.html-rMIIeo6A.js" as="script"><link rel="prefetch" href="/pinkpig/assets/GraphQL.html-bfynpN28.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MicroService.html-g3al1TAE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MybatisPlus.html-sdUl1j23.js" as="script"><link rel="prefetch" href="/pinkpig/assets/mq.html-GVhvoh0x.js" as="script"><link rel="prefetch" href="/pinkpig/assets/SQL.html-tnO0vGuJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/mysql.html-8XzQszAC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/redis.html-fY2wtl1K.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Docker.html-qwP2xxod.js" as="script"><link rel="prefetch" href="/pinkpig/assets/K8S.html-oj7-BwOi.js" as="script"><link rel="prefetch" href="/pinkpig/assets/linux.html--lEVq_DH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_python_environment.html-FTALu8uS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_python_data_type.html-4DZjOZoK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_python_operator.html-_ySPrGQR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_python_method.html-6NcPVocP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_python_builtin_module.html-BRLEaBW6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_python_popular_package.html-vpoaW7WC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_ai_concept.html-e70kDhbx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_neural_net_train.html-c2UBGGz8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_pytorch_operation.html-m5I8McFT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_pytorch_practice_nn.html-efxU81Ei.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_linear_nn.html-rNdpOwPm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_heterogeneous_graph.html-L__BGe9y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AI_evolution.html-YeJPkduz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/0.时空复杂度.html-02l1JYv-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.分治思想_递归实现.html-oXp0GLe7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.二进制_位运算.html-u7W17u-R.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.排序.html-PBi_GK1b.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.二分查找.html-J77tEt0y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.动态规划_贪心.html-2Gcm1wg3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.字符串.html-xc2VYW6d.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.数学.html-uMM3aqDb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.算法技巧.html-UKw5Y81J.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.数组.html-0AeTtDp2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.链表.html-XdTWCCh8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.栈.html-Tv3vCLBH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.队列.html-2QOenpMW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.堆（优先队列）.html-KhqMQs40.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.树.html-N11c09lR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.图.html-tzXEd2Ax.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.哈希表（散列表）.html-Ss0uUWNN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java语言基础.html-FafXo_jx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/python算法刷题语法快速恢复.html-8VKi4F5C.js" as="script"><link rel="prefetch" href="/pinkpig/assets/404.html-U9jzBQsS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Phu45JU9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-U9gwD359.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--OvLgmJp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-tJVNe8U1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-NtXcNG4C.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-zzEv_hiW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-BxljfEN8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-wsnKZZyY.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FdEYArYm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-KW2JM_sy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ANuHkaRe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-_DRLm3q0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-nChzfwBh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-X9ESL1Dh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Pdl7H7Zh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-KI1A7Xvt.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-48T-DoUg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-hGDoFxOc.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-h7vt7wl-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-5myq3UnC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Am_Rj7Yv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-JNYrY0sP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--b_RC3K7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-wHj1Wc2h.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-c4UgAkis.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-pUf8dhdK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2nuTpMGr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-XYe5bETU.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-RMCe_awG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PWfOR839.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--ewWxRbE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-_PpYh-iV.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-NMClFCTL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-gRaMPnWI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Nt2rhkwL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-G4nx0C-w.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2nQvHRf1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-DPSpf-nX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-EsqIaA0d.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-CTBt5Viz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-MyNGeZ90.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-BmPZ8qsu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-nd4m2kxL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-8J7p-r_K.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-g5UQteql.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-jKzcWASB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-UWOpmb-K.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-8iQZU0ZE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-7M6fBS-1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-de3tXy30.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--U7QkUym.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Ky6CrP90.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-t8ZHuy9o.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-TyQBJjPA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-bvAt2UY_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Oq-j-E74.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-IEeaQIT9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-0kzOTIMG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-P2yz_VdM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-tA9hqhiw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Wjfz7617.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-oNqtJjAm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-3rYreNW8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-TT3twP7o.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-u6GsDZyK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FgUOSKL3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ezW3lZoS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/pinkpig/"><img class="vp-nav-logo" src="/pinkpig/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Channel" class="vp-link nav-link nav-link" href="/pinkpig/channel/"><!---->Channel<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Language" class="vp-link nav-link nav-link" href="/pinkpig/language/"><!---->Language<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="LLM" class="vp-link nav-link active nav-link active" href="/pinkpig/llm/"><!---->LLM<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Python" class="vp-link nav-link nav-link" href="/pinkpig/python/"><!---->Python<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Java" class="vp-link nav-link nav-link" href="/pinkpig/java/"><!---->Java<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Frontend" class="vp-link nav-link nav-link" href="/pinkpig/frontend/"><!---->Frontend<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Middleware" class="vp-link nav-link nav-link" href="/pinkpig/middleware/"><!---->Middleware<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="CS" class="vp-link nav-link nav-link" href="/pinkpig/cs/"><!---->CS<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Operations" class="vp-link nav-link nav-link" href="/pinkpig/operations/"><!---->Operations<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Tools" class="vp-link nav-link nav-link" href="/pinkpig/tools/"><!---->Tools<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!----><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Llm</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a aria-label="/llm/llm_resources" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm_resources.html"><!---->/llm/llm_resources<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Agent</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Computer Use</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Langchain</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><!----><span class="vp-sidebar-title">Llm</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a aria-label="Llama源码解读" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm/llama.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Llama源码解读<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Llama进阶" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm/llama_advanced.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Llama进阶<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="LLM汇总" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm/llm_summary.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>LLM汇总<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="streamlit构建对话式应用程序" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm/streamlit.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>streamlit构建对话式应用程序<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Transformer源码解读" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/pinkpig/llm/llm/transformer.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Transformer源码解读<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="1. About" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_1-about"><!---->1. About<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="2. 模型总体架构" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_2-模型总体架构"><!---->2. 模型总体架构<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="3. 超参数" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_3-超参数"><!---->3. 超参数<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="4. 张量维度转换" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_4-张量维度转换"><!---->4. 张量维度转换<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5. 可训练参数量" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_5-可训练参数量"><!---->5. 可训练参数量<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="5.1. MultiHeadedAttention" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_5-1-multiheadedattention"><!---->5.1. MultiHeadedAttention<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5.2. PositionwiseFeedForward" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_5-2-positionwisefeedforward"><!---->5.2. PositionwiseFeedForward<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5.3. LayerNorm" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_5-3-layernorm"><!---->5.3. LayerNorm<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5.4. Embeddings" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_5-4-embeddings"><!---->5.4. Embeddings<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5.5. 总的可训练参数量" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_5-5-总的可训练参数量"><!---->5.5. 总的可训练参数量<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6. 源码" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-源码"><!---->6. 源码<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="6.1. 完整模型" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-1-完整模型"><!---->6.1. 完整模型<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.2. EncoderDecoder（编码器-解码器结构）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-2-encoderdecoder-编码器-解码器结构"><!---->6.2. EncoderDecoder（编码器-解码器结构）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.3. Encoder（编码器）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-3-encoder-编码器"><!---->6.3. Encoder（编码器）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.4. Decoder（解码器）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-4-decoder-解码器"><!---->6.4. Decoder（解码器）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.5. MultiHeadedAttention（多头注意力）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-5-multiheadedattention-多头注意力"><!---->6.5. MultiHeadedAttention（多头注意力）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.6. PositionwiseFeedForward（基于位置的前馈网络）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-6-positionwisefeedforward-基于位置的前馈网络"><!---->6.6. PositionwiseFeedForward（基于位置的前馈网络）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.7. Embeddings（嵌入层）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-7-embeddings-嵌入层"><!---->6.7. Embeddings（嵌入层）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.8. PositionalEncoding（位置编码）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-8-positionalencoding-位置编码"><!---->6.8. PositionalEncoding（位置编码）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.9. Generator（生成器）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-9-generator-生成器"><!---->6.9. Generator（生成器）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.10. clones（克隆）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-10-clones-克隆"><!---->6.10. clones（克隆）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.11. LayerNorm（层归一化）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-11-layernorm-层归一化"><!---->6.11. LayerNorm（层归一化）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.12. SublayerConnection（子层连接）" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-12-sublayerconnection-子层连接"><!---->6.12. SublayerConnection（子层连接）<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.13. 模型使用的例子" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/llm/transformer.html#_6-13-模型使用的例子"><!---->6.13. 模型使用的例子<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li></ul><!--]--></li><li><!--[--><a aria-label="一文带你了解LangChain: 使用大语言模型构建强大的应用程序" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm/langchain.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>一文带你了解LangChain: 使用大语言模型构建强大的应用程序<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="从源码视角，窥探LangChain的运行逻辑" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/llm/langchain_source_code.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>从源码视角，窥探LangChain的运行逻辑<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><!--[--><a aria-label="Open Interpreter" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/open_interpreter.html"><!---->Open Interpreter<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">Rag</span><span class="vp-arrow end"></span></button><!----></section></li></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Transformer源码解读</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-05-24T00:00:00.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 24 min</span><meta property="timeRequired" content="PT24M"></span><span class="page-category-info" aria-label="Category🌈" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item category6 clickable" role="navigation">LLM</span><!--]--><meta property="articleSection" content="LLM"></span><span class="page-tag-info" aria-label="Tag🏷" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><!--[--><span class="page-tag-item tag6 clickable" role="navigation">LLM</span><!--]--><meta property="keywords" content="LLM"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-about">1. About</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-模型总体架构">2. 模型总体架构</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-超参数">3. 超参数</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-张量维度转换">4. 张量维度转换</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-可训练参数量">5. 可训练参数量</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-1-multiheadedattention">5.1. MultiHeadedAttention</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-2-positionwisefeedforward">5.2. PositionwiseFeedForward</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-3-layernorm">5.3. LayerNorm</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-4-embeddings">5.4. Embeddings</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-5-总的可训练参数量">5.5. 总的可训练参数量</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-源码">6. 源码</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-1-完整模型">6.1. 完整模型</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-2-encoderdecoder-编码器-解码器结构">6.2. EncoderDecoder（编码器-解码器结构）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-3-encoder-编码器">6.3. Encoder（编码器）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-4-decoder-解码器">6.4. Decoder（解码器）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-5-multiheadedattention-多头注意力">6.5. MultiHeadedAttention（多头注意力）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-6-positionwisefeedforward-基于位置的前馈网络">6.6. PositionwiseFeedForward（基于位置的前馈网络）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-7-embeddings-嵌入层">6.7. Embeddings（嵌入层）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-8-positionalencoding-位置编码">6.8. PositionalEncoding（位置编码）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-9-generator-生成器">6.9. Generator（生成器）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-10-clones-克隆">6.10. clones（克隆）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-11-layernorm-层归一化">6.11. LayerNorm（层归一化）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-12-sublayerconnection-子层连接">6.12. SublayerConnection（子层连接）</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-13-模型使用的例子">6.13. 模型使用的例子</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="transformer源码解读" tabindex="-1"><a class="header-anchor" href="#transformer源码解读" aria-hidden="true">#</a> Transformer源码解读</h1><ul><li><ol><li>About</li></ol></li><li><ol start="2"><li>模型总体架构</li></ol></li><li><ol start="3"><li>超参数</li></ol></li><li><ol start="4"><li>张量维度转换</li></ol></li><li><ol start="5"><li>可训练参数量</li></ol></li><li><ol start="6"><li>源码</li></ol></li></ul><!-- more --><h2 id="_1-about" tabindex="-1"><a class="header-anchor" href="#_1-about" aria-hidden="true">#</a> 1. About</h2><p>论文：Attention Is All You Need</p><p>年份：2017</p><p>公司：Google</p><p>源码（哈佛NLP团队实现的Pytorch版）：https://github.com/harvardnlp/annotated-transformer/</p><p>配套源码解析： https://nlp.seas.harvard.edu/annotated-transformer/</p><h2 id="_2-模型总体架构" tabindex="-1"><a class="header-anchor" href="#_2-模型总体架构" aria-hidden="true">#</a> 2. 模型总体架构</h2><figure><img src="/pinkpig/assets/Transformer_Overall_Architecture-3tnx_yjq.png" alt="Transformer模型总体架构" tabindex="0" loading="lazy"><figcaption>Transformer模型总体架构</figcaption></figure><p>编码器（左侧部分）：&quot;Inputs&quot;这是编码器的输入，比如说你中文翻英文的话，那么这就是你中文的句子。编码器将输入序列（x<sub>1</sub>,...,x<sub>n</sub>）映射为一系列向量表示 z =（z<sub>1</sub>,...,z<sub>n</sub>）。其中每个x<sub>t</sub>表示输入序列中的第t个元素（例如，一个单词），每个z<sub>t</sub>是相对于x<sub>t</sub>的向量表示。这些向量表示可以被视为输入序列的抽象表示，使得机器学习模型能够更好地理解输入序列。</p><p>解码器（右侧部分）：&quot;Outputs&quot;这是解码器的输入，解码器在做预测的时候是没有输入的。&quot;Shifted Right&quot;就是一个一个往右移。解码器根据编码器的输出进行工作，对z中的每个元素，生成目标序列（y<sub>1</sub>,...,y<sub>m</sub>）,一个时间步生成一个元素，其中m可能与n不同。在每一步中，模型都是自回归的，在生成下一个结果时，会将先前生成的结果加入输入序列来一起预测。（自回归模型的特点）</p><p>解码器与编码器的一个重要区别是，解码器是自回归的（auto-regressive）。这意味着它逐步生成输出序列，而不是一次性处理整个序列。在生成第一个输出y<sub>1</sub>之后，解码器使用先前生成的输出作为输入来生成下一个输出，以此类推，直到生成完整的目标序列。</p><p>总结来说，编码器将输入序列转换为一系列向量表示，而解码器根据编码器的输出逐步生成目标序列。解码器的自回归特性使得它能够逐步生成输出，并利用先前生成的输出作为输入。这种编码器-解码器架构在序列到序列任务中被广泛应用，如机器翻译、摘要生成等。</p><figure><img src="/pinkpig/assets/Transformer_Source_Code_Architecture-wdUsfZDe.jpg" alt="源码对照模型架构" tabindex="0" loading="lazy"><figcaption>源码对照模型架构</figcaption></figure><h2 id="_3-超参数" tabindex="-1"><a class="header-anchor" href="#_3-超参数" aria-hidden="true">#</a> 3. 超参数</h2><figure><img src="/pinkpig/assets/Transformer_Hyperparameters-ePsO0-QM.png" alt="超参数" tabindex="0" loading="lazy"><figcaption>超参数</figcaption></figure><h2 id="_4-张量维度转换" tabindex="-1"><a class="header-anchor" href="#_4-张量维度转换" aria-hidden="true">#</a> 4. 张量维度转换</h2><figure><img src="/pinkpig/assets/Encoder_Tensor_Dimension_Transformation--hFf2Slu.png" alt="编码器张量维度转换" tabindex="0" loading="lazy"><figcaption>编码器张量维度转换</figcaption></figure><figure><img src="/pinkpig/assets/Decoder_Tensor_Dimension_Transformation-F_HW4iby.png" alt="解码器张量维度转换" tabindex="0" loading="lazy"><figcaption>解码器张量维度转换</figcaption></figure><h2 id="_5-可训练参数量" tabindex="-1"><a class="header-anchor" href="#_5-可训练参数量" aria-hidden="true">#</a> 5. 可训练参数量</h2><figure><img src="/pinkpig/assets/Trainable_Parameters-NgRC5WB4.png" alt="可训练参数量" tabindex="0" loading="lazy"><figcaption>可训练参数量</figcaption></figure><h3 id="_5-1-multiheadedattention" tabindex="-1"><a class="header-anchor" href="#_5-1-multiheadedattention" aria-hidden="true">#</a> 5.1. MultiHeadedAttention</h3><p>该块的模型参数：MultiHeadedAttention的4个线性变换的权重和偏置。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mtext>、</mtext><mi>K</mi><mtext>、</mtext><mi>V</mi></mrow><annotation encoding="application/x-tex">Q、K、V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord cjk_fallback">、</span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord cjk_fallback">、</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>的权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>Q</mi></msub><mtext>、</mtext><msub><mi>W</mi><mi>K</mi></msub><mtext>、</mtext><msub><mi>W</mi><mi>V</mi></msub></mrow><annotation encoding="application/x-tex">W_Q、W_K、W_V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和对应偏置，以及输出权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>O</mi></msub></mrow><annotation encoding="application/x-tex">W_O</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和对应偏置</p><p>该块的参数量大小：4个权重矩阵的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo separator="true">,</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d=512,d=512]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">512</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">512</span><span class="mclose">]</span></span></span></span>, 4个偏置的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d=512]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">512</span><span class="mclose">]</span></span></span></span>, MultiHeadedAttention块总的参数量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>∗</mo><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">4*(d^2+d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></p><p>该块的个数：编码层有1个子层是MultiHeadedAttention，解码层有2个子层是MultiHeadedAttention，编码器和解码器分别有6个相同的编码层和解码层，所以总共有18个MultiHeadedAttention块</p><p>Transformer中该块总的参数量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>18</mn><mo>∗</mo><mn>4</mn><mo>∗</mo><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>72</mn><mo>∗</mo><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>18</mn><mo>∗</mo><mn>4</mn><mo>∗</mo><mo stretchy="false">(</mo><mn>51</mn><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><mn>512</mn><mo stretchy="false">)</mo><mo>=</mo><mn>18911232</mn></mrow><annotation encoding="application/x-tex">18*4*(d^2+d)=72*(d^2+d)=18*4*(512^2+512)=18911232</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">18</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">72</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">18</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">51</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">512</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">18911232</span></span></span></span></p><h3 id="_5-2-positionwisefeedforward" tabindex="-1"><a class="header-anchor" href="#_5-2-positionwisefeedforward" aria-hidden="true">#</a> 5.2. PositionwiseFeedForward</h3><p>该块的模型参数：PositionwiseFeedForward的2个线性变换的权重和偏置。第一个线性层是先将维度从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>映射到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>d</mi><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">4d=2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2048</span></span></span></span>,第二个线性层再将维度从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi>d</mi><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">4d=2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2048</span></span></span></span>映射到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span></p><p>该块的参数量大小：第一个线性层的权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">W_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo separator="true">,</mo><mn>4</mn><mi>d</mi><mo>=</mo><mn>2048</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d=512,4d=2048]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">512</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2048</span><span class="mclose">]</span></span></span></span>,偏置的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>4</mn><mi>d</mi><mo>=</mo><mn>2048</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[4d=2048]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2048</span><span class="mclose">]</span></span></span></span>, 第二个线性层的权重矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">W_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>4</mn><mi>d</mi><mo>=</mo><mn>2048</mn><mo separator="true">,</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[4d=2048,d=512]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord">2048</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">512</span><span class="mclose">]</span></span></span></span>,偏置的形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d=512]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">512</span><span class="mclose">]</span></span></span></span>, PositionwiseFeedForward块总的参数量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>∗</mo><mn>4</mn><mi>d</mi><mo>+</mo><mn>4</mn><mi>d</mi><mo>+</mo><mn>4</mn><mi>d</mi><mo>∗</mo><mi>d</mi><mo>+</mo><mi>d</mi><mo>=</mo><mn>8</mn><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mn>5</mn><mi>d</mi></mrow><annotation encoding="application/x-tex">d*4d+4d+4d*d+d=8d^2+5d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">4</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">8</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">5</span><span class="mord mathnormal">d</span></span></span></span></p><p>该块的个数：编码层和解码层各有1个子层是PositionwiseFeedForward，编码器和解码器分别有6个相同的编码层和解码层，所以总共有12个PositionwiseFeedForward块</p><p>Transformer中该块总的参数量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn><mo>∗</mo><mo stretchy="false">(</mo><mn>8</mn><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mn>5</mn><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>96</mn><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mn>60</mn><mi>d</mi><mo>=</mo><mn>96</mn><mo>∗</mo><mn>51</mn><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><mn>60</mn><mo>∗</mo><mn>512</mn><mo>=</mo><mn>25196544</mn></mrow><annotation encoding="application/x-tex">12*(8d^2+5d)=96d^2+60d=96*512^2+60*512=25196544</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">12</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">8</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">5</span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">96</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">60</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">96</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">51</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">60</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">25196544</span></span></span></span></p><h3 id="_5-3-layernorm" tabindex="-1"><a class="header-anchor" href="#_5-3-layernorm" aria-hidden="true">#</a> 5.3. LayerNorm</h3><p>该块的模型参数：LayerNorm包含2个可训练参数：缩放参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span>和平移参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span></p><p>该块的参数量大小：两个参数形状都为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>d</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[d]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal">d</span><span class="mclose">]</span></span></span></span>, LayerNorm块总的参数量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>+</mo><mi>d</mi><mo>=</mo><mn>2</mn><mi>d</mi></mrow><annotation encoding="application/x-tex">d+d=2d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">2</span><span class="mord mathnormal">d</span></span></span></span></p><p>该块的个数：编码层和解码层的每个子层连接有一个LayerNorm，共有5个子层连接，编码器和解码器分别有6个相同的编码层和解码层，并在6个编码层和解码层后有一个LayerNorm，所以总共有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn><mo>∗</mo><mn>2</mn><mo>+</mo><mn>2</mn><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">6*2+2=32</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32</span></span></span></span>个LayerNorm块</p><p>Transformer中该块总的参数量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn><mo>∗</mo><mn>2</mn><mi>d</mi><mo>=</mo><mn>64</mn><mi>d</mi><mo>=</mo><mn>64</mn><mo>∗</mo><mn>512</mn><mo>=</mo><mn>32768</mn></mrow><annotation encoding="application/x-tex">32*2d=64d=64*512=32768</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">2</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">64</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">32768</span></span></span></span></p><h3 id="_5-4-embeddings" tabindex="-1"><a class="header-anchor" href="#_5-4-embeddings" aria-hidden="true">#</a> 5.4. Embeddings</h3><p>该块的模型参数：该块出现在Transformer中的3个位置（Input Embeddings、Output Embeddings、Generator），但是参数共享，所以只有一份参数</p><p>该块的参数量大小：形状为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo separator="true">,</mo><mi>d</mi><mo>=</mo><mn>512</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[vocab,d=512]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">512</span><span class="mclose">]</span></span></span></span>, Embeddings块总的参数量为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">vocab*d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span></p><p>该块的个数：1</p><p>Transformer中该块总的参数量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mi>d</mi><mo>=</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">vocab*d=vocab*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span></p><h3 id="_5-5-总的可训练参数量" tabindex="-1"><a class="header-anchor" href="#_5-5-总的可训练参数量" aria-hidden="true">#</a> 5.5. 总的可训练参数量</h3><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>72</mn><mo>∗</mo><mo stretchy="false">(</mo><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mi>d</mi><mo stretchy="false">)</mo><mo>+</mo><mn>96</mn><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mn>60</mn><mi>d</mi><mo>+</mo><mn>64</mn><mi>d</mi><mo>+</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mi>d</mi><mo>=</mo><mn>168</mn><mo>∗</mo><msup><mi>d</mi><mn>2</mn></msup><mo>+</mo><mn>196</mn><mo>∗</mo><mi>d</mi><mo>+</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mi>d</mi><mo>=</mo><mn>168</mn><mo>∗</mo><mn>51</mn><msup><mn>2</mn><mn>2</mn></msup><mo>+</mo><mn>196</mn><mo>∗</mo><mn>512</mn><mo>+</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mn>512</mn><mo>=</mo><mn>44140544</mn><mo>+</mo><mi>v</mi><mi>o</mi><mi>c</mi><mi>a</mi><mi>b</mi><mo>∗</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">72*(d^2+d)+96d^2+60d+64d+vocab*d=168*d^2+196*d+vocab*d=168*512^2+196*512+vocab*512=44140544+vocab*512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">72</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">96</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord">60</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">168</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">196</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">168</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">51</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">196</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">44140544</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">oc</span><span class="mord mathnormal">ab</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span></p><h2 id="_6-源码" tabindex="-1"><a class="header-anchor" href="#_6-源码" aria-hidden="true">#</a> 6. 源码</h2><h3 id="_6-1-完整模型" tabindex="-1"><a class="header-anchor" href="#_6-1-完整模型" aria-hidden="true">#</a> 6.1. 完整模型</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> math
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">import</span> log_softmax
<span class="token keyword">import</span> copy

<span class="token keyword">def</span> <span class="token function">make_model</span><span class="token punctuation">(</span>src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> N<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span> d_model<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> d_ff<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> h<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    根据超参数构建一个模型。
    src_vocab:源句子的单词个数
    tgt_vocab:目标句子的单词个数
    N:编码层和解码层的层数
    d_model:词嵌入的维度/输入和输出的维度
    d_ff:Feed-Forward网络中隐藏层的维度/内层的维度
    h:注意力的头数
    dropout:是一种防止深度学习网络过拟合的技术，它是随机丢弃神经元，从而增加网络的泛化能力
    &quot;&quot;&quot;</span>
    c <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy <span class="token comment"># 深拷贝</span>
    attn <span class="token operator">=</span> MultiHeadedAttention<span class="token punctuation">(</span>h<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>  <span class="token comment"># 创建多头注意力机制实例</span>
    ff <span class="token operator">=</span> PositionwiseFeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>  <span class="token comment"># 创建位置前馈网络实例</span>
    position <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>  <span class="token comment"># 创建位置编码实例</span>
    model <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>
        Encoder<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 创建编码器实例</span>
        Decoder<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 创建解码器实例</span>
        nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 创建源语言嵌入层实例</span>
        nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 创建目标语言嵌入层实例</span>
        Generator<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># 创建生成器实例</span>
    <span class="token punctuation">)</span>

    <span class="token comment"># 初始化参数</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> p<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>p<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-2-encoderdecoder-编码器-解码器结构" tabindex="-1"><a class="header-anchor" href="#_6-2-encoderdecoder-编码器-解码器结构" aria-hidden="true">#</a> 6.2. EncoderDecoder（编码器-解码器结构）</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    A standard Encoder-Decoder architecture. Base for this and many other models.
    一个标准的编码器-解码器架构。是本例和许多其他模型的基础。
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span> src_embed<span class="token punctuation">,</span> tgt_embed<span class="token punctuation">,</span> generator<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder <span class="token comment"># 编码器</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder <span class="token comment"># 解码器</span>
        self<span class="token punctuation">.</span>src_embed <span class="token operator">=</span> src_embed <span class="token comment"># 源嵌入</span>
        self<span class="token punctuation">.</span>tgt_embed <span class="token operator">=</span> tgt_embed <span class="token comment"># 目标嵌入</span>
        self<span class="token punctuation">.</span>generator <span class="token operator">=</span> generator <span class="token comment"># 生成器</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Take in and process masked src and target sequences.&quot;</span>
        <span class="token string">&quot;接收并处理 mask 的 源（src） 和 目标（target） 序列。&quot;</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>src<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>src_embed<span class="token punctuation">(</span>src<span class="token punctuation">)</span><span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span> <span class="token comment"># 对源序列进行编码</span>

    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tgt_embed<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span><span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span> <span class="token comment"># 对目标序列进行解码</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-3-encoder-编码器" tabindex="-1"><a class="header-anchor" href="#_6-3-encoder-编码器" aria-hidden="true">#</a> 6.3. Encoder（编码器）</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Core encoder is a stack of N layers
    编码器由N=6个完全相同的层组成。
    包含N=6个层的堆叠
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span> <span class="token comment"># 创建 N 个 layer 的副本，并存储在 self.layers 中</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>size<span class="token punctuation">)</span> <span class="token comment"># 创建一个 LayerNorm ，并存储在 self.norm 中</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Pass the input (and mask) through each layer in turn.逐层传递输入和掩码&quot;</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span> <span class="token comment"># 逐层对输入 x 进行处理</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 对处理后的结果 x 进行 Layer Normalization（层归一化）</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Encoder is made up of self-attn and feed forward (defined below) 编码器由自注意力和前馈神经网络组成（如下）。&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> feed_forward<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> self_attn <span class="token comment"># 自注意力机制</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> feed_forward <span class="token comment"># 前馈神经网络</span>
        self<span class="token punctuation">.</span>sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 克隆两个子层连接</span>
        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Follow Figure 1 (left) for connections. 参考图1（左侧）进行连接。&quot;</span>
        <span class="token comment"># 第一个子层连接：自注意力机制</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 第二个子层连接：前馈神经网络</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-4-decoder-解码器" tabindex="-1"><a class="header-anchor" href="#_6-4-decoder-解码器" aria-hidden="true">#</a> 6.4. Decoder（解码器）</h3><p>解码器也是由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">N=6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">6</span></span></span></span> 个完全相同的层组成。</p><p>除了每个decoder层中的两个子层之外，decoder还有第三个子层，该层对encoder的输出执行multi-head attention。（即encoder-decoder-attention层，q向量来自上一层的输入，k和v向量是encoder最后层的输出向量memory）与encoder类似，我们在每个子层再采用残差连接，然后进行层标准化。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Generic N layer decoder with masking.
    通用的 带有掩码（masking）的 N层解码器
    包含 N 个层的堆叠
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span> <span class="token comment"># 克隆N个解码器层</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>size<span class="token punctuation">)</span> <span class="token comment"># 归一化层</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            <span class="token comment"># 应用每个解码器层，传递输入x、记忆memory、源掩码src_mask和目标掩码tgt_mask</span>
            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 对输出进行归一化处理</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Decoder is made of self-attn, src-attn, and feed forward (defined below) 解码器由自注意力机制（self-attn）、源注意力机制（src-attn）和前馈神经网络（feed forward）组成。&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> src_attn<span class="token punctuation">,</span> feed_forward<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size <span class="token comment"># 解码器层的大小</span>
        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> self_attn <span class="token comment"># 自注意力机制</span>
        self<span class="token punctuation">.</span>src_attn <span class="token operator">=</span> src_attn <span class="token comment"># 源注意力机制</span>
        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> feed_forward <span class="token comment"># 前馈神经网络</span>
        self<span class="token punctuation">.</span>sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token comment"># 克隆三个子层连接</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;Follow Figure 1 (right) for connections. 参考图1（右侧）进行连接。&quot;</span>
        m <span class="token operator">=</span> memory <span class="token comment"># 记忆</span>
        <span class="token comment"># 第一个子层连接：自注意力机制</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 第二个子层连接：源注意力机制</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>src_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> m<span class="token punctuation">,</span> m<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 第三个子层连接：前馈神经网络</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-5-multiheadedattention-多头注意力" tabindex="-1"><a class="header-anchor" href="#_6-5-multiheadedattention-多头注意力" aria-hidden="true">#</a> 6.5. MultiHeadedAttention（多头注意力）</h3><p>为什么使用多头的两点概括：①为了解决模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身位置的问题；②一定程度上h越大整个模型的表达能力越强，越能提高模型对于注意力权重的合理分配。</p><p>为什么要缩放：对于较大的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>来说在完成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>后将会得到很大的值，而这将导致在经过sofrmax操作后产生非常小的梯度，不利于网络的训练</p><p>在实践中，我们同时计算一组query的attention函数，并将它们组合成一个矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span></span></span></span>. key和value也一起组成矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>. 。 我们计算的输出矩阵为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex"> \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord"><span class="mord mathrm">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p><p>其中Q、K和V分别为3个矩阵，且其（第2个）维度分别为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">d_q,d_k,d_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,从后面的计算过程其实可以发现<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>q</mi></msub><mo>=</mo><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">d_q = d_v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;缩放的点积注意力
     &#39;Scaled Dot Product Attention&#39;
    query、key、value和输出都是向量。输出为value的加权和，其中每个value的权重通过query与相应key的兼容函数来计算
    &quot;&quot;&quot;</span>

    d_k <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 获取查询向量的最后一个维度大小，即注意力的维度</span>

    scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span>  <span class="token comment"># 计算注意力得分</span>

    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>  <span class="token comment"># 对掩码为0的位置进行填充，使得对应位置的注意力得分变为一个很小的负数</span>

    p_attn <span class="token operator">=</span> scores<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 对注意力得分进行softmax归一化，得到注意力权重</span>

    <span class="token keyword">if</span> dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        p_attn <span class="token operator">=</span> dropout<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span>  <span class="token comment"># 对注意力权重进行dropout操作</span>

    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>p_attn<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">,</span> p_attn  <span class="token comment"># 返回加权后的value和注意力权重</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">M</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">H</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">d</mi></mrow><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi></mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><msub><mi mathvariant="normal">d</mi><mn>1</mn></msub></mrow><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><msub><mi mathvariant="normal">d</mi><mi mathvariant="normal">h</mi></msub></mrow><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup><mspace linebreak="newline"></mspace><mtext>where </mtext><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><msub><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi></msub></mrow><mo>=</mo><mrow><mi mathvariant="normal">A</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\ \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Concat</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathrm">hea</span><span class="mord"><span class="mord mathrm">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathrm">hea</span><span class="mord"><span class="mord mathrm">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord text"><span class="mord">where</span></span><span class="mspace nobreak"> </span><span class="mord"><span class="mord mathrm">hea</span><span class="mord"><span class="mord mathrm">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3175em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathrm mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathrm">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>其中映射由权重矩阵完成： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-2.4413em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mi>O</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>h</mi><msub><mi>d</mi><mi>v</mi></msub><mo>×</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8804em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1645em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>.</p><p>在这项工作中，我们采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">h=8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">8</span></span></span></span> 个平行attention层或者叫head。对于这些head中的每一个，我们使用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><msub><mi>d</mi><mi>v</mi></msub><mo>=</mo><msub><mi>d</mi><mtext>model</mtext></msub><mi mathvariant="normal">/</mi><mi>h</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">d_k=d_v=d_{\text{model}}/h=64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span>，将每个head的维度减小，此时总计算成本与具有全部维度的单个head attention相似。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">MultiHeadedAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> h<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadedAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> d_model <span class="token operator">%</span> h <span class="token operator">==</span> <span class="token number">0</span>
        <span class="token comment"># 我们假设 d_v 总是等于 d_k</span>
        self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> d_model <span class="token operator">//</span> h  <span class="token comment"># 每个头部的注意力维度</span>
        self<span class="token punctuation">.</span>h <span class="token operator">=</span> h  <span class="token comment"># 头部的数量</span>
        <span class="token comment"># 定义四个Linear networks, 大小是(512, 512),里面有两类可训练参数，Weights，其大小为512*512，以及biases，其大小为512=d_model</span>
        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> clones<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 线性变换层的集合</span>
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># 用于存储注意力权重</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># (batch.size,1,seq.len) -&gt; (batch.size,1,1,seq.len)</span>
            mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        nbatches <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 批次大小</span>

        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        1) 在批次中对所有线性投影进行处理
        这里是前三个Linear Networks的具体应用，
        例如query=(batch.size, seq.len, 512) -&gt; Linear network -&gt; (batch.size, seq.len, 512)
        -&gt; view -&gt; (batch.size, seq.len, 8, 64) -&gt; transpose(1,2) -&gt; (batch.size, 8, seq.len, 64)，
        其他的key和value也是类似地，从(batch.size, seq.len, 512) -&gt; (batch.size, 8, seq.len, 64)
        &quot;&quot;&quot;</span>
        query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value <span class="token operator">=</span> <span class="token punctuation">[</span>lin<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> lin<span class="token punctuation">,</span> x <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>linears<span class="token punctuation">,</span> <span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

        <span class="token comment"># 2) 在批次中对所有投影向量应用注意力机制</span>
        x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attn <span class="token operator">=</span> attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">,</span> dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>

        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        3) 使用视图进行&quot;拼接&quot;，
        x ~ (batch.size, 8, seq.len, 64) -&gt; transpose(1,2) -&gt;
        (batch.size, seq.len, 8, 64) -&gt; contiguous() and view -&gt;
        (batch.size, seq.len, 8*64) = (batch.size, seq.len, 512)
        &quot;&quot;&quot;</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>
            x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h <span class="token operator">*</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">del</span> query
        <span class="token keyword">del</span> key
        <span class="token keyword">del</span> value

        <span class="token comment"># 4) 然后应用最后一个线性层</span>
        <span class="token comment"># 执行第四个Linear network，把(batch.size, seq.len, 512)经过一次linear network，得到(batch.size, seq.len, 512)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-6-positionwisefeedforward-基于位置的前馈网络" tabindex="-1"><a class="header-anchor" href="#_6-6-positionwisefeedforward-基于位置的前馈网络" aria-hidden="true">#</a> 6.6. PositionwiseFeedForward（基于位置的前馈网络）</h3><p>网络包括两个线性变换，并在两个线性变换中间有一个ReLU激活函数。</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">F</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">N</mi></mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex"> \mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">FFN</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p><p>尽管两层都是线性变换，但它们在层与层之间使用不同的参数。输入和输出的维度都是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d_{\text{model}}=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span> 内层维度是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>f</mi><mi>f</mi></mrow></msub><mo>=</mo><mn>2048</mn></mrow><annotation encoding="application/x-tex">d_{ff}=2048</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">ff</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2048</span></span></span></span>。（也就是第一层输入512维,输出2048维；第二层输入2048维，输出512维）</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">PositionwiseFeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;实现 FFN（Feed-Forward Network）&quot;</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># d_model = 512</span>
        <span class="token comment"># d_ff = 2048 = 512*4</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionwiseFeedForward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：weights矩阵，(512, 2048)，以及 biases偏移向量, (2048)</span>
        self<span class="token punctuation">.</span>w_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>
        <span class="token comment"># 构建第二个全连接层, (2048, 512)，两种可训练参数：weights矩阵，(2048, 512)，以及 biases偏移向量, (512)</span>
        self<span class="token punctuation">.</span>w_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        (batch.size, seq.len, 512) -&gt; self.w_1 -&gt; (batch.size, seq.len, 2048)
        -&gt; relu -&gt; (batch.size, seq.len, 2048)
        -&gt; dropout -&gt; (batch.size, seq.len, 2048)
        -&gt; self.w_2 -&gt; (batch.size, seq.len, 512)
        &quot;&quot;&quot;</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>w_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>relu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-7-embeddings-嵌入层" tabindex="-1"><a class="header-anchor" href="#_6-7-embeddings-嵌入层" aria-hidden="true">#</a> 6.7. Embeddings（嵌入层）</h3><p>将输入token和输出token转换为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>维的向量</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Embeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Embeddings<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># (vocab.len, 512)</span>
        self<span class="token punctuation">.</span>lut <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># (batch.size, seq.len) -&gt; (batch.size, seq.len, 512)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>lut<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-8-positionalencoding-位置编码" tabindex="-1"><a class="header-anchor" href="#_6-8-positionalencoding-位置编码" aria-hidden="true">#</a> 6.8. PositionalEncoding（位置编码）</h3><p>由于我们的模型不包含循环和卷积，为了让模型利用序列的顺序，我们必须加入一些序列中token的相对或者绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入embeddinng中。位置编码和embedding的维度相同，也是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> , 所以这两个向量可以相加。</p><p>使用不同频率的正弦和余弦函数：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> PE_{(pos,2i)} = \sin(pos / 10000^{2i/d_{\text{model}}}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><mn>1000</mn><msup><mn>0</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> PE_{(pos,2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}}) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0385em;vertical-align:-0.3552em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.5198em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3552em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span> 是位置， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 是维度。也就是说，位置编码的每个维度对应于一个正弦曲线。 这些波长形成一个从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>π</mi></mrow><annotation encoding="application/x-tex">2\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10000</mn><mo>⋅</mo><mn>2</mn><mi>π</mi></mrow><annotation encoding="application/x-tex">10000 \cdot 2\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">10000</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span>. 的集合级数。 我们选择这个函数是因为我们假设它会让模型很容易学习对相对位置的关注，因为对任意确定的偏移 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">PE_{pos+k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span> 可以表示为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">PE_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>的线性函数。</p><p>此外，我们会将编码器和解码器堆栈中的embedding和位置编码的和再加一个dropout。对于基本模型，我们使用的dropout比例是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>d</mi><mi>r</mi><mi>o</mi><mi>p</mi></mrow></msub><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">P_{drop}=0.1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.1</span></span></span></span>.</p><p>它使用指数函数来生成一系列按位置缩放的值。这些值后续将用于计算正弦和余弦位置编码。让我们逐步解析这段代码： torch.arange(0, d_model, 2): 创建一个从0开始到d_model（不包括）的一维张量，步长为2。 d_model通常是Transformer模型中的一个参数，表示编码的尺寸或“深度”。这一步生成的序列用于计算每个维度的位置编码的频率。 * -(math.log(10000.0) / d_model): math.log(10000.0)计算10000的自然对数。 这个值之后被除以d_model以获得一个缩放因子，然后乘上前面torch.arange生成的序列的每个元素。 由于有一个负号，这意味着符号的翻转。此计算实现了Transformer中位置编码的一个关键部分：随着维度的增加，每个位置的频率按对数级别减少。 torch.exp(...): 对上一步计算的结果应用指数函数。由于输入的每个元素都是负值（因为前面有负号），这会产生一系列在0到1之间递减的值。 这些值在计算正弦和余弦函数时提供了不同的波长，这是Transformer模型中利用位置信息的一种方式。 转换为简单的话来说，这段代码通过在不同频率上为每个位置生成不同的缩放因子，来为Transformer模型中的每个位置编码生成一个序列。 这种基于位置的编码帮助模型理解单词或标记之间的相对或绝对位置关系，这是自然语言处理（NLP）中的一个重要概念。 位置编码与输入嵌入相加后，就可以提供给模型输入的完整表示，包含了单词的语义信息及其在序列中的位置信息。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        <span class="token comment"># 事先准备好max_len=5000的序列的位置编码</span>
        <span class="token comment"># (5000,512)矩阵，一共5000个位置，每个位置用一个512维度向量来表示其位置编码</span>
        pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>
        <span class="token comment"># (5000)-&gt;(5000,1)</span>
        position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># (256)</span>
        div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token operator">-</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 偶数下标的位置,(5000,256)</span>
        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>
        <span class="token comment"># 奇数下标的位置,(5000,256)</span>
        pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span>
        <span class="token comment"># (5000, 512) -&gt; (1, 5000, 512) 为batch.size留出位置</span>
        pe <span class="token operator">=</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">&quot;pe&quot;</span><span class="token punctuation">,</span> pe<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        :param x: Embeddings的词嵌入结果
        :return: Embeddings结果+位置编码结果
        注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数
        x.size(1)是src.seq.len, 从准备好的5000位置中截取序列长度的大小
        在具体相加的时候，会扩展(1,src.seq.len,512)为(batch.size,src.seq.len,512)
        每个batch中的序列，都使用一样的位置编码
        &quot;&quot;&quot;</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-9-generator-生成器" tabindex="-1"><a class="header-anchor" href="#_6-9-generator-生成器" aria-hidden="true">#</a> 6.9. Generator（生成器）</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;Define standard linear + softmax generation step.&quot;
    &quot;定义标准的 linear + softmax 生成步骤。
    使用普通的线性变换和softmax函数将解码器输出转换为预测的下一个token的概率
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span> <span class="token comment"># 线性投影层，将输入维度转换为词汇表（vocab）大小</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 对线性投影结果应用log_softmax函数，进行归一化和概率计算</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>log_softmax函数就是对softmax函数的每个元素求对数操作，也是为了概率归一化的操作，常用于神经网络中，尤其是注意力机制中概率得分的计算。由于 softmax 得到的概率分布和 log_softmax 得到的概率分布可以相加减，因此将原始分数转换为对数形式可以避免溢出，并且更容易计算概率乘法。LogSoftmax相对于Softmax的优势在于对数运算时求导更容易，加快了反向传播的速度。解决Softmax可能存在的上溢和下溢的问题。</p><h3 id="_6-10-clones-克隆" tabindex="-1"><a class="header-anchor" href="#_6-10-clones-克隆" aria-hidden="true">#</a> 6.10. clones（克隆）</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">clones</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Produce N identical layers.&quot;</span>
    <span class="token string">&quot;复制n个相同的层&quot;</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-11-layernorm-层归一化" tabindex="-1"><a class="header-anchor" href="#_6-11-layernorm-层归一化" aria-hidden="true">#</a> 6.11. LayerNorm（层归一化）</h3><p>这段代码实现了一个简单的 Layer Normalization 模块：在 init 方法中，它接受一个整数 features 和一个小的常数 eps (eps=1e-6)作为参数。它首先调用 nn.Module 的构造函数来初始化父类。然后，它创建了两个可学习的参数 a_2 和 b_2，分别用于缩放和平移操作。这些参数的形状为 (features,)，其中 features 是输入张量的最后一个维度的大小。最后，它将 eps 存储在实例变量 self.eps 中。在 forward 方法中，它接受输入张量 x 作为参数。它首先计算 x 在最后一个维度上的均值 mean 和标准差 std。然后，它使用这些统计量对 x 进行 Layer Normalization，即将 x 减去均值并除以标准差。最后，它将缩放参数 a_2 乘以归一化后的结果，并加上平移参数 b_2，得到最终的输出。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Construct a layernorm module (See citation for details).&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># features是一个整数</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>a_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        mean <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        std <span class="token operator">=</span> x<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>a_2 <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>std <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b_2
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-12-sublayerconnection-子层连接" tabindex="-1"><a class="header-anchor" href="#_6-12-sublayerconnection-子层连接" aria-hidden="true">#</a> 6.12. SublayerConnection（子层连接）</h3><p>每个子层的输出是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">L</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">N</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">m</mi></mrow><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">LayerNorm</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Sublayer</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span>, 其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{Sublayer}(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Sublayer</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> 是子层本身实现的函数。我们将 dropout（Dropout: A Simple Way to Prevent Neural Networks from Overfitting，The key idea is to randomly drop units (along with their connections) from the neural network during training.） 应用于每个子层的输出，然后再将其添加到子层输入中并进行归一化。 为了便于进行残差连接（residual connections），模型中的所有子层以及embedding层产生的输出的维度都为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d_{\text{model}}=512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>.</p><blockquote><p>在残差连接中，输入通过一个跨层连接（shortcut connection）直接添加到层的dropout后的输出上(x + self.dropout(sublayer(self.norm(x))))。为了保持维度一致性，可以在残差连接中应用 Layer Normalization。这样可以确保残差连接中的输入和输出都经过了相同的归一化处理，从而提高网络的训练效果和性能。</p></blockquote><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SublayerConnection</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.

    一个残差连接（residual connection）后跟一个层归一化（LayerNorm）
    为了代码的简洁性，将层归一化放在残差连接之前
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># size=d_model=512; dropout=0.1</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> sublayer<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        Apply residual connection to any sublayer with the same size.
        应用残差连接和层归一化，返回最终的输出，将残差连接应用于具有相同大小的任何子层。
        x (batch.size, seq.len, 512)
        sublayer 是一个具体的 MultiHeadAttention 或者 Position-wise FeedForward 对象的结果

        x (batch.size, seq.len, 512) -&gt; norm (LayerNorm) -&gt; (batch.size, seq.len, 512)
        -&gt; sublayer (MultiHeadAttention or PositionwiseFeedForward, 子层传入的x是层归一化后的x)
        -&gt; (batch.size, seq.len, 512) -&gt; dropout -&gt; (batch.size, seq.len, 512)

        然后输入的x（没有走sublayer) + 上面的结果，即实现了残差相加的功能
        &quot;&quot;&quot;</span>
        <span class="token keyword">return</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>sublayer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_6-13-模型使用的例子" tabindex="-1"><a class="header-anchor" href="#_6-13-模型使用的例子" aria-hidden="true">#</a> 6.13. 模型使用的例子</h3><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">&#39;__main__&#39;</span><span class="token punctuation">:</span>
    flag <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">if</span> flag <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token comment"># 假设英语词汇表大小为10000，中文词汇表大小为15000</span>
        src_vocab_size <span class="token operator">=</span> <span class="token number">10000</span>
        tgt_vocab_size <span class="token operator">=</span> <span class="token number">15000</span>

        src <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 创建输入序列张量</span>
        src_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment"># 创建输入序列的掩码张量</span>
        ys <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">)</span>  <span class="token comment"># 创建初始输出序列张量</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>ys<span class="token punctuation">)</span>
        test_model <span class="token operator">=</span> make_model<span class="token punctuation">(</span>src_vocab_size<span class="token punctuation">,</span> tgt_vocab_size<span class="token punctuation">)</span> <span class="token comment"># 创建一个模型实例</span>
        test_model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 将模型设置为评估模式</span>
        memory <span class="token operator">=</span> test_model<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>src<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span> <span class="token comment"># 编码输入序列，得到记忆张量</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;src:</span><span class="token interpolation"><span class="token punctuation">{</span>src<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;memory:</span><span class="token interpolation"><span class="token punctuation">{</span>memory<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;ys:</span><span class="token interpolation"><span class="token punctuation">{</span>ys<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>ys<span class="token punctuation">)</span>
            t <span class="token operator">=</span> subsequent_mask<span class="token punctuation">(</span>ys<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
            out <span class="token operator">=</span> test_model<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>
                memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> ys<span class="token punctuation">,</span> subsequent_mask<span class="token punctuation">(</span>ys<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
                <span class="token punctuation">)</span>  <span class="token comment"># 解码输出序列</span>
            t <span class="token operator">=</span> out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;out:</span><span class="token interpolation"><span class="token punctuation">{</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">,generator的入参：</span><span class="token interpolation"><span class="token punctuation">{</span>t<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
            prob <span class="token operator">=</span> test_model<span class="token punctuation">.</span>generator<span class="token punctuation">(</span>out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 通过生成器获得下一个词的概率分布</span>
            _<span class="token punctuation">,</span> next_word <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>prob<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 选择概率最高的下一个词</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;最高概率的下一个词:&quot;</span><span class="token punctuation">,</span> next_word<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 打印最高概率的下一个词的索引</span>
            next_word <span class="token operator">=</span> next_word<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 获取下一个词的索引</span>
            ys <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                    <span class="token punctuation">[</span>ys<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>src<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>next_word<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span>
                <span class="token punctuation">)</span>  <span class="token comment"># 将下一个词添加到输出序列中</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;未经训练的预测结果:&quot;</span><span class="token punctuation">,</span> ys<span class="token punctuation">)</span>
    <span class="token keyword">elif</span> flag <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        V <span class="token operator">=</span> <span class="token number">11</span>
        batch_size <span class="token operator">=</span> <span class="token number">80</span>
        data_iter <span class="token operator">=</span> data_gen<span class="token punctuation">(</span>V<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>

        model <span class="token operator">=</span> make_model<span class="token punctuation">(</span>V<span class="token punctuation">,</span> V<span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> batch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 使用模型进行前向传播</span>
            out <span class="token operator">=</span> model<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>
                batch<span class="token punctuation">.</span>src<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>tgt<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>src_mask<span class="token punctuation">,</span> batch<span class="token punctuation">.</span>tgt_mask
            <span class="token punctuation">)</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span>out<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">subsequent_mask</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    Mask out subsequent positions. 
    屏蔽后续位置的注意力
    在训练时将当前单词的未来信息屏蔽掉，阻止此单词关注到后面的单词。

    防止在当前位置关注到后面的位置。这种掩码结合将输出embedding偏移一个位置，确保对位置i的预测只依赖位置i之前的已知输出。
    &quot;&quot;&quot;</span>
    <span class="token comment"># 注意力矩阵的形状</span>
    attn_shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> size<span class="token punctuation">,</span> size<span class="token punctuation">)</span>
    <span class="token comment"># 创建上三角矩阵</span>
    subsequent_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>triu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>attn_shape<span class="token punctuation">)</span><span class="token punctuation">,</span> diagonal<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>
        torch<span class="token punctuation">.</span>uint8
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> subsequent_mask <span class="token operator">==</span> <span class="token number">0</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">Batch</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    批处理和掩码
    用于在训练过程中保存一个数据批次及其掩码的对象。
    &quot;&quot;&quot;</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> tgt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> pad<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 2 = &lt;blank&gt;，用于指定填充标记的索引</span>
        <span class="token comment"># src: 源语言序列，(batch.size, src.seq.len)</span>
        <span class="token comment"># 二维tensor，第一维度是batch.size；第二个维度是源语言句子的长度</span>
        <span class="token comment"># 例如：[ [2,1,3,4], [2,3,1,4] ]这样的二行四列的，</span>
        <span class="token comment"># 1-4代表每个单词word的id</span>
        
        <span class="token comment"># tgt: 目标语言序列，默认为空，其shape和src类似</span>
        <span class="token comment"># (batch.size, tgt.seq.len)，</span>
        <span class="token comment"># 二维tensor，第一维度是batch.size；第二个维度是目标语言句子的长度</span>
        <span class="token comment"># 例如tgt=[ [2,1,3,4], [2,3,1,4] ] for a &quot;copy network&quot;</span>
        <span class="token comment"># (输出序列和输入序列完全相同）</span>
        
        <span class="token comment"># pad: 源语言和目标语言统一使用的 位置填充符号，&#39;&lt;blank&gt;&#39;</span>
        <span class="token comment"># 所对应的id，这里默认为0</span>
        <span class="token comment"># 例如，如果一个source sequence，长度不到4，则在右边补0</span>
        <span class="token comment"># [1,2] -&gt; [1,2,0,0]</span>
        self<span class="token punctuation">.</span>src <span class="token operator">=</span> src  <span class="token comment"># 源序列张量</span>
        <span class="token comment"># 在注意力机制的计算中，这个掩码可以用来确保模型不会考虑到填充位置的元素，</span>
        <span class="token comment"># 即通过让填充位置的注意力权重接近于零或实际上为零。</span>
        <span class="token comment"># 这样处理，模型就只会在非填充的元素上计算注意力，从而避免填充值干扰模型理解和处理序列的能力。</span>
        self<span class="token punctuation">.</span>src_mask <span class="token operator">=</span> <span class="token punctuation">(</span>src <span class="token operator">!=</span> pad<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 源序列的掩码张量，用于遮盖填充位置</span>
        <span class="token comment"># src = (batch.size, seq.len) -&gt; != pad -&gt; </span>
        <span class="token comment"># (batch.size, seq.len) -&gt; usnqueeze -&gt;</span>
        <span class="token comment"># (batch.size, 1, seq.len) 相当于在倒数第二个维度扩展</span>
        <span class="token comment"># e.g., src=[ [2,1,3,4], [2,3,1,4] ]对应的是</span>
        <span class="token comment"># src_mask=[ [[1,1,1,1], [1,1,1,1]] ]</span>
        <span class="token keyword">if</span> tgt <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>tgt <span class="token operator">=</span> tgt<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># 目标序列张量，去除最后一个位置的标记</span>
            <span class="token comment"># tgt 相当于目标序列的前N-1个单词的序列</span>
            <span class="token comment">#（去掉了最后一个词）</span>
            self<span class="token punctuation">.</span>tgt_y <span class="token operator">=</span> tgt<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># 目标序列张量的下一个位置的标记</span>
            <span class="token comment"># tgt_y 相当于目标序列的后N-1个单词的序列</span>
            <span class="token comment"># (去掉了第一个词）</span>
            <span class="token comment"># 目的是(src + tgt) 来预测出来(tgt_y)，</span>
            self<span class="token punctuation">.</span>tgt_mask <span class="token operator">=</span> self<span class="token punctuation">.</span>make_std_mask<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tgt<span class="token punctuation">,</span> pad<span class="token punctuation">)</span>  <span class="token comment"># 目标序列的掩码张量，用于遮盖填充位置和未来位置</span>
            self<span class="token punctuation">.</span>ntokens <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>tgt_y <span class="token operator">!=</span> pad<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 目标序列中非填充标记的数量</span>

    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">make_std_mask</span><span class="token punctuation">(</span>tgt<span class="token punctuation">,</span> pad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token string">&quot;创建一个掩码，用于隐藏填充位置和未来的词。&quot;</span>
        <span class="token comment"># 这里的tgt类似于：</span>
        <span class="token comment">#[ [2,1,3], [2,3,1] ] （最初的输入目标序列，分别去掉了最后一个词</span>
        <span class="token comment"># pad=0, &#39;&lt;blank&gt;&#39;的id编号</span>
        tgt_mask <span class="token operator">=</span> <span class="token punctuation">(</span>tgt <span class="token operator">!=</span> pad<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 创建一个掩码张量，用于遮盖填充位置</span>
        <span class="token comment"># 得到的tgt_mask类似于</span>
        <span class="token comment"># tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)</span>
        <span class="token comment"># shape=(2,1,3)</span>
        tgt_mask <span class="token operator">=</span> tgt_mask <span class="token operator">&amp;</span> subsequent_mask<span class="token punctuation">(</span>tgt<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>
            tgt_mask<span class="token punctuation">.</span>data
        <span class="token punctuation">)</span>  <span class="token comment"># 与一个用于遮盖未来位置的掩码张量相与，以得到最终的目标序列掩码张量</span>
        <span class="token comment"># 先看subsequent_mask, 其输入的是tgt.size(-1)=3</span>
        <span class="token comment"># 这个函数的输出为= tensor([[[1, 0, 0],</span>
        <span class="token comment"># [1, 1, 0],</span>
        <span class="token comment"># [1, 1, 1]]], dtype=torch.uint8)</span>
        <span class="token comment"># type_as 把这个tensor转成tgt_mask.data的type(也是torch.uint8)</span>
        
        <span class="token comment"># 这样的话，&amp;的两边的tensor形状分别是(2,1,3) &amp; (1,3,3) 进行与操作之后得到形状（2，3，3）的结果</span>
        <span class="token comment">#tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)</span>
        <span class="token comment">#and</span>
        <span class="token comment"># tensor([[[1, 0, 0], [1, 1, 0], [1, 1, 1]]], dtype=torch.uint8)</span>
        
        <span class="token comment"># 形状(2,3,3)就是得到的tensor</span>
        <span class="token comment"># tgt_mask.data = tensor([[[1, 0, 0],</span>
        <span class="token comment"># [1, 1, 0],</span>
        <span class="token comment"># [1, 1, 1]],</span>

        <span class="token comment">#[[1, 0, 0],</span>
        <span class="token comment"># [1, 1, 0],</span>
        <span class="token comment"># [1, 1, 1]]], dtype=torch.uint8)</span>
        <span class="token keyword">return</span> tgt_mask
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">data_gen</span><span class="token punctuation">(</span>V<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> nbatches<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token string">&quot;Generate random data for a src-tgt copy task. 合成数据&quot;</span>
    <span class="token triple-quoted-string string">&quot;&quot;&quot;
    尝试一个简单的复制任务开始。给定来自小词汇表的一组随机输入符号symbols，目标是生成这些相同的符号
    &quot;&quot;&quot;</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>nbatches<span class="token punctuation">)</span><span class="token punctuation">:</span>
        data <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> V<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
        src <span class="token operator">=</span> data<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        tgt <span class="token operator">=</span> data<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">yield</span> Batch<span class="token punctuation">(</span>src<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="streamlit构建对话式应用程序" class="vp-link nav-link prev nav-link prev" href="/pinkpig/llm/llm/streamlit.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>streamlit构建对话式应用程序</div></a><a aria-label="一文带你了解LangChain: 使用大语言模型构建强大的应用程序" class="vp-link nav-link next nav-link next" href="/pinkpig/llm/llm/langchain.html"><div class="hint">Next<span class="arrow end"></span></div><div class="link">一文带你了解LangChain: 使用大语言模型构建强大的应用程序<span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span></div></a></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/pinkpig/assets/app-MeCKi1WJ.js" defer></script>
  </body>
</html>
