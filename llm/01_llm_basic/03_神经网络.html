<!doctype html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.13" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://liz-in-tech.github.io/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><meta property="og:site_name" content="Liz"><meta property="og:title" content="神经网络"><meta property="og:description" content="神经网络 1. 欠拟合与过拟合 欠拟合（Underfitting）：模型在训练数据上表现不佳，无法很好捕捉到数据的特征和模式的现象 模型过于简单 高偏差（high bias） 过拟合（Overfitting）：模型在训练数据上表现很好，但在未见过的测试数据上表现较差的现象（泛化能力差， 记忆了训练数据） 模型过于复杂 通常发生在模型过于复杂或者训练数据较少的情况下 解决方式 数据增强（Data Augmentation）：可以对数据进行扩增，如旋转、缩放、翻转等，从而增加训练数据的多样性，减少模型对于训练数据的过拟合 提前停止（Early Stopping）：在训练过程中监控模型在验证数据上的性能，当性能开始下降时，提前停止训练，避免过拟合 正则化（Regularization）：可以通过在模型的损失函数中引入正则化项来限制模型的参数值，从而减少模型的复杂度，防止过拟合"><meta property="og:type" content="article"><meta property="og:locale" content="en-US"><meta property="og:updated_time" content="2025-05-21T08:52:56.000Z"><meta property="article:author" content="Liz"><meta property="article:modified_time" content="2025-05-21T08:52:56.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"神经网络","image":[""],"dateModified":"2025-05-21T08:52:56.000Z","author":[{"@type":"Person","name":"Liz","url":"https://github.com/liz-in-tech"}]}</script><link rel="icon" herf="/blogger.png"><link rel="icon" href="/pinkpig/blogger.png"><title>神经网络 | Liz</title><meta name="description" content="神经网络 1. 欠拟合与过拟合 欠拟合（Underfitting）：模型在训练数据上表现不佳，无法很好捕捉到数据的特征和模式的现象 模型过于简单 高偏差（high bias） 过拟合（Overfitting）：模型在训练数据上表现很好，但在未见过的测试数据上表现较差的现象（泛化能力差， 记忆了训练数据） 模型过于复杂 通常发生在模型过于复杂或者训练数据较少的情况下 解决方式 数据增强（Data Augmentation）：可以对数据进行扩增，如旋转、缩放、翻转等，从而增加训练数据的多样性，减少模型对于训练数据的过拟合 提前停止（Early Stopping）：在训练过程中监控模型在验证数据上的性能，当性能开始下降时，提前停止训练，避免过拟合 正则化（Regularization）：可以通过在模型的损失函数中引入正则化项来限制模型的参数值，从而减少模型的复杂度，防止过拟合">
    <link rel="preload" href="/pinkpig/assets/style-B4ayxgRu.css" as="style"><link rel="stylesheet" href="/pinkpig/assets/style-B4ayxgRu.css">
    <link rel="modulepreload" href="/pinkpig/assets/app-5kh03Iqc.js"><link rel="modulepreload" href="/pinkpig/assets/03_神经网络.html-5b4toD6Z.js"><link rel="modulepreload" href="/pinkpig/assets/03_神经网络.html-ftbqULs1.js"><link rel="modulepreload" href="/pinkpig/assets/plugin-vue_export-helper-x3n3nnut.js">
    <link rel="prefetch" href="/pinkpig/assets/index.html-ibkdGA84.js" as="script"><link rel="prefetch" href="/pinkpig/assets/intro.html-VEF0ZusF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSAPP.html-DfrX1UAP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Netty.html-p8gk6fJ_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/RPC.html-7C88YC2N.js" as="script"><link rel="prefetch" href="/pinkpig/assets/competition.html-9FBFPDQz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/操作系统.html-1QgCNUH3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/浏览器技能.html-uFcrAJVS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/网络.html-36AG3eKy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/计算机技能.html-H-ZX4k04.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AntDesign.html-5NgDnkhT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSS.html-DWW8u9ge.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Expo.html-lJQuUz4Q.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Frontend.html-a7NJdFWI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/HTML.html-9wodgV5c.js" as="script"><link rel="prefetch" href="/pinkpig/assets/JavaScript.html-lvtDI2LR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Practice.html-lui81Yar.js" as="script"><link rel="prefetch" href="/pinkpig/assets/React.html-b8iVrI2O.js" as="script"><link rel="prefetch" href="/pinkpig/assets/npm.html-cf-i0MG_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/american_common.html-lo1Rxh7y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/commen_mistakes.html-mGgYvOuL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/grammar.html-H-3z-vXH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english3.html--aWr1mmm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english_detail.html-MQS8h2D6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/pronunciation.html-d7en6ro-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/reading.html--dBHGvtI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/sentence_pattern_and_expression.html-bv8MyCdJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/shadow.html-VczaNCfd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/00_llm_roadmap.html-bnnJVWew.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Git使用手册.html-2FKeMxuw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Markdown.html-8ZdG81kj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/photoshop.html-1AQeTxWq.js" as="script"><link rel="prefetch" href="/pinkpig/assets/python算法刷题语法快速恢复.html-CDW1KE6V.js" as="script"><link rel="prefetch" href="/pinkpig/assets/做题经验总结.html-TQb9JHor.js" as="script"><link rel="prefetch" href="/pinkpig/assets/快速恢复30题.html-dhq8wt62.js" as="script"><link rel="prefetch" href="/pinkpig/assets/快速恢复30题思路大纲.html-EW-3hIp0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/算法提升.html-3eGbfge7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/经典题汇总（每个细分类限定10题以内）.html-7Y8Rh4OG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java8学习笔记.html-gBdmNduj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/基础.html-1_ocEQ4I.js" as="script"><link rel="prefetch" href="/pinkpig/assets/集合.html-B3Wp3Upx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/juc.html-5GKFP4UE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/jvm.html-6rKN8Wso.js" as="script"><link rel="prefetch" href="/pinkpig/assets/spring.html-s5U1LwF9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Keymap.html--yTwT5Pe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Problem_and_plugin.html-fhQh8RqL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Maven--java包管理工具.html-scnAhanr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/careers.html-yiJQBxx-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/common.html-8eP_H2he.js" as="script"><link rel="prefetch" href="/pinkpig/assets/communication.html-Ws1GaxQj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computers.html-E9ZVn5M0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/describing_something.html-qB30C2Nz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/dreams.html-KfGof6S4.js" as="script"><link rel="prefetch" href="/pinkpig/assets/graduating.html-P8cgR_ex.js" as="script"><link rel="prefetch" href="/pinkpig/assets/greetings.html-5eDIKNUU.js" as="script"><link rel="prefetch" href="/pinkpig/assets/hobbies.html-j7YJS0Uj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/immigration.html-N3b_zhs7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/introducing_someone.html-8g3aWQlk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/phone.html-83IoFm_-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/routine.html-HbuB0pac.js" as="script"><link rel="prefetch" href="/pinkpig/assets/time_and_weather.html-iKp7YvRN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/traits.html-2kgfa3hS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_llm_evolution.html-yia_gfta.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_emergent_abilities_and_scaling_law.html-DVn6qo4L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_神经网络1.html-CjsHq7_f.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_rnn.html-H_M06n55.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_attention.html-MApSKzbp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_nlp.html-K0tT5FAc.js" as="script"><link rel="prefetch" href="/pinkpig/assets/07_cnn.html-AYCo06nu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_编码器与解码器.html-sqKHqcqy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_gpt.html-Za3C2MCe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_llama.html-31qpWODi.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_qwen.html-j6_AwMpp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_deepseek.html-B3W1djTi.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_LLM关键技术概览.html-anKaf6G3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_LLM构建过程.html-mPzu15NJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_预训练.html-DqQfxNWb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_微调.html-tnqPShz0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_对齐.html-6tM9Mu-J.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_LLM数据工程.html-bgQHo46E.js" as="script"><link rel="prefetch" href="/pinkpig/assets/07_tokenizer.html-ybjzGM0K.js" as="script"><link rel="prefetch" href="/pinkpig/assets/08_word_embedding.html-NkIVwupR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/09_分布式训练框架.html-f_4CE-ce.js" as="script"><link rel="prefetch" href="/pinkpig/assets/10_ray.html-snaCKVPs.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_解码策略.html--PzsIBtw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_LLM推理.html-UiLUY0jo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_推理框架.html-Qt3AIVVU.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_LLM评估.html-lmUpBwjN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_优化技术.html-qepFvgp1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_LLM压缩.html-u6JYyz3R.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AI_resources.html-ER0hIuhg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/cursor.html-SPLucZvd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/frequent_use_command.html-kuEEE1V7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/google_colab.html-NsrJLy3H.js" as="script"><link rel="prefetch" href="/pinkpig/assets/ipdb.html-lNneRT7f.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llm显存占用计算以及GPU的选择.html-tYtb1LPm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/prompt_experience.html-RjAspTy1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/不传之秘.html-yim4NsOO.js" as="script"><link rel="prefetch" href="/pinkpig/assets/GraphQL.html-zq9chJmn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MicroService.html-gqQ9Z22K.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MybatisPlus.html-sXrDLPET.js" as="script"><link rel="prefetch" href="/pinkpig/assets/rocketmq.html-Hl75cf25.js" as="script"><link rel="prefetch" href="/pinkpig/assets/SQL.html-h6ZRnzFD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/mysql.html-w2oyLSBD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/redis.html-RgiRoRd9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Docker.html-v652K6De.js" as="script"><link rel="prefetch" href="/pinkpig/assets/K8S.html-hVxZM-yZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/docker_from_xmind.html-etYvtf7i.js" as="script"><link rel="prefetch" href="/pinkpig/assets/linux.html-GKFxuA1k.js" as="script"><link rel="prefetch" href="/pinkpig/assets/linux_command.html-iB7MbZV8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/wsl.html-oi6sho5k.js" as="script"><link rel="prefetch" href="/pinkpig/assets/english_words.html-QD9WjIVg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/experience.html-NGzTOw4o.js" as="script"><link rel="prefetch" href="/pinkpig/assets/paper_find.html--xIE94bO.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_python_environment.html-Ogc-g3Pl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_python_data_type.html-SsgcwWnj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_python_operator.html-WW78TP0B.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_python_method.html-gy5ccBYv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_python_builtin_module.html-W4QAcEOF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_python_popular_package.html-4W72Pdg1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/07_python_web.html-2wx4z3ck.js" as="script"><link rel="prefetch" href="/pinkpig/assets/00_deep_learning_frameworks.html-6aphJD4D.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_ai_concept.html-jerJY8kp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_neural_net_train.html-YPETtbT8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_pytorch_operation.html-yukjJeR7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_pytorch_practice_nn.html-6bU2zAkC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_linear_nn.html-V-M3_Tfb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_heterogeneous_graph.html-eXFNwBOH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AI_evolution.html-dPUJTcbb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/0.时空复杂度.html-duqIgH8Z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.分治思想_递归实现.html-ZR8C9fAr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.二进制_位运算.html-IPYNI70m.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.排序.html-SwglYgQW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.二分查找.html-6XjBZtvX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.动态规划_贪心.html-JlAVeSTu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.字符串.html-H_Kw0VHp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.数学.html-JB3Fd046.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.算法技巧.html-0zv2JKDG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.数组.html-2pX8ASkd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.链表.html-MDTUDdof.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.栈.html-VhlnLXDW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.队列.html-5RW94C74.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.堆（优先队列）.html-9UtOuX_z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.树.html-eY_fw8EL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.图.html-C6zmof_l.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.哈希表（散列表）.html-rEshAQ6r.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java语言基础.html-zkZgse3g.js" as="script"><link rel="prefetch" href="/pinkpig/assets/agent.html-NCow9cel.js" as="script"><link rel="prefetch" href="/pinkpig/assets/agent_research.html-pZN9c_1T.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computer_use_same.html-KOaRH6Ml.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain.html-i58jGmH0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/memory.html-ly8uWP03.js" as="script"><link rel="prefetch" href="/pinkpig/assets/prompt.html-lL6RQ9Ux.js" as="script"><link rel="prefetch" href="/pinkpig/assets/018_autorag.html-3i5c7At_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/rag_opensource.html-G7UpFpJ-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/404.html-ELYBSFNe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZtH9hb0Y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ndBqxueE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FhfXXQqd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Y18Gk6ug.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PYFEr-gw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-n6sTAWGG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-SpiN3Ual.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-lySh48SJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-QLcoUYFo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-OkAhqWR6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-WjAYn9Z-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2CIdeSnB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ayy3EF9I.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-zpwaHXIV.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-nGOSO6_S.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-iyn2Phdl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Ls2hwge3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-_e4jAiDs.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-MDiPagDx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-RmQTNmcI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-uTiIdO0F.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-TrYac60Q.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-OWFxg9Im.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PfTXE4w9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--tUsRM8W.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ik3y9HJ8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2LBMckbl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-jSFk7HPD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-aX5SAJZk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-avJpz5gY.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-7kqkDjc1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-11KJ3gwb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-LQuERQvK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-6iizFISA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-7EXy9ft5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-BYKqQoTu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-OUm-Xb__.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Q77JHGwp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-qSCg3dme.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-3AOtZ1O7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-NUrjn_rx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZkjR0Tnl.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-4fJRFXlS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-0Zni0yS2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-x4TH_xzP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-yZYnH68c.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-M3KBctL2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-8WjFejoc.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-KzgtbF_l.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-bUj5_ism.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-xtYkLxjR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-xfwIgr3s.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-AVn-_fFT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-V6IelPdJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-wvgAZYvE.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Bp62bPd5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FTMRL3lu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Ob7dvR-M.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2oWEcuGm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Di6h8Ky1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-5GuLlprQ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dbwd_1YG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dQ2QFe76.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-DKIbv2Jr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-kmPKSOrX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-PjKCTcgX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-WHhL-UfW.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FHbtXQBu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-j0J10-eI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-43MvekNo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/intro.html-Lz1tvhlg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSAPP.html-FYvNKAb6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Netty.html-ksIzIZN-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/RPC.html-v88DvVwB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/competition.html-tQyN3blG.js" as="script"><link rel="prefetch" href="/pinkpig/assets/操作系统.html-v1ATdXda.js" as="script"><link rel="prefetch" href="/pinkpig/assets/浏览器技能.html-N3HuX4yj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/网络.html-eMn1F6a9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/计算机技能.html-74VymBN0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AntDesign.html-sEouzod9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/CSS.html-xwykh7E_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Expo.html-ZisuC_Wn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Frontend.html-IQTlmH9M.js" as="script"><link rel="prefetch" href="/pinkpig/assets/HTML.html-eh4nO5Ux.js" as="script"><link rel="prefetch" href="/pinkpig/assets/JavaScript.html-qxsQvr-l.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Practice.html-L39oRBEj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/React.html-vm5N1cGw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/npm.html-upykG0Jg.js" as="script"><link rel="prefetch" href="/pinkpig/assets/american_common.html-SVEOh-A6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/commen_mistakes.html-l-pFaZyk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/grammar.html-bp3CnfV0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english3.html-t7mTEzM2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/new_concept_english_detail.html-Wz1WVDsY.js" as="script"><link rel="prefetch" href="/pinkpig/assets/pronunciation.html-BmUU3mXf.js" as="script"><link rel="prefetch" href="/pinkpig/assets/reading.html-wwiUZ_vb.js" as="script"><link rel="prefetch" href="/pinkpig/assets/sentence_pattern_and_expression.html-XiB7uI_h.js" as="script"><link rel="prefetch" href="/pinkpig/assets/shadow.html-OkXTYWFH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/00_llm_roadmap.html-ejOWJst8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Git使用手册.html-ysAs0MPt.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Markdown.html-B2K1UBm5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/photoshop.html-ne9cyXto.js" as="script"><link rel="prefetch" href="/pinkpig/assets/python算法刷题语法快速恢复.html-lKWiPvy3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/做题经验总结.html-G20_Tt9n.js" as="script"><link rel="prefetch" href="/pinkpig/assets/快速恢复30题.html-eycgTwdy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/快速恢复30题思路大纲.html-SGmGnMZd.js" as="script"><link rel="prefetch" href="/pinkpig/assets/算法提升.html-JMIIXSSj.js" as="script"><link rel="prefetch" href="/pinkpig/assets/经典题汇总（每个细分类限定10题以内）.html-YNQVMZzo.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java8学习笔记.html-zr3ssenN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/基础.html-M3WgZU1L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/集合.html-kbzqjc-7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/juc.html-o_k8nHXI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/jvm.html-pNjLZv2y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/spring.html-g2T5BQuw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Keymap.html-QpuZfOuI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/IDEA_Problem_and_plugin.html--EtJtxf0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Maven--java包管理工具.html-khAbpD4z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/careers.html-K6QlUmeA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/common.html-pOa33DWR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/communication.html-bBbB6OEU.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computers.html-lA-rWAel.js" as="script"><link rel="prefetch" href="/pinkpig/assets/describing_something.html-5W7W9r34.js" as="script"><link rel="prefetch" href="/pinkpig/assets/dreams.html-zfOnwzkI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/graduating.html-tbFPJwkX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/greetings.html-fgSsA3Ak.js" as="script"><link rel="prefetch" href="/pinkpig/assets/hobbies.html-lJqvXWnx.js" as="script"><link rel="prefetch" href="/pinkpig/assets/immigration.html-DevaLb5L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/introducing_someone.html-KeIHVr4A.js" as="script"><link rel="prefetch" href="/pinkpig/assets/phone.html-fqp9wxLw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/routine.html-Pm3_wvtI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/time_and_weather.html-wMSlMnY7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/traits.html-gv_2gW2g.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_llm_evolution.html-JCdCDekL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_emergent_abilities_and_scaling_law.html-AjHwc5oV.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_神经网络1.html-YBoKObFm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_rnn.html-fd7w_Iyz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_attention.html-YzHHpYFF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_nlp.html-Cb5S4QKD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/07_cnn.html-Wq1wiKr5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_编码器与解码器.html--mSLm6An.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_gpt.html-ZIiEDOhT.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_llama.html-SbH5PDTm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_qwen.html-0bvruicH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_deepseek.html--sIDrQxV.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_LLM关键技术概览.html-sVuNiUB0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_LLM构建过程.html-h0RYZnZh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_预训练.html-QWuwDd9g.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_微调.html-VoXQBeST.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_对齐.html-yY5k9jne.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_LLM数据工程.html-CsPot76S.js" as="script"><link rel="prefetch" href="/pinkpig/assets/07_tokenizer.html-9M-sS949.js" as="script"><link rel="prefetch" href="/pinkpig/assets/08_word_embedding.html-sugwHC0O.js" as="script"><link rel="prefetch" href="/pinkpig/assets/09_分布式训练框架.html-dVLcu60c.js" as="script"><link rel="prefetch" href="/pinkpig/assets/10_ray.html-SjxmeXtZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_解码策略.html-ogNLqzz8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_LLM推理.html-HskXxAkC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_推理框架.html-G6Q4AvOk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_LLM评估.html-VOovO5hr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_优化技术.html-7Zg4dTlP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_LLM压缩.html-fQLNQ26Z.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AI_resources.html-cyMO1w-L.js" as="script"><link rel="prefetch" href="/pinkpig/assets/cursor.html-53AF4onC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/frequent_use_command.html-J0OWQoEI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/google_colab.html-wjPfH_3n.js" as="script"><link rel="prefetch" href="/pinkpig/assets/ipdb.html-uA09zd2y.js" as="script"><link rel="prefetch" href="/pinkpig/assets/llm显存占用计算以及GPU的选择.html-tUYfSXo1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/prompt_experience.html-C5E-lsl1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/不传之秘.html-EdH4OjI1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/GraphQL.html-L-d_99FO.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MicroService.html-vfZ5SUs3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/MybatisPlus.html-p6vdkeYm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/rocketmq.html-fVmZW1YX.js" as="script"><link rel="prefetch" href="/pinkpig/assets/SQL.html-vcHoVDc0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/mysql.html-bDmaK-5w.js" as="script"><link rel="prefetch" href="/pinkpig/assets/redis.html-3SU6jo2G.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Docker.html-IHHSc-X5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/K8S.html-fUfvznwr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/docker_from_xmind.html-D4suFzfi.js" as="script"><link rel="prefetch" href="/pinkpig/assets/linux.html-J5r9oN5o.js" as="script"><link rel="prefetch" href="/pinkpig/assets/linux_command.html-Z9BPJ0zz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/wsl.html-2d9tCbEP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/english_words.html-6JIzNuvD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/experience.html-g9nePVPk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/paper_find.html-v-mg-7T0.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_python_environment.html-IJZ7w1xh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_python_data_type.html-XPjY_K09.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_python_operator.html-H9DkRJUe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_python_method.html-Kj0HuG5a.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_python_builtin_module.html-1_B6fkU8.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_python_popular_package.html-7Mm3v2I4.js" as="script"><link rel="prefetch" href="/pinkpig/assets/07_python_web.html-o_Mz6YkM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/00_deep_learning_frameworks.html-HilDRkkH.js" as="script"><link rel="prefetch" href="/pinkpig/assets/01_ai_concept.html-E2k0nU9U.js" as="script"><link rel="prefetch" href="/pinkpig/assets/02_neural_net_train.html-xJs8SdLP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/03_pytorch_operation.html-2bfCAm5-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/04_pytorch_practice_nn.html-k96j2PPP.js" as="script"><link rel="prefetch" href="/pinkpig/assets/05_linear_nn.html-gLR3sVbM.js" as="script"><link rel="prefetch" href="/pinkpig/assets/06_heterogeneous_graph.html-sRkWeTXZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/AI_evolution.html-v3nI0fJ9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/0.时空复杂度.html-9KvliE3W.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.分治思想_递归实现.html-I0gwXA_c.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.二进制_位运算.html-zX18wh24.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.排序.html-_MQ2AEV1.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.二分查找.html-dtpIS_Zr.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.动态规划_贪心.html-8Kvjuw6q.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.字符串.html-UGr6mJCO.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.数学.html-4spzuXpk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.算法技巧.html-VWQtfXe3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/1.数组.html-9n0qZCst.js" as="script"><link rel="prefetch" href="/pinkpig/assets/2.链表.html-jCv9jLNh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/3.栈.html-0-JtY_PD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/4.队列.html-ahuotlAf.js" as="script"><link rel="prefetch" href="/pinkpig/assets/5.堆（优先队列）.html-ZxHDrUhJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/6.树.html-KXmLr-TD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/7.图.html-Z9jKmzWf.js" as="script"><link rel="prefetch" href="/pinkpig/assets/8.哈希表（散列表）.html-d3rq2Mt3.js" as="script"><link rel="prefetch" href="/pinkpig/assets/Java语言基础.html-2oNHmgrD.js" as="script"><link rel="prefetch" href="/pinkpig/assets/agent.html-WDkYnw11.js" as="script"><link rel="prefetch" href="/pinkpig/assets/agent_research.html-b5ROgQOe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/computer_use_same.html-Wq6D6_PL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/langchain.html-IM275jPI.js" as="script"><link rel="prefetch" href="/pinkpig/assets/memory.html-A8pMV0K9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/prompt.html-Qub6W947.js" as="script"><link rel="prefetch" href="/pinkpig/assets/018_autorag.html-hBFkRGnB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/rag_opensource.html-KwOpduSk.js" as="script"><link rel="prefetch" href="/pinkpig/assets/404.html-7abDO_BK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZS6WSkIh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-06v2-Uco.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-MUv4asyJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-O1RM1ffn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-g77aAeDY.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html--QG6cNLB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dWNfXMaF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-W0pouqlp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-BaxzkgSf.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-lEihyvLZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-H4WChx1S.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-D_tixLye.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Yld0CbnB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-LJ630Avv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-AomgKDPu.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-umdNh-wq.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-NnvTvXu2.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-KBzd8aZB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-CbznlTBf.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-NpvAQCkS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2sq2k-12.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-vMzOK5nF.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-zhCLdGjm.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-si3bAKZz.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-H3Hme5Un.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-s2Frdodh.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-OepIoaKe.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-R9A8_rwi.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-1JmKF-_4.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-1sNpDVmN.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2cJrdgEa.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-iZNlxd2j.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-yCqIF83l.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-2WvW6_6T.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Lpv86nWA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Oiy5ECk_.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Gyy_T6Vn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-Lc_lGaPC.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-pZTEuRPy.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-wgv1jU6B.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-SAok6JJZ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-by27jwO7.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-pi_P-4-s.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-MeDpafSV.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZCRkV1_p.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-6XUxBLQq.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-W2URW4xA.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-kMn8DM4F.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-K0LjNKx6.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-0Xz9z5Ae.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-8mwoA9M9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-6PCBnuQ9.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-f0l8RwXJ.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dnpqUjfK.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-H8Zc8901.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-kvR4BVcL.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-I1b49KnS.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-BxOS0CVn.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-N7k99txp.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ZqLY7wAB.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-7aurrHxw.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-FgGkbwbt.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-q0RXIqUR.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-JMLVlLek.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-dhcwFA1-.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-gUWaWBE5.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-ec8sn9Bv.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-BPeO2e3E.js" as="script"><link rel="prefetch" href="/pinkpig/assets/index.html-VITIZh6g.js" as="script"><link rel="prefetch" href="/pinkpig/assets/photoswipe.esm-08_zHRDQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">Skip to main content</a><!--]--><!--[--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/pinkpig/"><img class="vp-nav-logo" src="/pinkpig/blogger.png" alt><!----><span class="vp-site-name hide-in-pad">Liz</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="Language" class="vp-link nav-link nav-link" href="/pinkpig/language/"><!---->Language<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="LLM" class="vp-link nav-link active nav-link active" href="/pinkpig/llm/"><!---->LLM<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Python" class="vp-link nav-link nav-link" href="/pinkpig/python/"><!---->Python<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Java" class="vp-link nav-link nav-link" href="/pinkpig/java/"><!---->Java<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Frontend" class="vp-link nav-link nav-link" href="/pinkpig/frontend/"><!---->Frontend<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Middleware" class="vp-link nav-link nav-link" href="/pinkpig/middleware/"><!---->Middleware<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="CS" class="vp-link nav-link nav-link" href="/pinkpig/cs/"><!---->CS<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Operations" class="vp-link nav-link nav-link" href="/pinkpig/operations/"><!---->Operations<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Tools" class="vp-link nav-link nav-link" href="/pinkpig/tools/"><!---->Tools<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><span class="font-icon icon fa-fw fa-sm fas fa-book" style=""></span><span class="vp-sidebar-title">Llm</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><!----><span class="vp-sidebar-title">01 Llm Basic</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a aria-label="/llm/01_llm_basic/03_神经网络1" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C1.html"><!---->/llm/01_llm_basic/03_神经网络1<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Attention(注意力机制)" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/05_attention.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>Attention(注意力机制)<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="CNN" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/07_cnn.html"><!---->CNN<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="LLM发展历程" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/01_llm_evolution.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>LLM发展历程<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="NLP" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/06_nlp.html"><!---->NLP<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="RNNs" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/04_rnn.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>RNNs<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="两种不同的性能提升理论：涌现能力和扩展法则" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/01_llm_basic/02_emergent_abilities_and_scaling_law.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>两种不同的性能提升理论：涌现能力和扩展法则<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="神经网络" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>神经网络<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="1. 欠拟合与过拟合" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_1-欠拟合与过拟合"><!---->1. 欠拟合与过拟合<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="2. 直观表现" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_2-直观表现"><!---->2. 直观表现<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="3. 分层结构" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_3-分层结构"><!---->3. 分层结构<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="3.1. 层的分类" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_3-1-层的分类"><!---->3.1. 层的分类<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a aria-label="4. MLP,Multiple-Layer Perceiver(多层感知器)" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_4-mlp-multiple-layer-perceiver-多层感知器"><!---->4. MLP,Multiple-Layer Perceiver(多层感知器)<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5. 神经网络本质：整个神经网络，就是一个有成千上万个可调节参数的超级大函数" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_5-神经网络本质-整个神经网络-就是一个有成千上万个可调节参数的超级大函数"><!---->5. 神经网络本质：整个神经网络，就是一个有成千上万个可调节参数的超级大函数<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="5.1. 模型的整体表示" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_5-1-模型的整体表示"><!---->5.1. 模型的整体表示<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5.2. 模型参数 = 权重参数 + 偏置参数" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_5-2-模型参数-权重参数-偏置参数"><!---->5.2. 模型参数 = 权重参数 + 偏置参数<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="5.3. 超参数(Hyperparameters)" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_5-3-超参数-hyperparameters"><!---->5.3. 超参数(Hyperparameters)<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6. 神经网络的训练" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_6-神经网络的训练"><!---->6. 神经网络的训练<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="6.0. LLM 的训练目标" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_6-0-llm-的训练目标"><!---->6.0. LLM 的训练目标<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.1. 前向传播(Forward Propagation)" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_6-1-前向传播-forward-propagation"><!---->6.1. 前向传播(Forward Propagation)<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="6.2. 反向传播(Back Propagation)" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_6-2-反向传播-back-propagation"><!---->6.2. 反向传播(Back Propagation)<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a aria-label="7. 示例" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_7-示例"><!---->7. 示例<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="7.1. 任务" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_7-1-任务"><!---->7.1. 任务<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="7.2. 特征提取" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_7-2-特征提取"><!---->7.2. 特征提取<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="7.3. 神经网络" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_7-3-神经网络"><!---->7.3. 神经网络<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="7.4. 识别的结果" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/pinkpig/llm/01_llm_basic/03_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html#_7-4-识别的结果"><!---->7.4. 识别的结果<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">02 Llm Architecture</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">03 Llm Training</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">04 Llm Reasoning</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">05 Llm Engineering</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><!----><span class="vp-sidebar-title">06 Llm Experience</span><span class="vp-arrow end"></span></button><!----></section></li><li><!--[--><a aria-label="LLM Roadmap" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/pinkpig/llm/00_llm_roadmap.html"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>LLM Roadmap<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>神经网络</h1><div class="page-info"><span class="page-author-info" aria-label="Author🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="https://github.com/liz-in-tech" target="_blank" rel="noopener noreferrer">Liz</a></span><span property="author" content="Liz"></span></span><!----><span class="page-date-info" aria-label="Writing Date📅" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2025-03-29T07:33:01.000Z"></span><!----><span class="page-reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 22 min</span><meta property="timeRequired" content="PT22M"></span><!----><!----></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">On This Page<button type="button" class="print-button" title="Print"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_1-欠拟合与过拟合">1. 欠拟合与过拟合</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_2-直观表现">2. 直观表现</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_3-分层结构">3. 分层结构</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_3-1-层的分类">3.1. 层的分类</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_4-mlp-multiple-layer-perceiver-多层感知器">4. MLP,Multiple-Layer Perceiver(多层感知器)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_5-神经网络本质-整个神经网络-就是一个有成千上万个可调节参数的超级大函数">5. 神经网络本质：整个神经网络，就是一个有成千上万个可调节参数的超级大函数</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-1-模型的整体表示">5.1. 模型的整体表示</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-2-模型参数-权重参数-偏置参数">5.2. 模型参数 = 权重参数 + 偏置参数</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_5-3-超参数-hyperparameters">5.3. 超参数(Hyperparameters)</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_6-神经网络的训练">6. 神经网络的训练</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-0-llm-的训练目标">6.0. LLM 的训练目标</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-1-前向传播-forward-propagation">6.1. 前向传播(Forward Propagation)</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_6-2-反向传播-back-propagation">6.2. 反向传播(Back Propagation)</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#_7-示例">7. 示例</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-1-任务">7.1. 任务</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-2-特征提取">7.2. 特征提取</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-3-神经网络">7.3. 神经网络</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#_7-4-识别的结果">7.4. 识别的结果</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="神经网络" tabindex="-1"><a class="header-anchor" href="#神经网络" aria-hidden="true">#</a> 神经网络</h1><h2 id="_1-欠拟合与过拟合" tabindex="-1"><a class="header-anchor" href="#_1-欠拟合与过拟合" aria-hidden="true">#</a> 1. 欠拟合与过拟合</h2><ul><li>欠拟合（Underfitting）：模型在训练数据上表现不佳，无法很好捕捉到数据的特征和模式的现象 <ul><li>模型过于简单</li><li>高偏差（high bias）</li></ul></li><li>过拟合（Overfitting）：模型在训练数据上表现很好，但在未见过的测试数据上表现较差的现象（泛化能力差， 记忆了训练数据） <ul><li>模型过于复杂</li><li>通常发生在<strong>模型过于复杂</strong>或者<strong>训练数据较少</strong>的情况下</li><li>解决方式 <ul><li>数据增强（Data Augmentation）：可以对数据进行扩增，如旋转、缩放、翻转等，从而增加训练数据的多样性，减少模型对于训练数据的过拟合</li><li>提前停止（Early Stopping）：在训练过程中监控模型在验证数据上的性能，当性能开始下降时，提前停止训练，避免过拟合</li><li>正则化（Regularization）：可以通过在模型的损失函数中引入正则化项来限制模型的参数值，从而减少模型的复杂度，防止过拟合</li></ul></li></ul></li></ul><figure><img src="/pinkpig/assets/image-1-HvUW_6X2.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-2-J_50bzFT.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-8-ZVDh5cJl.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-10-vdXiLVIA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="_2-直观表现" tabindex="-1"><a class="header-anchor" href="#_2-直观表现" aria-hidden="true">#</a> 2. 直观表现</h2><ul><li>是由多个点和线组成的 <ul><li>每一个点: 神经元 <ul><li>每个非输入层的神经元都是一个非线性的激活函数，都有一个偏置参数</li><li>神经元接收多个输入值（通常用向量表示），对输入值进行加权求和；然后通过一个激活函数（Activation Function）进行非线性映射，生成神经元的输出</li></ul></li><li>每一条线: 连接相邻两层的神经元，代表一个权重，形容了连接的强弱</li></ul></li></ul><figure><img src="/pinkpig/assets/nlp_006-97x9TtH-.png" alt="公式" tabindex="0" loading="lazy"><figcaption>公式</figcaption></figure><h2 id="_3-分层结构" tabindex="-1"><a class="header-anchor" href="#_3-分层结构" aria-hidden="true">#</a> 3. 分层结构</h2><h3 id="_3-1-层的分类" tabindex="-1"><a class="header-anchor" href="#_3-1-层的分类" aria-hidden="true">#</a> 3.1. 层的分类</h3><h4 id="_3-1-1-按功能和特点" tabindex="-1"><a class="header-anchor" href="#_3-1-1-按功能和特点" aria-hidden="true">#</a> 3.1.1. 按功能和特点</h4><ul><li>全连接层</li><li>激活层</li><li>规范化层/归一化层</li><li>卷积层</li><li>池化层</li><li>循环层</li><li>嵌入层</li></ul><h5 id="_3-1-1-1-全连接层-fully-connected-layer" tabindex="-1"><a class="header-anchor" href="#_3-1-1-1-全连接层-fully-connected-layer" aria-hidden="true">#</a> 3.1.1.1. 全连接层(Fully Connected Layer)</h5><p>两层之间，每层的每个神经元都和另一层的所有神经元都相连</p><h5 id="_3-1-1-2-激活层" tabindex="-1"><a class="header-anchor" href="#_3-1-1-2-激活层" aria-hidden="true">#</a> 3.1.1.2. 激活层</h5><p>激活层为非线性映射，没有激活函数，无论网络有多深，输出都只是输入的线性组合，这将大大限制网络的表示能力。</p><p>在实践中，ReLU（Rectified Linear Unit）及其变种是最常用的激活函数，因为它们的计算效率高，并且能够在一定程度上缓解梯度消失问题。激活函数通常应用于卷积层的输出上，为网络引入非线性，增加模型的深度和复杂度，使模型能够捕捉更加复杂的特征。</p><ul><li>激活函数：加入激活函数就是给模型引入非线性能力 <ul><li>每个非输入层的神经元是一个激活函数，激活函数本质是一个非线性的函数</li><li>激活函数引入了非线性操作，使得神经网络可以更好地拟合复杂的非线性关系</li><li>通过激活函数的处理，入参和返参的结构没有变，只调整了其中的值</li><li>如果单看一个神经元，激活函数的入参和返参都是一个数；一般同一层的神经元使用同样的激活函数，会进行同一层的神经元作为向量一起计算，那么激活函数的入参和返参就是同样结构为【该层神经元数*1】的列向量</li><li>入参：加权求和的结果</li><li>返参：对入参非线性处理的结果</li><li>选择合适的激活函数对于模型的性能至关重要</li></ul></li><li>常用激活函数（不同的激活函数，特点和作用不同） <ul><li>Sigmoid函数 <ul><li>将输入映射到[0, 1]之间</li><li>常用于二分类问题或者作为输出层的激活函数</li><li>可能会导致梯度消失问题</li></ul></li><li>Tanh函数 <ul><li>将输入映射到[-1, 1]之间</li><li>常用于二分类问题或者作为输出层的激活函数</li><li>可能会导致梯度消失问题</li><li>Sigmoid和tanh的特点是将输出映射在[0,1]和[-1,1]之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门；而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。</li><li>tanh 函数可以由 sigmoid 函数经过平移和拉伸得到，两者函数曲线形状相似</li><li>tanh 函数可以理解为是基于 sigmoid 函数的一种改进的激活函数</li></ul></li><li>Softmax函数 <ul><li>将输入映射为概率分布（输出的向量的每一个值都为正而且元素之和为一）</li><li>常用于多分类问题的输出层</li></ul></li><li>ReLU（Rectified Linear Unit） <ul><li>将负数输入映射为0，保留正数输入</li><li>常用于隐藏层的激活函数</li><li>适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。</li><li>经过实际实验发现，使用 ReLU 作为激活函数，模型收敛的速度比 sigmoid 和 tanh 快</li><li>ReLU 有可能会导致梯度爆炸问题，解决方法是梯度截断</li><li>ReLU的变体：都是为了解决 ReLU 的 Dead ReLU 问题 <ul><li>Leaky ReLU <ul><li>其在z&gt;0的部分与ReLU一样保持不变；在z&lt;0的部分，采用一个非常小的斜率0.01</li><li>但是该方法的缺点是效果并不稳定，所以实际实验中使用该方法的并不多</li></ul></li><li>PReLU <ul><li>可学习参数作为z&lt;0的斜率</li></ul></li><li>RReLU <ul><li>随机数作为z&lt;0的斜率</li></ul></li><li>ELU（指数线性单元）</li></ul></li></ul></li></ul></li></ul><figure><img src="/pinkpig/assets/nlp_008-ku0yQRoE.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h5 id="_3-1-1-3-规范化层-归一化层-特征缩放" tabindex="-1"><a class="header-anchor" href="#_3-1-1-3-规范化层-归一化层-特征缩放" aria-hidden="true">#</a> 3.1.1.3. 规范化层/归一化层 (特征缩放)</h5><p>是深度学习中常用的技术，旨在通过调整网络中间层的输出，使其具有稳定的分布。这有助于加快训练速度，提高模型性能，并且还能起到一定的正则化效果。</p><p>能够加速网络的收敛并提升准确</p><p>神经网络在训练过程中，随着深度加深，输入值分布会发生偏移</p><p>归一化将所有特征的值缩放到0-1之间</p><p>把每层神经网络输入值的分布强行拉回到均值为0方差为1的标准正态分布。（纠偏回正过程）</p><p>为什么要进行归一化处理？ 归一化可以减少内部协变量偏移（internal covariate shift），即保证网络各层输入分布的稳定性，从而使模型训练更加稳定和快速。归一化通常在卷积操作和激活函数之间进行。</p><figure><img src="/pinkpig/assets/image-HWfgRL6L.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>归一化算法：</p><ul><li>Batch Normalization (BN)</li><li>Layer Normalization (LN)</li><li>Instance Normalization (IN)</li><li>Group Normalization (GN)</li></ul><h5 id="_3-1-1-4-卷积层" tabindex="-1"><a class="header-anchor" href="#_3-1-1-4-卷积层" aria-hidden="true">#</a> 3.1.1.4. 卷积层</h5><figure><img src="/pinkpig/assets/nlp_009-fpuwjR6i.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>在被卷积矩阵上移动做乘法和加法得到提取后的特征</p><p>因为卷积层能够有效地提取图像特征，并减少模型中参数的数量（相比于全连接层），从而降低过拟合的风险并提高模型的泛化能力。卷积操作保持了图像的空间层次结构，使得网络能够利用图像的局部空间相关性。</p><h5 id="_3-1-1-5-池化层" tabindex="-1"><a class="header-anchor" href="#_3-1-1-5-池化层" aria-hidden="true">#</a> 3.1.1.5. 池化层</h5><p>压缩特征，保留主要特征</p><p>池化操作可以认为是将一张分辨率高的图片转化为分辨率较低的图片</p><p>池化层通常跟随在卷积层之后，其主要作用是进行下采样或降维，减少数据的空间大小，从而减少计算量和参数数量，防止过拟合。</p><p>三种池化策略</p><ul><li>最大池化</li><li>最小池化</li><li>平均池化</li></ul><figure><img src="/pinkpig/assets/nlp_010-uLwipgoU.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_3-1-2-按位置" tabindex="-1"><a class="header-anchor" href="#_3-1-2-按位置" aria-hidden="true">#</a> 3.1.2. 按位置</h4><ul><li>输入层/第一层神经元 <ul><li>模型的输入：能表示原始数据信息的向量</li></ul></li><li>隐层/中间层(Hidden Layer) <ul><li>输入层和输出层之间的神经元集合</li><li>隐层的神经元数量、激活函数的选择以及层次结构的设计等因素都会对神经网络的性能和学习能力产生影响，因此在设计和训练神经网络时，隐层的设置通常需要经验和调优</li></ul></li><li>输出层/最后一层神经元 <ul><li>模型的输出</li></ul></li></ul><h2 id="_4-mlp-multiple-layer-perceiver-多层感知器" tabindex="-1"><a class="header-anchor" href="#_4-mlp-multiple-layer-perceiver-多层感知器" aria-hidden="true">#</a> 4. MLP,Multiple-Layer Perceiver(多层感知器)</h2><ul><li>MLP是最基本的神经网络，属于前馈神经网络（Feedforward Neural Network)。在前馈神经网络中，信息在网络中从输入层（Input Layer）经过多个中间层（Hidden Layers）传递到输出层（Output Layer），并且信息在网络中只能向前传播，不会形成循环。前馈神经网络也被称为无记忆神经网络（Memoryless Neural Network），因为网络中没有存储之前计算的状态。</li><li>优点：具有较强的非线性建模能力；可以进行端到端的学习；可以并行计算</li><li>缺点：容易陷入局部最优；容易过拟合；全连接的结构太过复杂，参数超级多</li></ul><h2 id="_5-神经网络本质-整个神经网络-就是一个有成千上万个可调节参数的超级大函数" tabindex="-1"><a class="header-anchor" href="#_5-神经网络本质-整个神经网络-就是一个有成千上万个可调节参数的超级大函数" aria-hidden="true">#</a> 5. 神经网络本质：整个神经网络，就是一个有成千上万个可调节参数的超级大函数</h2><h3 id="_5-1-模型的整体表示" tabindex="-1"><a class="header-anchor" href="#_5-1-模型的整体表示" aria-hidden="true">#</a> 5.1. 模型的整体表示</h3><p>f(x;<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>)</p><p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>为要更新的参数</p><h3 id="_5-2-模型参数-权重参数-偏置参数" tabindex="-1"><a class="header-anchor" href="#_5-2-模型参数-权重参数-偏置参数" aria-hidden="true">#</a> 5.2. 模型参数 = 权重参数 + 偏置参数</h3><ul><li>权重参数(w, weight) <ul><li>神经网络每一条线代表一个权重，是一个权重参数</li></ul></li><li>偏置参数(b, bias) <ul><li>神经网络中每个非输入层的神经元有一个偏置参数</li></ul></li></ul><p>当我们讨论机器如何学习时，我们其实就是在讨论：电脑如何设置成千上万的模型参数，从而让它能够正确帮我们解决问题</p><h3 id="_5-3-超参数-hyperparameters" tabindex="-1"><a class="header-anchor" href="#_5-3-超参数-hyperparameters" aria-hidden="true">#</a> 5.3. 超参数(Hyperparameters)</h3><p>人为设置的，不属于模型参数的范畴</p><p>是在机器学习算法中需要手动设置的参数，用于控制模型的学习过程和性能。与模型参数不同，超参数的值不能通过训练数据自动学习得到，而需要在训练之前由人工设置。</p><p>超参数的选择会影响模型的性能和训练速度，因此合理的超参数选择对于获得良好的模型性能非常重要。</p><h4 id="_5-3-1-常见的超参数" tabindex="-1"><a class="header-anchor" href="#_5-3-1-常见的超参数" aria-hidden="true">#</a> 5.3.1. 常见的超参数</h4><ul><li>Batch Size(批大小)</li><li>Learning Rate(学习率)</li><li>Epochs(迭代次数)</li><li>正则化(Regularization)</li><li>激活函数</li><li>网络结构（层数、神经元数量等）</li><li>损失函数</li><li>优化算法</li></ul><h4 id="_5-3-2-batch-size-批大小" tabindex="-1"><a class="header-anchor" href="#_5-3-2-batch-size-批大小" aria-hidden="true">#</a> 5.3.2. Batch Size(批大小)</h4><p>一次参数更新中使用的样本数量</p><p>在目前大多数模型的训练过程中，训练数据通常被划分为多个小批次（Batch）进行处理，每个批次包含一定数量的样本。</p><p>批大小的选择会直接影响模型的训练速度和内存占用。</p><ul><li>较大的批大小可以加速训练过程，因为可以利用硬件的并行性进行计算，但可能会占用更多的内存。</li><li>较小的批大小则可以减小内存占用，但可能导致计算效率较低。</li></ul><p>合适的批大小通常需要根据具体的任务和硬件资源进行选择，常见的取值范围通常在几十到几千之间。</p><p>机器学习训练时，Mini-Batch 的大小优选为2的幂，如 64 或 128，原因是？</p><ul><li>答：GPU 对2的幂次的 batch 可以发挥更佳的性能，利于并行化处理</li><li>主要是出于<strong>计算效率</strong>和<strong>内存对齐</strong>的考虑。硬件（如GPU）的内存分配和并行计算单元（如CUDA核心）通常针对2的幂次方优化，能更高效地处理对齐的内存访问和并行任务。此外，2的幂次方有助于优化矩阵运算、内存带宽利用率，并减少线程浪费，从而加速训练过程。这种选择在深度学习框架（如PyTorch、TensorFlow）中已成为常见实践。</li></ul><figure><img src="/pinkpig/assets/image-11-fxDOTicF.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_5-3-3-learning-rate-学习率" tabindex="-1"><a class="header-anchor" href="#_5-3-3-learning-rate-学习率" aria-hidden="true">#</a> 5.3.3. Learning Rate(学习率)</h4><p>学习率用于控制模型在每一轮训练中对权重进行更新的步伐大小。具体来说，学习率决定了在梯度下降优化算法中，每次更新权重时权重的变化量。</p><p>学习率可以是恒定的，可以衰减，也可以自动调整</p><figure><img src="/pinkpig/assets/nlp_014-Gw137pxR.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>图中左边是参数向量，右边是梯度向量，我们是以梯度的倍数去调节左边的参数的，这个倍数其实就是学习率。</p><h4 id="_5-3-4-epochs-迭代次数" tabindex="-1"><a class="header-anchor" href="#_5-3-4-epochs-迭代次数" aria-hidden="true">#</a> 5.3.4. Epochs(迭代次数)</h4><p>表示模型在整个训练数据集上的训练次数</p><p>迭代次数的大小会影响模型的性能</p><ul><li>过少迭代次数可能导致欠拟合</li><li>过多迭代次数可能导致过拟合</li></ul><h4 id="_5-3-5-正则化-regularization" tabindex="-1"><a class="header-anchor" href="#_5-3-5-正则化-regularization" aria-hidden="true">#</a> 5.3.5. 正则化(Regularization)</h4><p>正则化是一种限制模型的参数值，减少模型的复杂度，从而避免模型在训练数据上过拟合的技术。</p><p>正则化方法</p><ul><li>L1 正则化(L1 Regularization，也称为 Lasso 正则化) <ul><li>一句话概括：把某些参数的权重变成0</li><li>L1 正则化可以用于控制神经网络中的参数稀疏性，使得模型权重倾向于稀疏化，从而减小模型复杂性，提高模型泛化能力</li><li>L1会使学到的模型很稀疏，得到稀疏的权重</li></ul></li><li>L2 正则化(L2 Regularization，也称为 Ridge 正则化) <ul><li>一句话概括：抑制模型的一些权重取值过大</li><li>L2 正则化可以用于控制神经网络中的参数值的大小，使得模型权重趋向于较小的值，从而减小模型复杂性，提高模型泛化能力</li><li>L2会使得权重系数取值变得平均，得到平滑的权重</li></ul></li><li>Dropout 正则化 <ul><li>一句话概括：扔掉一些神经元</li><li>Dropout 可以随机地设置某些神经元的输出为零，从而迫使网络在训练过程中不依赖于特定的神经元，从而减小模型复杂性，提高模型泛化能力</li></ul></li></ul><figure><img src="/pinkpig/assets/image-3-qXG_btZj.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-4-EZpqEnrk.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-5-qjc62Dgy.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-6-LnxqkUxO.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-7-7reKjwUi.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="/pinkpig/assets/image-9-ieZNPonk.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>正则化的直观解释：https://mp.weixin.qq.com/s?__biz=MzIwOTc2MTUyMg==&amp;mid=2247484510&amp;idx=1&amp;sn=d7df438e67e460ee4f30df1310d22147&amp;chksm=976fa1c3a01828d54c8b715851b5ce3bf2e63cb601a98a296228f4a0b98829e0b1b779fc1b68&amp;scene=21#wechat_redirect</p><h4 id="_5-3-6-损失函数-目标函数-成本函数-目标损失函数-loss-function" tabindex="-1"><a class="header-anchor" href="#_5-3-6-损失函数-目标函数-成本函数-目标损失函数-loss-function" aria-hidden="true">#</a> 5.3.6. 损失函数/目标函数/成本函数/目标损失函数(Loss Function)</h4><p>用于评估模型的表现好坏</p><ul><li>在监督式学习中用来度量模型预测值与真实标签之间差异的函数</li><li>损失函数的大小取决于模型对训练数据的综合表现</li></ul><h5 id="_5-3-6-1-公式" tabindex="-1"><a class="header-anchor" href="#_5-3-6-1-公式" aria-hidden="true">#</a> 5.3.6.1. 公式</h5><p>L(y,f(x;<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>))</p><ul><li>入参：模型参数 <ul><li>直观看，入参是模型输出层结果，但本轮的模型参数唯一影响这个结果，所以真正的入参是所有的成千上万的模型参数</li></ul></li><li>返参：一个实数</li></ul><h5 id="_5-3-6-2-常用损失函数" tabindex="-1"><a class="header-anchor" href="#_5-3-6-2-常用损失函数" aria-hidden="true">#</a> 5.3.6.2. 常用损失函数</h5><h6 id="_5-3-6-2-1-交叉熵损失函数-cross-entropy-loss-function" tabindex="-1"><a class="header-anchor" href="#_5-3-6-2-1-交叉熵损失函数-cross-entropy-loss-function" aria-hidden="true">#</a> 5.3.6.2.1. 交叉熵损失函数(Cross-Entropy Loss Function)</h6><p>是一种用于测量两个概率分布之间差异的数学函数</p><p>公式</p><figure><img src="/pinkpig/assets/nlp_017-ZrahlH2f.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>对模型输出层中与真实标签对应的那个概率值取负对数</p><p>真实标签的概率值越大，Loss就越小</p><p>适用</p><p>语言模型的损失函数几乎都是交叉熵损失函数</p><p>语言模型里，我们经常去依据前文的信息去预测下一个单词会是什么；神经网络最后输出的其实也是一个所有词的概率分布。此时，交叉熵损失函数可以帮助我们度量模型预测的概率分布与我们真正想要的概率分布之间的差异。</p><p>常用于分类任务中，特别是在神经网络中作为损失函数</p><h6 id="_5-3-6-2-2-平方损失函数-quadratic-loss-function" tabindex="-1"><a class="header-anchor" href="#_5-3-6-2-2-平方损失函数-quadratic-loss-function" aria-hidden="true">#</a> 5.3.6.2.2. 平方损失函数(Quadratic Loss Function)</h6><p>适用</p><p>一般不用此</p><p>不适用于分类问题，而适用于预测的输出值是一个实数值的任务中</p><p>在强化学习的TD算法里，我们会用到它</p><p>公式</p><p>预测值与真实值的差的平方的二分之一</p><figure><img src="/pinkpig/assets/nlp_018-Mpc-0du2.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h6 id="_5-3-6-2-3-均方误差-mse-mean-square-error-l2-loss" tabindex="-1"><a class="header-anchor" href="#_5-3-6-2-3-均方误差-mse-mean-square-error-l2-loss" aria-hidden="true">#</a> 5.3.6.2.3. 均方误差 (MSE, Mean Square Error) / L2 Loss</h6><p>是模型预测值 f(x) 与真实样本值 y 之间差值平方的平均值</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MSE</mtext><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex"> \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">MSE</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.1141em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p>MSE 对异常值更敏感（平方放大误差），常用于强调大误差的场景（如回归任务）</p><h6 id="_5-3-6-2-4-平均绝对误差-mae-mean-absolute-error-l1-loss" tabindex="-1"><a class="header-anchor" href="#_5-3-6-2-4-平均绝对误差-mae-mean-absolute-error-l1-loss" aria-hidden="true">#</a> 5.3.6.2.4. 平均绝对误差 (MAE, Mean Absolute Error) / L1 Loss</h6><p>是指模型预测值f(x)和真实值y之间距离的平均值</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MAE</mtext><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mi mathvariant="normal">∣</mi><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex"> \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord text"><span class="mord">MAE</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span></p><p>MAE 更鲁棒，适用于误差需线性惩罚的场景（如金融预测）</p><h4 id="_5-3-7-优化算法" tabindex="-1"><a class="header-anchor" href="#_5-3-7-优化算法" aria-hidden="true">#</a> 5.3.7. 优化算法</h4><p>优化算法就是决定在更新参数时如何调整步长、方向等因素，以最小化损失函数的值。</p><p>最常见的优化算法：小批量梯度下降</p><ul><li>常用参数更新方法 <ul><li>梯度下降：在一个方向上更新和调整模型的参数，来最小化损失函数</li><li>随机梯度下降（Stochastic gradient descent，SGD） <ul><li>对每个训练样本进行参数更新，每次执行都进行一次更新，且执行速度更快</li></ul></li><li>小批量梯度下降（Mini Batch Gradient Descent） <ul><li>对每个批次中的n个训练样本执行一次更新</li><li>使用小批量梯度下降的优点 <ul><li>可以减少参数更新的波动，最终得到效果更好和更稳定的收敛</li><li>可以使用上矩阵优化方法，使计算小批量数据的梯度更加高效</li></ul></li><li>通常来说，小批量样本的大小范围是从50到256，可以根据实际问题而有所不同。</li><li>在训练神经网络时，通常都会选择小批量梯度下降算法</li></ul></li><li>动量（Momentum）技术 <ul><li>通过优化相关方向的训练和弱化无关方向的振荡，来加速SGD训练</li></ul></li><li>Adam算法 <ul><li>即自适应时刻估计方法（Adaptive Moment Estimation），能计算每个参数的自适应学习率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减平均值，这一点与动量类似</li></ul></li></ul></li></ul><p>优化器</p><ul><li>一阶方法 <ul><li>随机梯度下降（SGD）、动量（Momentum）、牛顿动量法（Nesterov动量）、AdaGrad（自适应梯度）、RMSProp（均方差传播）、Adam、Nadam</li></ul></li><li>二阶方法 <ul><li>牛顿法、拟牛顿法、共轭梯度法（CG）、BFGS、L-BFGS</li></ul></li><li>自适应优化算法 <ul><li>Adagrad</li><li>RMSProp</li><li>Adam</li></ul></li></ul><p>梯度下降陷入局部最优有什么解决办法？ 可以用BGD、SGD、MBGD、momentum，RMSprop，Adam等方法来避免陷入局部最优。</p><h2 id="_6-神经网络的训练" tabindex="-1"><a class="header-anchor" href="#_6-神经网络的训练" aria-hidden="true">#</a> 6. 神经网络的训练</h2><p>学习过程就是反复去做梯度下降，让损失最小化的过程</p><h3 id="_6-0-llm-的训练目标" tabindex="-1"><a class="header-anchor" href="#_6-0-llm-的训练目标" aria-hidden="true">#</a> 6.0. LLM 的训练目标</h3><p>LLM 的训练目标通常是最大似然估计（Maximum Likelihood Estimation，MLE）。最大似然估计是统计学中一种常用的参数估计方法，其核心思想是通过最大化似然函数来估计模型参数，使得在该参数下观测到的数据出现的概率最大。</p><p>在LLM的训练过程中，使用的数据通常是大量的文本语料库。训练目标是最大化模型生成训练数据中观察到的文本序列的概率。具体来说，对于每个文本序列，模型根据前文生成下一个词的条件概率分布，并通过最大化生成的词序列的概率来优化模型参数。</p><p>为了最大化似然函数，可以使用梯度下降等优化算法来更新模型参数，使得模型生成的文本序列的概率逐步提高。在训练过程中，通常会使用批量训练（batch training）的方法，通过每次处理一小批数据样本来进行参数更新。</p><h3 id="_6-1-前向传播-forward-propagation" tabindex="-1"><a class="header-anchor" href="#_6-1-前向传播-forward-propagation" aria-hidden="true">#</a> 6.1. 前向传播(Forward Propagation)</h3><p>是神经网络训练的第一步</p><p>在神经网络中，从输入层到输出层的方向，进行信息传递的过程</p><p>在前向传播中，输入数据通过神经网络的多个层（包括输入层、隐藏层和输出层）按照预定的权重和偏置进行计算，从输入层传递到输出层，最终得到神经网络的预测结果</p><h3 id="_6-2-反向传播-back-propagation" tabindex="-1"><a class="header-anchor" href="#_6-2-反向传播-back-propagation" aria-hidden="true">#</a> 6.2. 反向传播(Back Propagation)</h3><p>反向传播就是公式从输出到输入的方向</p><p>反向传播是前向传播的逆过程，通过计算损失函数对神经网络输出的梯度，从输出层到输入层逐层的传播，计算每一层的梯度，并用于更新模型参数</p><p>反向传播的基本思想是根据链式法则计算损失函数对模型参数的梯度</p><h4 id="_6-2-1-梯度下降-gradient-descent" tabindex="-1"><a class="header-anchor" href="#_6-2-1-梯度下降-gradient-descent" aria-hidden="true">#</a> 6.2.1. 梯度下降(Gradient Descent)</h4><p>梯度下降就是更新模型参数，让损失做最大的下降的一次操作</p><p>负梯度向量其实告诉了我们，怎么去调节每一个参数，可以让神经网络的损失下降的最快。</p><h4 id="_6-2-2-梯度" tabindex="-1"><a class="header-anchor" href="#_6-2-2-梯度" aria-hidden="true">#</a> 6.2.2. 梯度</h4><p>梯度是一个向量，向量的维度就是模型参数的个数</p><p>它表示了这个多元函数在某一点上变化最快的方向和变化率，沿着这个方向，函数值的变化最大。</p><p>直观理解：最陡的方向</p><h4 id="_6-2-3-公式" tabindex="-1"><a class="header-anchor" href="#_6-2-3-公式" aria-hidden="true">#</a> 6.2.3. 公式</h4><figure><img src="/pinkpig/assets/nlp_019-Q3E-STTg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>假定每次训练我们使用 N 个样本，那么 Batch Size 就是 N。此时，Loss Function 就是这 N 个交叉熵函数的大小的平均值，我们是基于这个损失值去做反向传播，更新模型的参数的。</p><h4 id="_6-2-4-步骤" tabindex="-1"><a class="header-anchor" href="#_6-2-4-步骤" aria-hidden="true">#</a> 6.2.4. 步骤</h4><p>计算神经网络中各层权重和偏置的梯度并更新模型参数</p><ul><li>1.计算梯度 <ul><li>如何去计算那些梯度，变成了神经网络算法在工程实现上的核心问题。我们称之为反向传播算法</li></ul></li><li>2.更新模型参数 新的参数值 = 旧的参数值 - 学习率 * 梯度中该参数对应的值</li></ul><h4 id="_6-2-5-why" tabindex="-1"><a class="header-anchor" href="#_6-2-5-why" aria-hidden="true">#</a> 6.2.5. why</h4><p>让损失最小化，第一直觉就是求损失函数的最小值或极小值。但有成千上万参数的函数求最小值或极小值无法做到。</p><p>既然求不了最小值或极小值，那我们该怎么做呢？ 多元函数是一个曲面，因为模型的参数是随机初始化的，所以函数的结果就随机的落在了曲面的一个点上。</p><p>那我们就计算在每个方向的斜率，找到斜率最大的那个方向（最陡的方向），也就是梯度。然后沿着梯度方向往下走。（斜率很大，就多走几步快速下降；斜率很小，就少走几步，防止调过头）</p><p>对于每个点都如此反复，一直这么做下去，那么它就迟早会落到一个低点了。</p><p>想象一个人在凹凸不平的连绵不绝的山丘里找下山的路</p><h4 id="_6-2-6-梯度上升和梯度下降" tabindex="-1"><a class="header-anchor" href="#_6-2-6-梯度上升和梯度下降" aria-hidden="true">#</a> 6.2.6. 梯度上升和梯度下降</h4><ul><li>梯度上升 <ul><li>让多元函数的所有参数都沿着梯度的正方向走</li><li>此时函数值上升最快</li><li>适用场景：我们在强化学习的策略学习中，最大化状态价值函数，会做梯度上升</li></ul></li><li>梯度下降 <ul><li>让多元函数的所有参数都沿着梯度的反方向走</li><li>此时函数值下降最快</li><li>适用场景：在更多的场合，我们为了最小化损失，会去做梯度下降</li></ul></li></ul><h4 id="_6-2-7-梯度消失问题和梯度爆炸问题" tabindex="-1"><a class="header-anchor" href="#_6-2-7-梯度消失问题和梯度爆炸问题" aria-hidden="true">#</a> 6.2.7. 梯度消失问题和梯度爆炸问题</h4><h5 id="_6-2-7-1-what" tabindex="-1"><a class="header-anchor" href="#_6-2-7-1-what" aria-hidden="true">#</a> 6.2.7.1. what</h5><ul><li>梯度消失问题(Gradient Vanishing) <ul><li>现象 <ul><li>反向传播到较深层时，梯度非常小，甚至趋于零，导致这些层的权重更新变得非常缓慢或停止更新，使得这些层的参数无法得到有效的训练。</li></ul></li><li>导致的原因和解决 <ul><li>矩阵连乘 <ul><li>在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸</li><li>解决：残差连接和 normalize <ul><li>由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为矩阵连乘导致的梯度爆炸或梯度消失的问题已经解决了</li></ul></li></ul></li><li>使用了Sigmoid和tanh激活函数 <ul><li>解决：换用ReLU激活函数</li></ul></li></ul></li></ul></li><li>梯度爆炸问题(Gradient Explosion) <ul><li>现象 <ul><li>反向传播到较深层时，梯度非常大，导致权重参数更新过大，从而使得模型的训练极不稳定</li></ul></li><li>导致原因 <ul><li>矩阵连乘 <ul><li>在反向传播时，梯度会连乘，当梯度都小于1.0时，就会出现梯度消失；当梯度都大于1.0时，就会出现梯度爆炸</li></ul></li></ul></li><li>解决途径： <ul><li>梯度截断（也就是给一个最大值范围，如果超出了就取给定的最大值）</li><li>残差连接和 normalize <ul><li>由于使用了残差连接和 normalize 之后梯度消失和梯度爆炸已经极少出现了，所以目前可以认为矩阵连乘导致的梯度爆炸或梯度消失的问题已经解决了</li></ul></li></ul></li></ul></li></ul><h5 id="_6-2-7-2-why" tabindex="-1"><a class="header-anchor" href="#_6-2-7-2-why" aria-hidden="true">#</a> 6.2.7.2. why</h5><p>反向传播过程中，深层的网络的梯度计算需要经过连乘的操作，从而可能导致梯度的值指数级地变化，从而使梯度变得非常大或非常小</p><h5 id="_6-2-7-3-how" tabindex="-1"><a class="header-anchor" href="#_6-2-7-3-how" aria-hidden="true">#</a> 6.2.7.3. how</h5><p>归一化(Normalization)：将数据的均值变为0，方差变为1</p><h2 id="_7-示例" tabindex="-1"><a class="header-anchor" href="#_7-示例" aria-hidden="true">#</a> 7. 示例</h2><h3 id="_7-1-任务" tabindex="-1"><a class="header-anchor" href="#_7-1-任务" aria-hidden="true">#</a> 7.1. 任务</h3><p>识别手写数字</p><h3 id="_7-2-特征提取" tabindex="-1"><a class="header-anchor" href="#_7-2-特征提取" aria-hidden="true">#</a> 7.2. 特征提取</h3><p>构成这张图的28*28=784个像素点的灰度</p><figure><img src="/pinkpig/assets/nlp_022-6GARV98k.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>像素点的灰度：对于每一个像素，我们都给他赋一个从0到1的值。一个像素点越黑，他就越接近0；一个像素点越白，他就越接近1</p><p>将一张图表示为784维的、每个元素都在0到1之间的向量</p><h3 id="_7-3-神经网络" tabindex="-1"><a class="header-anchor" href="#_7-3-神经网络" aria-hidden="true">#</a> 7.3. 神经网络</h3><h4 id="_7-3-1-整体结构" tabindex="-1"><a class="header-anchor" href="#_7-3-1-整体结构" aria-hidden="true">#</a> 7.3.1. 整体结构</h4><figure><img src="/pinkpig/assets/nlp_023-CDihARF9.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="_7-3-2-分层结构" tabindex="-1"><a class="header-anchor" href="#_7-3-2-分层结构" aria-hidden="true">#</a> 7.3.2. 分层结构</h4><h5 id="_7-3-2-1-输入层" tabindex="-1"><a class="header-anchor" href="#_7-3-2-1-输入层" aria-hidden="true">#</a> 7.3.2.1. 输入层</h5><p>784个神经元</p><h5 id="_7-3-2-2-隐层" tabindex="-1"><a class="header-anchor" href="#_7-3-2-2-隐层" aria-hidden="true">#</a> 7.3.2.2. 隐层</h5><p>2层</p><ul><li>隐层的第1层：16个神经元</li><li>隐层的第2层：16个神经元</li></ul><h5 id="_7-3-2-3-输出层" tabindex="-1"><a class="header-anchor" href="#_7-3-2-3-输出层" aria-hidden="true">#</a> 7.3.2.3. 输出层</h5><p>10个神经元，每个神经元分别代表0~9的手写数字</p><h4 id="_7-3-3-函数视角" tabindex="-1"><a class="header-anchor" href="#_7-3-3-函数视角" aria-hidden="true">#</a> 7.3.3. 函数视角</h4><p>共13002个模型参数</p><ul><li>12960个权重参数：784<em>16+16</em>16+16*10=12960</li><li>42个偏置参数：16+16+10=42</li></ul><p>线性代数表示（同一层的一起计算）</p><ul><li>输入层表示为[784*1]的列向量</li><li>隐藏层第一层 <ul><li>active_function1([16 x 784] x [784 x 1] + [16 x 1]) = [16 x 1]</li><li>细节 <ul><li>权重矩阵【16 x 784】</li><li>输入层【784 x 1】</li><li>隐藏层第一层的16个偏置参数【16 x 1】</li><li>激活函数 active_function1(【16 x 1】) = 【16 x 1】</li></ul></li></ul></li><li>隐藏层第二层 <ul><li>active_function2(【16 x 16】【16 x 1】+【16 x 1】)=【16 x 1】</li></ul></li><li>输出层 <ul><li>active_function3(【10 x 16】【16 x 1】+【10 x 1】)=【10 x 1】</li></ul></li></ul><h3 id="_7-4-识别的结果" tabindex="-1"><a class="header-anchor" href="#_7-4-识别的结果" aria-hidden="true">#</a> 7.4. 识别的结果</h3><p>输出层中值最大的神经元对应的手写数字</p></div><!--[--><!----><!--]--><footer class="page-meta"><!----><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="两种不同的性能提升理论：涌现能力和扩展法则" class="vp-link nav-link prev nav-link prev" href="/pinkpig/llm/01_llm_basic/02_emergent_abilities_and_scaling_law.html"><div class="hint"><span class="arrow start"></span>Prev</div><div class="link"><span class="font-icon icon fa-fw fa-sm fas fa-lightbulb" style=""></span>两种不同的性能提升理论：涌现能力和扩展法则</div></a><!----></nav><!----><!--[--><!----><!--]--><!--]--></main><!--]--><!----></div><!--]--><!--]--><!----><!--]--></div>
    <script type="module" src="/pinkpig/assets/app-5kh03Iqc.js" defer></script>
  </body>
</html>
